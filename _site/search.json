[
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Causal-Behavioral Framework for Data Analysis\n\n\nIntroductory part to a series on Florent Buisson’s book, Behavioral Data Analysis with R & Python\n\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPizza Runner\n\n\nSecond part of a series of SQL case studies where I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoodie Fi\n\n\nThird part of a series of SQL case studies\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Bank\n\n\nFourth part of a series of SQL case studies… more CTEs!\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDanny’s Diner\n\n\nIn this first part of a series of SQL case studies, I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships of Variables in Sleep Data\n\n\nIn this post, I use regression, with an emphasis on understanding the casual relationships between variables\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSales Monitoring Dashboard\n\n\nMy take on the AdventureWorks dataset; featuring database creation, data modeling in Power BI and python scripting\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising COVID-19’s Impact in Nigeria\n\n\nTracking a pandemic with data\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)\n\n\nFeaturing data modeling in Power BI, and Dax\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#introduction",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#introduction",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "title": "Foodie Fi",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#database-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#database-connection",
    "title": "Foodie Fi",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"foodie_fi\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#queries",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#queries",
    "title": "Foodie Fi",
    "section": "Queries",
    "text": "Queries\n\n1. How many customers has Foodie-Fi ever had?\n\n\nSELECT count(DISTINCT customer_id)\nFROM subscriptions;\n\n\n1 records\n\n\ncount\n\n\n\n\n1000\n\n\n\n\n\n\n\n3. What is the monthly distribution of trial plan start_date values for our dataset use the start of the month as the group by value?\n\nSELECT upper(to_char(start_date, 'month')) as start_month,\n    count(*) frequency\nfrom subscriptions\nwhere plan_id = 0\ngroup by 1\nORDER BY 2 desc;\n\n\nDisplaying records 1 - 10\n\n\nstart_month\nfrequency\n\n\n\n\nMARCH\n94\n\n\nJULY\n89\n\n\nAUGUST\n88\n\n\nMAY\n88\n\n\nJANUARY\n88\n\n\nSEPTEMBER\n87\n\n\nDECEMBER\n84\n\n\nAPRIL\n81\n\n\nJUNE\n79\n\n\nOCTOBER\n79\n\n\n\n\n\n\n\n3. What plan start_date values occur after the year 2020 for our dataset? Show the breakdown by count of events for each plan_name ?\n\n\nSELECT p.plan_name,\n    count(*) as count_of_events_after_2020\nfrom subscriptions s\n    natural join plans p\nwhere EXTRACT(\n        year\n        from start_date\n    ) &gt; 2020\ngroup by 1;\n\n\n4 records\n\n\nplan_name\ncount_of_events_after_2020\n\n\n\n\npro annual\n63\n\n\nchurn\n71\n\n\npro monthly\n60\n\n\nbasic monthly\n8\n\n\n\n\n\n\n\n4. What is the customer count and percentage of customers who have churned rounded to 1 decimal place?\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id = 4\n)\nSELECT cte1.whole as total_customers,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_churned\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ntotal_customers\npct_churned\n\n\n\n\n2650\n12\n\n\n\n\n\n\n\n5. How many customers have churned straight after their initial free trial? what percentage is this rounded to the nearest whole number?\n\nwith cte as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\ncte2 as (\n    -- getting rows whose values satisfy the condition in the question\n    select *\n    from cte\n    where plan_id = 0\n        and lead_plan_id = 4\n) -- solution\nselect count(*) as count_of_customers_who_churned_after_free_trial\nfrom cte2 \n\n\n1 records\n\n\ncount_of_customers_who_churned_after_free_trial\n\n\n\n\n92\n\n\n\n\n\n\n\n6. What is the number and percentage of customer plans after their initial free trial?\n\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id &lt;&gt; 0\n)\nSELECT cte2.part as customer_count_after_trial_plan,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_ccatp\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ncustomer_count_after_trial_plan\npct_ccatp\n\n\n\n\n1650\n62\n\n\n\n\n\n\n\n7. How many customers have upgraded to an annual plan in 2020?\n\n\nwith cte1 as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\n-- filtering to only annual plans\ncte2 as (\n    select *,\n        lead_plan_id - plan_id as diff\n    from cte1\n    where lead_plan_id = 3\n) -- excluding churned customers and unupgraded plans\nselect count(DISTINCT customer_id) as upgraded_customers_2020_count\nfrom cte2\nwhere (diff &gt; 0)\n    and (lead_plan_id &lt;&gt; 4)\n    and EXTRACT(\n        year\n        from start_date\n    ) = 2020;\n\n\n1 records\n\n\nupgraded_customers_2020_count\n\n\n\n\n253\n\n\n\n\n\n\n\n8. How many days on average does it take for a customer to upgrade to an annual plan from the – day they join Foodie-Fi?\n\nwith cte1 as (\n    select *,\n        max(plan_id) over (partition by customer_id) as highest_plan_suscribed,\n        max(start_date) over (partition by customer_id) as date_of_hps,\n        min(start_date) over (partition by customer_id) as date_of_lps,\n        row_number() over (partition by customer_id) as sn\n    from subscriptions\n    order by customer_id,\n        start_date,\n        plan_id\n),\ncte2 as(\n    select *,\n        date_of_hps - date_of_lps as diff_in_days\n    from cte1\n    where highest_plan_suscribed = 3\n        and sn = 1\n)\nselect round(avg(diff_in_days)::numeric, 2) as avg_days_to_upgrade_to_annual\nfrom cte2\n\n\n1 records\n\n\navg_days_to_upgrade_to_annual\n\n\n\n\n105.95\n\n\n\n\n\n\n\n9. How many customers downgraded from a pro monthly to a basic monthly plan in 2020?\n\nwith cte1 as(\n        select *,\n            lead(plan_id) over(partition by customer_id) as lead_plan_id\n        from subscriptions\n        order by customer_id,\n            start_date,\n            plan_id\n    ),\n    cte2 as (\n        select customer_id,\n            plan_id,\n            lead_plan_id,\n            start_date\n            from cte1\n        where plan_id = 2\n            and lead_plan_id = 1\n            and EXTRACT(\n                year\n                from start_date\n            ) = 2020\n    )\nselect count(*) as number_of_customers_downgrade_from_prom_basm\nfrom cte2\n\n\n1 records\n\n\nnumber_of_customers_downgrade_from_prom_basm\n\n\n\n\n0"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#closing-the-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#closing-the-connection",
    "title": "Foodie Fi",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html",
    "href": "posts/2023-11-18-sleep-disorders/index.html",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "",
    "text": "The goal is to build a casual model using linear regression that explains quality of sleep, and logistic regression to explain sleep disorders\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n# descriptives\nlibrary(datawizard)\n\nWarning: package 'datawizard' was built under R version 4.3.2\n\nlibrary(kableExtra)\nlibrary(skimr)\nlibrary(qqplotr)\n\nWarning: package 'qqplotr' was built under R version 4.3.2\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.2\n\n# os\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.3.2"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#data-cleaning",
    "href": "posts/2023-11-18-sleep-disorders/index.html#data-cleaning",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Data cleaning",
    "text": "Data cleaning\nI observed the following issues in the data\n\nOccupation variable having levels with very few counts. Those with few counts can be consolidated with similar categories.\nThe BMI category variable has both “Normal” and “Normal Weight” as levels. This is most likely as data entry error.\nThe heart rate variable is not in a suitable format to work with. I’ll extract the systolic blood pressure from the variable as an integer. I would not need the diastolic blood pressure as I observed strong multicollinearity between both diastolic and systolic blood pressure.\n\n\n# observe occupations with low frequency counts\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# observe data entry error in bmi_category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# regrouping occupation variable so that occupations with lower counts are consolidated with other similar occupations. Simultaneously correcting data entry error in bmi_category variable\nsleep &lt;- sleep %&gt;% mutate( \n  occupation=ifelse(sleep$occupation %in% c(\"Software Engineer\", \"Scientist\", \"Engineer\"),\"Technical\", \nifelse(sleep$occupation %in% c(\"Salesperson\", \"Sales Representative\", \"Manager\"), \"Sales\", ifelse(sleep$occupation %in% c(\"Doctor\", \"Nurse\"), \"Medical\",\n                                                          as.character(occupation)))),\nbmi_category = ifelse(sleep$bmi_category %in% c(\"Normal\", \"Normal Weight\"), \"Normal\", as.character(sleep$bmi_category)))\n\n\n# extracting systolic blood pressure from the blood_pressure variable as it such a numeric variable is easier to work with. Subsequently dropping hear_rate variable\n\nsleep &lt;- sleep %&gt;% mutate(\nbp_sys= parse_number(str_sub(sleep$blood_pressure, 1, 3)),\nperson_id = as.character(person_id)\n) %&gt;% select(!heart_rate)\n\n\n# confirming changes\nhead(sleep)  %&gt;% gt() %&gt;% opt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      person_id\n      gender\n      age\n      occupation\n      sleep_duration\n      quality_of_sleep\n      physical_activity_level\n      stress_level\n      bmi_category\n      blood_pressure\n      daily_steps\n      sleep_disorder\n      bp_sys\n    \n  \n  \n    1\nMale\n27\nTechnical\n6.1\n6\n42\n6\nOverweight\n126/83\n4200\nNone\n126\n    2\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    3\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    4\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    5\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    6\nMale\n28\nTechnical\n5.9\n4\n30\n8\nObese\n140/90\n3000\nInsomnia\n140\n  \n  \n  \n\n\n\n\n\n# confirming changes to BMI category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# confirming changes to occupation variable\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#associations",
    "href": "posts/2023-11-18-sleep-disorders/index.html#associations",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Associations",
    "text": "Associations\n\nResearch Question: “What strong associations exists between quality_of_sleep and other variables in the dataset?\n\n# | warning: false\n# | message: false\n# packages required for associations\nlibrary(correlation)\nlibrary(GGally)\n\n\n# plotting the grid plot of scatterplots of numeric variables\nggscatmat(sleep %&gt;% select_if(is.numeric)) + theme_light()\n\n\n\n\n\n# showing relationships with strong correlations (greater than |+-0.4|)\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation() %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs)) %&gt;% filter(abs(r) &gt; 0.4) %&gt;% arrange(r) \ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.899\n0.000\n    sleep_duration\nstress_level\n−0.811\n0.000\n    age\nstress_level\n−0.422\n0.000\n    age\nquality_of_sleep\n0.474\n0.000\n    age\nbp_sys\n0.606\n0.000\n    physical_activity_level\ndaily_steps\n0.773\n0.000\n    sleep_duration\nquality_of_sleep\n0.883\n0.000\n  \n  \n  \n\n\n\n\n\n# robust correlation analysis since the data are skewed. Results is filtered to include only relationships involving quality_of_sleep. Sorted ascendingly by p-value\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation(method=\"percentage\") %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs))  %&gt;% arrange(p)  %&gt;% filter( Parameter1 == \"quality_of_sleep\" | Parameter2 == \"quality_of_sleep\") %&gt;% arrange(p) %&gt;% filter(p &lt; 0.05)\ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.910\n0.000\n    sleep_duration\nquality_of_sleep\n0.892\n0.000\n    age\nquality_of_sleep\n0.432\n0.000\n    quality_of_sleep\nphysical_activity_level\n0.178\n0.005\n  \n  \n  \n\n\n\n\n\nResults\nThe most significant relationships (at alpha = .05) are the relationships between:\n\nquality of sleep and stress level, r(372) =-0.91, p &lt; 0.0001\nquality of sleep and sleep duration, r(372) = 0.89, p &lt; 0.0001\nquality of sleep and age, r(372) =0.43, p &lt; 0.0001\nquality of sleep and physical activity level, r(372) =0.18, p = 0.0054\n\nThese are the variables we would fit to our linear regression model."
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#research-questions",
    "href": "posts/2023-11-18-sleep-disorders/index.html#research-questions",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research questions:",
    "text": "Research questions:\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\nIs the relationship between stress level and quality of sleep moderated by occupation?\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\n\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\n\n# Visualizing the relationship of the three variables\nggplot(sleep, aes(x = sleep_duration, y = quality_of_sleep, color = sleep_disorder)) +\ngeom_jitter(size=1, alpha=0.8) +\nlabs(x = \"Sleep duration\", y = \"Quality of sleep\", color = \"Sleep disorder\") +\ngeom_smooth(method = \"lm\", se = FALSE, linewidth=0.9) + theme_bw()\n\n\n\n\nThe plot shows that there might be some moderation effects.\n\n# making sleep_disorder a factor variable and setting the base level to \"None\"\nsleep &lt;- sleep %&gt;% mutate(\n  sleep_disorder = as_factor(sleep_disorder) %&gt;% fct_relevel(\"None\")\n)\n\n\nqs_lm_03 &lt;- lm(quality_of_sleep ~ sleep_duration * sleep_disorder, data=sleep)\nqs_lm_03 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-0.5750673\n0.36703899\n-1.566774\n1.180269e-01\n    sleep_duration\n1.1144522\n0.04963586\n22.452560\n6.124844e-71\n    sleep_disorderSleep Apnea\n-3.4341228\n0.57712441\n-5.950403\n6.233752e-09\n    sleep_disorderInsomnia\n-3.2040380\n1.11193745\n-2.881491\n4.190005e-03\n    sleep_duration:sleep_disorderSleep Apnea\n0.4802913\n0.08000178\n6.003508\n4.632039e-09\n    sleep_duration:sleep_disorderInsomnia\n0.4503706\n0.16657925\n2.703642\n7.176641e-03\n  \n  \n  \n\n\n\n\n\nInterpretation\n\n\n\n\n\n\nTip\n\n\n\nThe effect of increasing sleep duration in someone with no sleep disorder on sleep quality is 1.11\nThe effect of sleep apnea on sleep quality compared to someone with no disorders is -3.43\nThe effect of Insomnia on sleep quality compared to someone with no disorders is -3.20\nThe effect of increasing sleep duration in someone with sleep anea on sleep quality compared to someone with no disorders is 0.48\nThe effect of increasing sleep duration in someone with insomnia on sleep quality compared to someone with no disorders is 0.45\n\n\n\ninteractions::sim_slopes(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of sleep_duration when sleep_disorder = Insomnia: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.56   0.15   1.26    1.86    10.26   0.00\n\nSlope of sleep_duration when sleep_disorder = Sleep Apnea: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.59   0.06   1.48    1.71    27.35   0.00\n\nSlope of sleep_duration when sleep_disorder = None: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.11   0.03   1.05    1.17    36.50   0.00\n\n\nThe slopes for the three regressions are not significantly different\n\ninteractions::interact_plot(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Sleep disorder\"\n  )\n\n\n\n\nLooking at the slope, we can that the moderation effect is not significant.\n\n\nConclusion\nSleep disorder is not a moderator of the relationship between sleep duration and quality of sleep\n\n\n\nIs the relationship between stress level and quality of sleep moderated by occupation?\n\nqs_lm_04 &lt;- lm(quality_of_sleep ~ stress_level * occupation, data=sleep)\nqs_lm_04 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n9.3661417\n0.22464236\n41.6935685\n2.847324e-140\n    stress_level\n-0.3208661\n0.04725953\n-6.7894487\n4.624546e-11\n    occupationLawyer\n3.3611310\n1.08304354\n3.1034126\n2.063488e-03\n    occupationMedical\n1.3637168\n0.24305144\n5.6108156\n4.008125e-08\n    occupationSales\n3.5481440\n1.02746628\n3.4532948\n6.193572e-04\n    occupationTeacher\n-0.2842770\n0.33263086\n-0.8546322\n3.933200e-01\n    occupationTechnical\n2.0623438\n0.25972133\n7.9406026\n2.559029e-14\n    stress_level:occupationLawyer\n-0.6336793\n0.21426067\n-2.9575158\n3.304916e-03\n    stress_level:occupationMedical\n-0.2851357\n0.04939422\n-5.7726532\n1.679151e-08\n    stress_level:occupationSales\n-0.6791339\n0.15058782\n-4.5098857\n8.775327e-06\n    stress_level:occupationTeacher\n-0.1447393\n0.07087144\n-2.0422799\n4.184795e-02\n    stress_level:occupationTechnical\n-0.4718021\n0.05572917\n-8.4659819\n6.415721e-16\n  \n  \n  \n\n\n\n\n\ninteractions::sim_slopes(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of stress_level when occupation = Lawyer: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.95   0.03   -1.02   -0.89   -29.71   0.00\n\nSlope of stress_level when occupation = Accountant: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.32   0.04   -0.40   -0.24    -7.92   0.00\n\nSlope of stress_level when occupation = Teacher: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.47   0.09   -0.65   -0.28    -5.01   0.00\n\nSlope of stress_level when occupation = Sales: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -1.00   1.04   -3.04    1.04    -0.97   0.33\n\nSlope of stress_level when occupation = Medical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.61   0.01   -0.62   -0.59   -81.54   0.00\n\nSlope of stress_level when occupation = Technical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.79   0.07   -0.93   -0.65   -10.97   0.00\n\n\n\ninteractions::interact_plot(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Occupation\"\n  )\n\n\n\n\nAgain, we do not observe a significant moderation effect of occupation on the relationship between stress level and quality of sleep\n\n\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\nTo test this, we are going to run 3 regressions:\n\nThe total effect of physical_activity_level on quality_of_sleep.\nThe effect of physical_activity_level on quality_of_sleep that is mediated by daily_steps, a.k.a. the indirect effect\nThe effect of physical_activity_level on quality_of_sleep that is not mediated by daily_steps, a.k.a. the direct effect\n\n\nTotal effect\nWe first determine the total effect by running a regression of physical_activity_level on quality_of_sleep (without including daily_steps):\n\nlm(quality_of_sleep ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n6.657\n0.183\n36.307\n0.000\n    physical_activity_level\n0.011\n0.003\n3.792\n0.000\n  \n  \n  \n\n\n\n\nThe total effect is approximately 0.011, meaning that an increase of one unit in physical activity increases sleep quality by 0.001 units on average, not holding daily steps constant.\n\n\nMediated/Indirect Effect\nThe effect of physical_activity_level on quality_of_sleep mediated by daily_steps can be obtained by multiplying together the effect of physical_activity_level on daily_steps and the effect of daily_steps on quality_of_sleep.\n\n1. physical_activity_level on daily_steps (a)\n\nlm(daily_steps ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n3265.57619\n160.321889\n20.36887\n1.718735e-62\n    physical_activity_level\n60.01692\n2.556092\n23.47995\n1.968256e-75\n  \n  \n  \n\n\n\n\nOne unit increase in physical_activity_level increases daily_steps by an average of 60 steps\n\n\n2. daily_steps on quality_of_sleep\n\nlm(quality_of_sleep ~ daily_steps + physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n7.4500815511\n2.609352e-01\n28.551459\n1.151887e-95\n    daily_steps\n-0.0002428667\n5.802065e-05\n-4.185867\n3.551189e-05\n    physical_activity_level\n0.0256601109\n4.506428e-03\n5.694113\n2.524969e-08\n  \n  \n  \n\n\n\n\n(a * b) = 60.07 * -0.00024 = -0.0144\n\n\n\nDirect Effect\nThis is simply the coefficient of physical_activity_level in the above regression = 0.0257\nAccording to Zhao et al., what we have here isCompetitive Mediation (Regularly Partial Mediation)\nIn the competitive partial mediation hypothesis, it is assumed that the intermediate variable (daily_steps) could sometimes increase and at times decrease the relationship between the independent and dependent variables. i.e an “inconsistent” model.\n\n\n\nDecision tree for determining mediation"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "href": "posts/2023-11-18-sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research question: are significant differences in the mean sleep quality of both male and female participants",
    "text": "Research question: are significant differences in the mean sleep quality of both male and female participants\n\n# summary statistics of quality_of_sleep by gender\n\nby(cbind(data=sleep$quality_of_sleep), sleep$gender, psych::describe)\n\nINDICES: Female\n     vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 185 7.66 1.28      8    7.76 1.48   4   9     5 -0.49    -0.83 0.09\n------------------------------------------------------------ \nINDICES: Male\n     vars   n mean sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 189 6.97  1      7    7.01 1.48   4   9     5 -0.35    -0.59 0.07\n\n\nThe mean of female participants is one point higher than males. But is this difference significant?\n\n# plotting a violin-errorbar plot to visualise the relationship\nggplot(sleep, aes(gender,quality_of_sleep))+\ngeom_violin() +\nstat_summary(fun.data=\"mean_cl_normal\") + \ntheme_minimal()\n\n\n\n\nSince the errorbars do not overlap, we can be fairly confident that the difference is significant. Lets confirm this hunch with a Welsh’s t-test.\n\n t.test(quality_of_sleep ~ gender,\n                    data = sleep,\n                    paired = FALSE,\n                    var.equal = FALSE,\n                    conf.level = 0.95,\n                    na.action = na.exclude)\n\n\n    Welch Two Sample t-test\n\ndata:  quality_of_sleep by gender\nt = 5.8593, df = 347.96, p-value = 1.078e-08\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n 0.4627786 0.9304432\nsample estimates:\nmean in group Female   mean in group Male \n            7.664865             6.968254 \n\n\nBecause the p-value = 1.078e-08 is less than our alpha of .05, we can conclude that the difference between both genders is significant. Also, since the confidence interval expressing the true difference in means does not cross zero, we can be confident that there exists a true difference, with a chance of us being wrong 5% of the time.\n\neffectsize::cohens_d(quality_of_sleep ~ gender, data = sleep) %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Cohens_d\n      CI\n      CI_low\n      CI_high\n    \n  \n  \n    0.608\n0.950\n0.400\n0.815\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOn average, female participants have better sleep quality (M = 7.66, SE = 0.09), than those not given a cloak (M = 6.97, SE = 0.07). Sleep quality is significantly different for both genders: the mean difference, M = 0.69, 95% CI [0.46, 0.93], was significantly different from 0, t(347.96) = 5.85, p = 01.078e-08. This effect was quite large, d= 0.61[0.4, 0.82]"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#conclusion-1",
    "href": "posts/2023-11-18-sleep-disorders/index.html#conclusion-1",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Conclusion",
    "text": "Conclusion\nI created this post as a way of solidifying my understanding of these concepts. I find that documenting the material helps it to stick iwth me better. I hope that you find this helpful. My deepest gratitude goes to Andy Field for his exceptional book on statistical analysis with R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chamber of (data) secrets",
    "section": "",
    "text": "The Causal-Behavioral Framework for Data Analysis\n\n\n\ncausal regression modelling\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPizza Runner\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoodie Fi\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Bank\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDanny’s Diner\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships of Variables in Sleep Data\n\n\n\nstats\n\n\ncausal regression modelling\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSales Monitoring Dashboard\n\n\n\npython\n\n\ndata visualisation\n\n\n\n\n\n\n\nEdun Joshua\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising COVID-19’s Impact in Nigeria\n\n\n\npython\n\n\ndata visualisation\n\n\n\n\n\n\n\nEdun Joshua\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)\n\n\n\npower bi\n\n\ndata visualisation\n\n\n\n\n\n\n\nEdun Joshua\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Musings of a data mage",
    "section": "",
    "text": "See all"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Edun Joshua is a data enthusiast and an environmental engineering student. Data science is a reprieve from the existential crises that haunt him."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nFederal University of Agriculture, Abeokuta | Alabata, Ogun State\nB.Eng. in Environmental Engineering | March 2017 - March 2023"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#introduction",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#introduction",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "title": "Danny’s Diner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram\n\n\nSales\nThe sales table captures all customer_id level purchases with an corresponding order_date and product_id information for when and what menu items were ordered.\n\n\nMenu\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\n\n\nMembers\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\nkey concepts: CTEs, window functions"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#database-connection",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#database-connection",
    "title": "Danny’s Diner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"danny_diners\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#queries",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#queries",
    "title": "Danny’s Diner",
    "section": "Queries",
    "text": "Queries\nNow let’s convert business questions into SQL queries!\n\n1. What is the total amount each customer spent at the restaurant?\n\n\nselect s.customer_id as customer,\n   sum(m.price) as total_amount\nfrom sales s\n   inner join menu m on s.product_id = m.product_id\ngroup by customer;\n\n\n3 records\n\n\ncustomer\ntotal_amount\n\n\n\n\nB\n74\n\n\nC\n36\n\n\nA\n76\n\n\n\n\n\n\n\n2. How many days has each customer visited the restaurant?\n\n\nselect customer_id,\n   count(\n      distinct extract(\n         day\n         from order_date\n      )\n   ) as no_of_days_visited\nfrom sales\ngroup by customer_id\norder by customer_id;\n\n\n3 records\n\n\ncustomer_id\nno_of_days_visited\n\n\n\n\nA\n4\n\n\nB\n5\n\n\nC\n2\n\n\n\n\n\nThis SQL query calculates the number of days each customer has visited a restaurant by grouping the sales transactions by customer ID, extracting the day from the order date, counting the distinct days, and ordering the results by customer ID.\n\n\n3. What was the first item from the menu purchased by each customer?\n\n\nwith cte as (\n   select customer_id,\n      order_date,\n      row_number() over (\n         partition by customer_id\n         order by order_date\n      ) as order_rank,\n      product_id\n   from sales\n)\nselect c.customer_id,\n   c.order_date,\n   c.order_rank,\n   m.product_name\nfrom cte c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\norder_date\norder_rank\nproduct_name\n\n\n\n\nA\n2021-01-01\n1\nsushi\n\n\nB\n2021-01-01\n1\ncurry\n\n\nC\n2021-01-01\n1\nramen\n\n\n\n\n\nWithin the CTE, the row_number() function is employed to assign a ranking to each order for each customer based on the order_date. The ranking starts from 1, indicating the first order for each customer. The main query then selects the customer ID, order date, order rank, and product name from the CTE, joining the menu table to retrieve the product name corresponding to the product ID. Finally, it filters the results to include only records with an order rank of 1, effectively selecting the first item purchased for each customer.\n\n\n4. What is the most purchased item on the menu and how many times was it purchased by all customers?\n\n\nwith cte as(\n   select product_id,\n      count(product_id)\n   from sales\n   group by product_id\n   order by count(product_id) desc\n   limit 1\n) -- Most purchased item on the menu is the product with the id 3 which is ramen, according to cte\nselect customer_id,\n   count(product_id) as count_of_most_purchased_product\nfrom sales\nwhere product_id in (\n      select product_id\n      from cte\n   )\ngroup by customer_id;\n\n\n3 records\n\n\ncustomer_id\ncount_of_most_purchased_product\n\n\n\n\nA\n3\n\n\nB\n2\n\n\nC\n3\n\n\n\n\n\nThe query utilizes a common table expression (CTE) named cte to generate a temporary result set. Within the CTE, the count() function is employed to count the number of times each product ID appears in the sales table. The results are then sorted in descending order based on the product count. The limit 1 clause restricts the output to the top row, effectively identifying the product ID with the highest count.\nThe main query then selects the customer ID and the count of the most purchased product for each customer. It filters the sales table to include only records where the product_id matches the one identified in the CTE, effectively focusing on the most purchased item. Finally, it groups the results by customer_id to determine how many times each customer purchased the most popular item.\n\n\n5. Which item was the most popular for each customer?\n\n\nwith cte_1 as (\n   select customer_id,\n      product_id,\n      count(product_id) as count_of_item\n   from sales\n   group by customer_id,\n      product_id\n),\ncte_2 as (\n   select *,\n      row_number() over (\n         partition by customer_id\n         order by count_of_item desc\n      ) as order_rank\n   from cte_1\n)\nselect c.customer_id,\n   m.product_name,\n   c.count_of_item\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\nproduct_name\ncount_of_item\n\n\n\n\nC\nramen\n3\n\n\nB\nramen\n2\n\n\nA\nramen\n3\n\n\n\n\n\nThis query aims to identify the most popular item for each customer. The query utilizes two common table expressions to process the data and generate the desired output. The first CTE, named cte_1, calculates the count of each product purchased by each customer. It groups the rows in the sales table by customer_id and product_id, and then counts the occurrences of each product ID for each customer. This step determines the frequency of each product purchase for each customer.\nThe second CTE, named cte_2, assigns a ranking to each product for each customer based on the purchase frequency calculated in cte_1. It uses the row_number() function and partitions the data by customer_id, sorting within each partition by the count_of_item in descending order. This step effectively identifies the product with the highest purchase frequency (i.e., the most popular item) for each customer.\nThe main query then selects the customer ID, product name, and purchase count for each customer’s most popular item. It joins the menu table to obtain the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the most popular item for each customer is selected.\n\n\n6. Which item was purchased first by the customer after they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date\n      ) as order_rank\n   from cte_1\n   where order_date &gt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nramen\n2021-01-10\n1\n\n\nB\nsushi\n2021-01-11\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date, ensuring a chronological order of transactions. The second CTE, named cte_2, focuses on purchases made after each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is later than the join_date (membership start date).\nAdditionally, it assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date. This identifies the first purchase (order_rank = 1) made by each customer after becoming a member. The main query then selects the customer ID, product name, order date, and order rank for each customer’s first purchase after becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the first purchase is selected.\n\n\n7. Which item was purchased just before the customer became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      join_date,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date desc\n      ) as order_rank\n   from cte_1\n   where order_date &lt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nsushi\n2021-01-01\n1\n\n\nB\nsushi\n2021-01-04\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date in ascending order, ensuring a chronological sequence of transactions.\nThe second CTE, cte_2, focuses on purchases made before each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date).\nIt assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date in descending order. This identifies the last purchase (order_rank = 1) made by each customer before becoming a member.\nThe main query then selects the customer ID, product name, order date, and order rank for each customer’s last purchase before becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the last purchase is selected.\n\n\n8. What is the total items and amount spent for each member before they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n      natural join menu\n   order by order_date\n)\nselect customer_id,\n   count(distinct product_id) as count_of_products,\n   sum(price) as total_amount_spent\nfrom cte_1\nwhere order_date &lt; join_date\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ncount_of_products\ntotal_amount_spent\n\n\n\n\nA\n2\n25\n\n\nB\n2\n40\n\n\n\n\n\nThe query utilizes a CTE named cte_1 to prepare the data and simplifies the aggregation in the main query.\nThe main query then aggregates the data for each customer based on their membership status. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date). For each customer, it counts the distinct product_id values to determine the total number of unique items purchased and calculates the sum of price values to determine the total amount spent. The results are grouped by customer_id to provide individual summaries for each member.\n\n\n9. If each $1 spent equates to 10 points and sushi has a 2x points multiplier - how many points would each customer have?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points\n   from members\n      natural join sales\n      natural join menu\n)\nselect customer_id,\n   sum(points) as total_points\nfrom cte\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n860\n\n\nB\n940\n\n\n\n\n\nThe CTE combines the members, sales, and menu tables, providing a comprehensive view of customer memberships, their purchases, and the corresponding product names.\nThe main query then summarizes the points earned for each customer. It groups the data from the CTE by customer_id and calculates the sum of points values for each group, effectively determining the total points earned by each customer.\n\n\n10. In the first week after a customer joins the program (including their join date) they earn 2x points on all items, – not just sushi - how many points do customer A and B have at the end of January?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points,\n      case\n         when order_date - join_date &lt;= 7 then 2\n         else 1\n      end multiplier\n   from members\n      natural join sales\n      natural join menu\n),\ncte_2 as (\n   select *,\n      points * multiplier as total_points\n   from cte\n)\nselect customer_id,\n   sum(total_points) as total_points\nfrom cte_2\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n1720\n\n\nB\n1760\n\n\n\n\n\nThe first CTE, named cte, joins the members, sales, and menu tables, and calculates the points earned for each purchase using a conditional CASE expression, similar to the previous query.\nAdditionally, it assigns a multiplier to each purchase based on whether it falls within the first week after the customer’s join date. For purchases within the first week, the multiplier is 2 (double points); for purchases outside the first week, the multiplier is 1 (standard points).\nThe second CTE, named cte_2, simplifies the calculation by multiplying the points and multiplier columns for each purchase, effectively determining the total points earned per transaction. The main query then summarizes the points earned for each customer, including the double points accrued during the first week. It groups the data from cte_2 by customer_id and calculates the sum of total_points values for each group, providing the total points earned by customer A and customer B at the end of January\n\n\n11. Recreate the following table output using the available data:\n\n\nselect s.customer_id,\n    s.order_date,\n    men.product_name,\n    men.price,\n    CASE\n        WHEN s.order_date &gt;= m.join_date THEN 'Y'\n        ELSE 'N'\n    END\nfrom sales s\n    LEFT JOIN menu men ON s.product_id = men.product_id\n    LEFT JOIN members m on m.customer_id = s.customer_id\nORDER BY s.customer_id,\n    s.order_date;\n\n\nDisplaying records 1 - 10\n\n\ncustomer_id\norder_date\nproduct_name\nprice\ncase\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#closing-the-connection",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#closing-the-connection",
    "title": "Danny’s Diner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#introduction",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#introduction",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "title": "Pizza Runner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#database-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#database-connection",
    "title": "Pizza Runner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"pizza_runner\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#queries",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#queries",
    "title": "Pizza Runner",
    "section": "Queries",
    "text": "Queries\n\nA. Pizza Metrics\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\nselect count(pizza_id)\nfrom customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n14\n\n\n\n\n\n\n\n2. How many unique customer orders were made?\n\n\nSELECT count(DISTINCT order_id) from customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n10\n\n\n\n\n\n\n\n3. How many successful orders were delivered by each runner?\n\n\nselect runner_id,\n    count(order_id) as count_of_successful_orders\nfrom runner_orders\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP BY runner_id;\n\n\n3 records\n\n\nrunner_id\ncount_of_successful_orders\n\n\n\n\n3\n1\n\n\n2\n3\n\n\n1\n4\n\n\n\n\n\n\n\n4. How many of each type of pizza was delivered?\n\n\nselect pn.pizza_name as pizza,\n    count(co.pizza_id) as count_of_pizza_delivered\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id\n    INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP by pn.pizza_name;\n\n\n2 records\n\n\npizza\ncount_of_pizza_delivered\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\n\n5. How many Vegetarian and Meatlovers were ordered by each customer?\n\n\nselect co.customer_id as customers, count(co.pizza_id) as count_of_pizza_ordered from customer_orders co INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id group by 1;\n\n\n5 records\n\n\ncustomers\ncount_of_pizza_ordered\n\n\n\n\n101\n3\n\n\n103\n4\n\n\n104\n3\n\n\n105\n1\n\n\n102\n3\n\n\n\n\n\n\n\n6. What was the maximum number of pizzas delivered in a single order?\n\n\nselect ro.order_id as order,\n    count(co.pizza_id) as number_of_pizzas\nfrom runner_orders ro\n    INNER JOIN customer_orders co ON ro.order_id = co.order_id\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 1;\n\n\n1 records\n\n\norder\nnumber_of_pizzas\n\n\n\n\n4\n3\n\n\n\n\n\n\n\n7. For each customer, how many delivered pizzas had at least 1 change and how many? and no changes?\n\n-- PART 1\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with at least 1 changes\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n        or (\n            exclusions not in ('', 'null')\n            or extras not in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n4 records\n\n\ncustomer_id\ndelivered_pizzas_with_changes\n\n\n\n\n102\n1\n\n\n105\n1\n\n\n104\n3\n\n\n103\n3\n\n\n\n\n\n\n-- PART 2: orders with no changes\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions = ''\n        and extras = ''\n        or (\n            exclusions in ('', 'null')\n            or extras in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_no_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n5 records\n\n\ncustomer_id\ndelivered_pizzas_with_no_changes\n\n\n\n\n101\n2\n\n\n102\n3\n\n\n103\n3\n\n\n104\n2\n\n\n105\n1\n\n\n\n\n\n\n\n8. How many pizzas were delivered that had both exclusions and extras?\n\n\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with both exclusions and extras\ncte2 as (\n    select *\n    from customer_orders\n    where (\n            exclusions &lt;&gt; 'null'\n            and extras &lt;&gt; 'null'\n        )\n        and exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n)\nselect *\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id;\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\norder_id..7\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10minutes\nnull\n10\n104\n1\n2, 6\n1, 4\n2020-01-11 18:34:49\n\n\n\n\n\n\n\n9. What was the total volume of pizzas ordered for each hour of the day?\n\n\nselect EXTRACT (\n        hour\n        from order_time\n    ) as hour_of_day,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n6 records\n\n\nhour_of_day\npizza_volume\n\n\n\n\n18\n3\n\n\n21\n3\n\n\n23\n3\n\n\n13\n3\n\n\n19\n1\n\n\n11\n1\n\n\n\n\n\n\n\n10. What was the volume of orders for each day of the week?\n\n-- select EXTRACT (\n--        dow\n--        from order_time\n--    ) as day_of_week,\n--    count(pizza_id) as pizza_volume\n-- from customer_orders\n-- GROUP BY 1;\n-- or\nselect to_char(order_time, 'Day') as day_of_week,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n4 records\n\n\nday_of_week\npizza_volume\n\n\n\n\nSaturday\n5\n\n\nThursday\n3\n\n\nFriday\n1\n\n\nWednesday\n5\n\n\n\n\n\n\n\n\nB. Runner and Customer Experience\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\n\nselect EXTRACT(\n        week\n        from registration_date\n    ),\n    count(runner_id)\nfrom runners\nGROUP BY 1;\n\n\n3 records\n\n\nextract\ncount\n\n\n\n\n1\n1\n\n\n53\n2\n\n\n2\n1\n\n\n\n\n\n\n\n2. What was the average time in minutes it took for each runner to arrive at the Pizza – Runner HQ to pickup the order?\n\nselect case\n        when ro.pickup_time = 'null' then null\n        else round(\n            EXTRACT(\n                epoch\n                from (\n                        ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                    )\n            ) / 60,\n            2\n        )\n    end as duration_till_pickup,\n    ro.pickup_time,\n    co.order_time\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id; \n\n\nDisplaying records 1 - 10\n\n\nduration_till_pickup\npickup_time\norder_time\n\n\n\n\n10.53\n2020-01-01 18:15:34\n2020-01-01 18:05:02\n\n\n10.03\n2020-01-01 19:10:54\n2020-01-01 19:00:52\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n10.47\n2020-01-08 21:10:57\n2020-01-08 21:00:29\n\n\nNA\nnull\n2020-01-08 21:03:13\n\n\n10.27\n2020-01-08 21:30:45\n2020-01-08 21:20:29\n\n\n\n\n\nThis query calculates the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pick up the order. It first joins the customer_orders and runner_orders tables to get the order information and the runner’s pickup time. Then, it checks if the pickup_time is null, and if so, it sets the corresponding duration_till_pickup to null. Otherwise, it calculates the time difference between the pickup_time and the order_time in seconds using the EXTRACT() and TIMESTAMP()functions. It then converts the time difference to minutes and rounds it to two decimal places. Finally, it selects the duration_till_pickup, pickup_time, and order_time for each order.\n\n\n3. What was the average time in minutes it took for each runner to deliver pizzas?\n\n\nselect runner_id,\n    round(\n        avg(\n            case\n                when left(duration, 2) ~ '^\\d+$' THEN cast(left(duration, 2) as integer)\n                else null\n            end\n        ),\n        2\n    ) as extracted_minutes\nfrom runner_orders\ngroup by runner_id;\n\n\n3 records\n\n\nrunner_id\nextracted_minutes\n\n\n\n\n3\n15.00\n\n\n2\n26.67\n\n\n1\n22.25\n\n\n\n\n\n\n\n5. What was the difference between the longest and shortest delivery times for all?\n\n\n-- If we define delivery times as the duration between ro.pickup_time - co.order_time + ro.duration\n-- then:\nwith cte1 as (\n    select case\n            when ro.pickup_time = 'null' then null\n            else round(\n                EXTRACT(\n                    epoch\n                    from (\n                            ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                        )\n                ) / 60,\n                2\n            )\n        end as duration_till_pickup,\n        ro.pickup_time,\n        co.order_time,\n        round(\n            case\n                when left(ro.duration, 2) ~ '^\\d+$' THEN cast(left(ro.duration, 2) as integer)\n                else null\n            end,\n            2\n        ) as cleaned_duration_minutes\n    from customer_orders co\n        INNER JOIN runner_orders ro on co.order_id = ro.order_id\n)\nselect max(duration_till_pickup + cleaned_duration_minutes) as longest_delivery_time,\n    min(duration_till_pickup + cleaned_duration_minutes) as shortest_delivery_time,\n    max(duration_till_pickup + cleaned_duration_minutes) - min(duration_till_pickup + cleaned_duration_minutes) as difference\nfrom cte1;\n\n\n1 records\n\n\nlongest_delivery_time\nshortest_delivery_time\ndifference\n\n\n\n\n69.28\n25.47\n43.81\n\n\n\n\n\nNow, this is a monstrous looking one 😅 .\nA (CTE) named cte1 is created, extracting relevant information such as duration_till_pickup, pickup_time, order_time, and cleaned_duration_minutes from the customer_orders (co) and runner_orders (ro) tables. The duration_till_pickup is calculated as the time between pickup and order in minutes. The cleaned_duration_minutes extracts the duration in minutes from the ro.duration field.\nThe main query then computes the maximum, minimum, and the difference between the sum of duration_till_pickup and cleaned_duration_minutes. These values represent the longest, shortest, and the time difference between delivery times for all orders.\n\n\n6. What was the average speed for each runner for each delivery and do you notice – any trend for these values?\n\n\nwith cte as (\n    select runner_id,\n        case\n            when distance ~ '.*' THEN cast(substring(distance, '[0-9\\-+\\.]+') as float)\n            else null\n        end as cleaned_distance_km,\n        case\n            when duration ~ '.*' THEN cast(substring(duration, '[0-9\\-+\\.]+') as float) / 60\n            else null\n        end as cleaned_duration_hr\n    from runner_orders\n)\nselect runner_id,\n    avg(cleaned_distance_km / cleaned_duration_hr) as speed_km_hr\nfrom cte\ngroup by 1;\n\n\n3 records\n\n\nrunner_id\nspeed_km_hr\n\n\n\n\n3\n40.00000\n\n\n2\n62.90000\n\n\n1\n45.53611\n\n\n\n\n\nThis psql query calculates the average speed for each runner for each delivery. It uses a CTE named cte to clean and extract relevant information such as cleaned_distance_km and cleaned_duration_hr from the runner_orders table. The main query then computes the average speed (cleaned_distance_km / cleaned_duration_hr) for each runner and presents the results grouped by runner_id. This allows you to observe trends in the average speed of each runner across their deliveries.\n\n\n7. What is the successful delivery percentage for each runner?\n\nwith part as (\n    select cte.runner_id,\n        count(*) as part_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    where cancel is null\n    group by runner_id\n),\nwhole as (\n    select runner_id,\n        count(*) as whole_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    group by runner_id\n)\nselect p.runner_id,\n    case\n        when w.whole_cancel = 0 then null\n        else round(\n            (p.part_cancel::numeric / w.whole_cancel) * 100,\n            2\n        )\n    end as percent\nfrom part p\n    inner join whole w on p.runner_id = w.runner_id;\n\n\n3 records\n\n\nrunner_id\npercent\n\n\n\n\n3\n50\n\n\n2\n75\n\n\n1\n100\n\n\n\n\n\n\n\n\nC. Ingredient Optimisation\n\n1. What are the standard ingredients for each pizza?\n\n\nselect pn.pizza_name, pt.topping_name\nfrom pizza_names pn inner join new_pizza_recipes np\non pn.pizza_id = np.pizza_id\ninner join pizza_toppings pt on pt.topping_id::text = ANY (np.toppings)\n\n\nDisplaying records 1 - 10\n\n\npizza_name\ntopping_name\n\n\n\n\nMeatlovers\nBacon\n\n\nMeatlovers\nBBQ Sauce\n\n\nMeatlovers\nBeef\n\n\nMeatlovers\nCheese\n\n\nMeatlovers\nChicken\n\n\nMeatlovers\nMushrooms\n\n\nMeatlovers\nPepperoni\n\n\nMeatlovers\nSalami\n\n\nVegetarian\nCheese\n\n\nVegetarian\nMushrooms\n\n\n\n\n\n\n\n\nD. Pricing and Ratings\n\n1. If a Meat Lovers pizza costs $12 and Vegetarian costs $10 and there were no charges for changes, how much money has Pizza Runner made so far if there are no delivery fees?\n\n-- Creating a view of cleaned data\nCREATE OR REPLACE VIEW clean_runner_orders AS\nselect order_id,\n    runner_id,\n    CASE\n        WHEN pickup_time = 'null' THEN NULL\n        ELSE pickup_time::TIMESTAMP\n    END,\n    cast(substring(distance, '[0-9\\-+\\.]+') as float) as distance,\n    cast(substring(duration, '[0-9\\-+\\.]+') as float) as duration,\n    nullif(cancellation, '') || nullif(cancellation, 'null') as cancellation\nfrom runner_orders;\n--\n\n\nwith cte1 as (\n    select co.pizza_id,\n        count(co.pizza_id) as quantity_sold\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n    GROUP BY 1\n)\nselect pizza_id,\n    quantity_sold * price as revenue\nfrom (\n        select *,\n            CASE\n                WHEN cte1.pizza_id = 1 THEN 12\n                WHEN cte1.pizza_id = 2 THEN 10\n            END AS price\n        from cte1\n    ) sq\n\n\n2 records\n\n\npizza_id\nrevenue\n\n\n\n\n1\n108\n\n\n2\n30\n\n\n\n\n\n\n-- Creating a random integer generator\ncreate or replace function random_between(low int, high int) returns int as $$ begin return floor(random() * (high - low + 1) + low);\nend;\n$$ language 'plpgsql' STRICT;\n\n\n\n2. If a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre travelled, how much money does Pizza Runner have left over after these deliveries?\n\nwith cte as (\n    select co.pizza_id,\n        ro.distance * 0.3 as runner_cost,\n        CASE\n            WHEN co.pizza_id = 1 THEN 12\n            WHEN co.pizza_id = 2 THEN 10\n        END AS price\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n)\nselect sum(price) revenue,\n    round(sum(runner_cost)::numeric, 2) cost,\n    round(sum(price) - sum(runner_cost)::numeric, 2) profit\nfrom cte\n\n\n1 records\n\n\nrevenue\ncost\nprofit\n\n\n\n\n138\n64.62\n73.38"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#closing-the-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#closing-the-connection",
    "title": "Pizza Runner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html",
    "title": "Data Bank",
    "section": "",
    "text": "Data Bank runs just like any other digital bank - but with a distributed data storage platform. Customers are allocated cloud data storage limits which are directly linked to how much money they have in their accounts. The management team at Data Bank want to increase their total customer base - but also need some help tracking just how much data storage their customers will need. This case study is all about calculating metrics, growth and helping the business analyse their data in a smart way to better forecast and plan for their future developments!\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"data_bank\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "title": "Data Bank",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#database-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#database-connection",
    "title": "Data Bank",
    "section": "Database Connection",
    "text": "Database Connection"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#queries",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#queries",
    "title": "Data Bank",
    "section": "Queries",
    "text": "Queries"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html",
    "href": "posts/2023-11-21-BDA-1/index.html",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "",
    "text": "#|message: false\n#|wwarning: false\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(boot)\n\nWarning: package 'boot' was built under R version 4.3.2\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.2\n\nlibrary(broom)\n# library(boot) #Required for Bootstrap simulations\n# library(rstudioapi)"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#variable-description",
    "href": "posts/2023-11-21-BDA-1/index.html#variable-description",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Variable description",
    "text": "Variable description"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "href": "posts/2023-11-21-BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Why Correlation Is Not Causation: A Confounder in Action",
    "text": "Why Correlation Is Not Causation: A Confounder in Action\n\n#Reading the data\nstand_tib &lt;- read_csv(\"chap1-stand_data.csv\")\n\nRows: 2400 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): icecream_sales, malt_sales, summer_months, temps\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(stand_tib) %&gt;% gt() %&gt;% opt_stylize(style = 6, color=\"cyan\")\n\n\n\n\n\n  \n    \n    \n      icecream_sales\n      malt_sales\n      summer_months\n      temps\n    \n  \n  \n    25649.78\n28592.18\n0\n28.59314\n    32694.93\n37152.84\n0\n37.13064\n    26467.80\n24074.19\n0\n24.11190\n    43438.79\n49169.47\n0\n49.21807\n    52452.63\n47249.40\n0\n47.25766\n    34130.66\n34228.50\n0\n34.25746\n  \n  \n  \n\n\n\n\n\n#Running linear regressions \n#Biased model (coeff should be 1,000)\nlm(icecream_sales ~ temps, data = stand_tib) %&gt;% \n  tidy() %&gt;% \n  gt() %&gt;% opt_stylize(style = 6, color =\"cyan\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-6169.844\n531.5062\n-11.60823\n2.385706e-30\n    temps\n1171.335\n9.0268\n129.76197\n0.000000e+00\n  \n  \n  \n\n\n\n\n\n#correct model for icecream (coeffs should be 1,000 and 20,000)\nsummary(lm(icecream_sales ~ temps + summer_months, data=stand_tib)) %&gt;% \n  tidy() %&gt;% \n  gt() %&gt;% opt_stylize(style = 6, color =\"cyan\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-7.068995\n374.401378\n-0.01888079\n0.9849378\n    temps\n1001.481846\n6.822242\n146.79657789\n0.0000000\n    summer_months\n19556.774716\n361.393093\n54.11496531\n0.0000000\n  \n  \n  \n\n\n\n\n\n#Model biased by extra controlling\n#note how the standard error of the estimate of temp increases a lot\n#note how the p-values for \nsummary(lm(icecream_sales ~ malt_sales + temps + summer_months, \n             data = stand_tib)) %&gt;% \n  tidy() %&gt;% \n  gt() %&gt;% opt_stylize(style = 6, color =\"cyan\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-15.827140\n374.580924\n-0.04225293\n0.9663006\n    malt_sales\n-1.701081\n2.082874\n-0.81669912\n0.4141816\n    temps\n2702.788460\n2083.160984\n1.29744580\n0.1946027\n    summer_months\n19548.167641\n361.571822\n54.06441117\n0.0000000\n  \n  \n  \n\n\n\n\n\nggplot(stand_tib, aes(x=icecream_sales, y=malt_sales)) + \n    geom_point() + labs(x='Malt sales', y='Iced coffee sales') + \n    theme_classic()"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#second-example",
    "href": "posts/2023-11-21-BDA-1/index.html#second-example",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Second Example",
    "text": "Second Example\n\n#Reading the data\nsurvey_tib &lt;- read_csv(\"chap1-survey_data.csv\")\n\nRows: 10000 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): vanilla, chocolate, shopped\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Reformatting shopped variable to binary\nsurvey_tib &lt;- survey_tib %&gt;%\n  mutate(shopped = as.factor(shopped) %&gt;% relevel(ref=\"0\"))\n\n\n## Figure 1-5. (a) Tastes for vanilla and chocolate are uncorrelated in the \n## overall population. (b) Taste for vanilla is higher for people who shop at \n## the ice-cream stand than for people who don’t. (c) Same thing with the taste for chocolate.\n\n#Scatterplot of chocolate versus vanilla taste \na &lt;- ggplot(survey_tib, aes(x=vanilla, y=chocolate)) + geom_point() +\n  xlim(c(0,28)) + ylim(c(0,28)) + geom_smooth(formula = 'y~x', method = lm, se = FALSE) +\n  labs(x='Taste for vanilla', y='Taste for chocolate') + theme_minimal()\n\n#Boxplot of vanilla taste against shopping behavior\nb &lt;- ggplot(survey_tib, aes(shopped, vanilla)) + geom_boxplot() +\n  labs(x='Shopped (Y/N)', y='Taste for vanilla') + ylim(c(0,30)) + theme_minimal()\n\n#Boxplot of chocolate taste against shopping behavior\nc &lt;- ggplot(survey_tib, aes(shopped, chocolate)) + geom_boxplot() +\n  labs(x='Shopped (Y/N)', y='Taste for chocolate') + ylim(c(0,30)) + theme_minimal()\n\nggpubr::ggarrange(a,b,c, ncol = 3)\n\n\n\n\n\n\n\n### Plotting same scatterplot but for shoppers only\n\n#Figure 1-6 Taste for vanilla and chocolate among shoppers. \nd &lt;- ggplot(survey_tib %&gt;% filter(shopped=='1'), aes(x=vanilla, y=chocolate)) + geom_point() + \n  labs(x='Taste for vanilla', y='Taste for chocolate') + geom_smooth(formula='y~x', method = lm,se = FALSE) + theme_minimal()\nd\n\n\n\n\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"pizza_runner\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#queries",
    "href": "posts/2023-11-21-BDA-1/index.html#queries",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Queries",
    "text": "Queries\n\nA. Pizza Metrics\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\nselect count(pizza_id)\nfrom customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n14\n\n\n\n\n\n\n\n2. How many unique customer orders were made?\n\n\nSELECT count(DISTINCT order_id) from customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n10\n\n\n\n\n\n\n\n3. How many successful orders were delivered by each runner?\n\n\nselect runner_id,\n    count(order_id) as count_of_successful_orders\nfrom runner_orders\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP BY runner_id;\n\n\n3 records\n\n\nrunner_id\ncount_of_successful_orders\n\n\n\n\n3\n1\n\n\n2\n3\n\n\n1\n4\n\n\n\n\n\n\n\n4. How many of each type of pizza was delivered?\n\n\nselect pn.pizza_name as pizza,\n    count(co.pizza_id) as count_of_pizza_delivered\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id\n    INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP by pn.pizza_name;\n\n\n2 records\n\n\npizza\ncount_of_pizza_delivered\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\n\n5. How many Vegetarian and Meatlovers were ordered by each customer?\n\n\nselect co.customer_id as customers, count(co.pizza_id) as count_of_pizza_ordered from customer_orders co INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id group by 1;\n\n\n5 records\n\n\ncustomers\ncount_of_pizza_ordered\n\n\n\n\n101\n3\n\n\n103\n4\n\n\n104\n3\n\n\n105\n1\n\n\n102\n3\n\n\n\n\n\n\n\n6. What was the maximum number of pizzas delivered in a single order?\n\n\nselect ro.order_id as order,\n    count(co.pizza_id) as number_of_pizzas\nfrom runner_orders ro\n    INNER JOIN customer_orders co ON ro.order_id = co.order_id\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 1;\n\n\n1 records\n\n\norder\nnumber_of_pizzas\n\n\n\n\n4\n3\n\n\n\n\n\n\n\n7. For each customer, how many delivered pizzas had at least 1 change and how many? and no changes?\n\n-- PART 1\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with at least 1 changes\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n        or (\n            exclusions not in ('', 'null')\n            or extras not in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n4 records\n\n\ncustomer_id\ndelivered_pizzas_with_changes\n\n\n\n\n102\n1\n\n\n105\n1\n\n\n104\n3\n\n\n103\n3\n\n\n\n\n\n\n-- PART 2: orders with no changes\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions = ''\n        and extras = ''\n        or (\n            exclusions in ('', 'null')\n            or extras in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_no_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n5 records\n\n\ncustomer_id\ndelivered_pizzas_with_no_changes\n\n\n\n\n101\n2\n\n\n102\n3\n\n\n103\n3\n\n\n104\n2\n\n\n105\n1\n\n\n\n\n\n\n\n8. How many pizzas were delivered that had both exclusions and extras?\n\n\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with both exclusions and extras\ncte2 as (\n    select *\n    from customer_orders\n    where (\n            exclusions &lt;&gt; 'null'\n            and extras &lt;&gt; 'null'\n        )\n        and exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n)\nselect *\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id;\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\norder_id..7\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10minutes\nnull\n10\n104\n1\n2, 6\n1, 4\n2020-01-11 18:34:49\n\n\n\n\n\n\n\n9. What was the total volume of pizzas ordered for each hour of the day?\n\n\nselect EXTRACT (\n        hour\n        from order_time\n    ) as hour_of_day,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n6 records\n\n\nhour_of_day\npizza_volume\n\n\n\n\n18\n3\n\n\n21\n3\n\n\n23\n3\n\n\n13\n3\n\n\n19\n1\n\n\n11\n1\n\n\n\n\n\n\n\n10. What was the volume of orders for each day of the week?\n\n-- select EXTRACT (\n--        dow\n--        from order_time\n--    ) as day_of_week,\n--    count(pizza_id) as pizza_volume\n-- from customer_orders\n-- GROUP BY 1;\n-- or\nselect to_char(order_time, 'Day') as day_of_week,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n4 records\n\n\nday_of_week\npizza_volume\n\n\n\n\nSaturday\n5\n\n\nThursday\n3\n\n\nFriday\n1\n\n\nWednesday\n5\n\n\n\n\n\n\n\n\nB. Runner and Customer Experience\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\n\nselect EXTRACT(\n        week\n        from registration_date\n    ),\n    count(runner_id)\nfrom runners\nGROUP BY 1;\n\n\n3 records\n\n\nextract\ncount\n\n\n\n\n1\n1\n\n\n53\n2\n\n\n2\n1\n\n\n\n\n\n\n\n2. What was the average time in minutes it took for each runner to arrive at the Pizza – Runner HQ to pickup the order?\n\nselect case\n        when ro.pickup_time = 'null' then null\n        else round(\n            EXTRACT(\n                epoch\n                from (\n                        ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                    )\n            ) / 60,\n            2\n        )\n    end as duration_till_pickup,\n    ro.pickup_time,\n    co.order_time\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id; \n\n\nDisplaying records 1 - 10\n\n\nduration_till_pickup\npickup_time\norder_time\n\n\n\n\n10.53\n2020-01-01 18:15:34\n2020-01-01 18:05:02\n\n\n10.03\n2020-01-01 19:10:54\n2020-01-01 19:00:52\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n10.47\n2020-01-08 21:10:57\n2020-01-08 21:00:29\n\n\nNA\nnull\n2020-01-08 21:03:13\n\n\n10.27\n2020-01-08 21:30:45\n2020-01-08 21:20:29\n\n\n\n\n\nThis query calculates the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pick up the order. It first joins the customer_orders and runner_orders tables to get the order information and the runner’s pickup time. Then, it checks if the pickup_time is null, and if so, it sets the corresponding duration_till_pickup to null. Otherwise, it calculates the time difference between the pickup_time and the order_time in seconds using the EXTRACT() and TIMESTAMP()functions. It then converts the time difference to minutes and rounds it to two decimal places. Finally, it selects the duration_till_pickup, pickup_time, and order_time for each order.\n\n\n3. What was the average time in minutes it took for each runner to deliver pizzas?\n\n\nselect runner_id,\n    round(\n        avg(\n            case\n                when left(duration, 2) ~ '^\\d+$' THEN cast(left(duration, 2) as integer)\n                else null\n            end\n        ),\n        2\n    ) as extracted_minutes\nfrom runner_orders\ngroup by runner_id;\n\n\n3 records\n\n\nrunner_id\nextracted_minutes\n\n\n\n\n3\n15.00\n\n\n2\n26.67\n\n\n1\n22.25\n\n\n\n\n\n\n\n5. What was the difference between the longest and shortest delivery times for all?\n\n\n-- If we define delivery times as the duration between ro.pickup_time - co.order_time + ro.duration\n-- then:\nwith cte1 as (\n    select case\n            when ro.pickup_time = 'null' then null\n            else round(\n                EXTRACT(\n                    epoch\n                    from (\n                            ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                        )\n                ) / 60,\n                2\n            )\n        end as duration_till_pickup,\n        ro.pickup_time,\n        co.order_time,\n        round(\n            case\n                when left(ro.duration, 2) ~ '^\\d+$' THEN cast(left(ro.duration, 2) as integer)\n                else null\n            end,\n            2\n        ) as cleaned_duration_minutes\n    from customer_orders co\n        INNER JOIN runner_orders ro on co.order_id = ro.order_id\n)\nselect max(duration_till_pickup + cleaned_duration_minutes) as longest_delivery_time,\n    min(duration_till_pickup + cleaned_duration_minutes) as shortest_delivery_time,\n    max(duration_till_pickup + cleaned_duration_minutes) - min(duration_till_pickup + cleaned_duration_minutes) as difference\nfrom cte1;\n\n\n1 records\n\n\nlongest_delivery_time\nshortest_delivery_time\ndifference\n\n\n\n\n69.28\n25.47\n43.81\n\n\n\n\n\nNow, this is a monstrous looking one 😅 .\nA (CTE) named cte1 is created, extracting relevant information such as duration_till_pickup, pickup_time, order_time, and cleaned_duration_minutes from the customer_orders (co) and runner_orders (ro) tables. The duration_till_pickup is calculated as the time between pickup and order in minutes. The cleaned_duration_minutes extracts the duration in minutes from the ro.duration field.\nThe main query then computes the maximum, minimum, and the difference between the sum of duration_till_pickup and cleaned_duration_minutes. These values represent the longest, shortest, and the time difference between delivery times for all orders.\n\n\n6. What was the average speed for each runner for each delivery and do you notice – any trend for these values?\n\n\nwith cte as (\n    select runner_id,\n        case\n            when distance ~ '.*' THEN cast(substring(distance, '[0-9\\-+\\.]+') as float)\n            else null\n        end as cleaned_distance_km,\n        case\n            when duration ~ '.*' THEN cast(substring(duration, '[0-9\\-+\\.]+') as float) / 60\n            else null\n        end as cleaned_duration_hr\n    from runner_orders\n)\nselect runner_id,\n    avg(cleaned_distance_km / cleaned_duration_hr) as speed_km_hr\nfrom cte\ngroup by 1;\n\n\n3 records\n\n\nrunner_id\nspeed_km_hr\n\n\n\n\n3\n40.00000\n\n\n2\n62.90000\n\n\n1\n45.53611\n\n\n\n\n\nThis psql query calculates the average speed for each runner for each delivery. It uses a CTE named cte to clean and extract relevant information such as cleaned_distance_km and cleaned_duration_hr from the runner_orders table. The main query then computes the average speed (cleaned_distance_km / cleaned_duration_hr) for each runner and presents the results grouped by runner_id. This allows you to observe trends in the average speed of each runner across their deliveries.\n\n\n7. What is the successful delivery percentage for each runner?\n\nwith part as (\n    select cte.runner_id,\n        count(*) as part_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    where cancel is null\n    group by runner_id\n),\nwhole as (\n    select runner_id,\n        count(*) as whole_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    group by runner_id\n)\nselect p.runner_id,\n    case\n        when w.whole_cancel = 0 then null\n        else round(\n            (p.part_cancel::numeric / w.whole_cancel) * 100,\n            2\n        )\n    end as percent\nfrom part p\n    inner join whole w on p.runner_id = w.runner_id;\n\n\n3 records\n\n\nrunner_id\npercent\n\n\n\n\n3\n50\n\n\n2\n75\n\n\n1\n100\n\n\n\n\n\n\n\n\nC. Ingredient Optimisation\n\n1. What are the standard ingredients for each pizza?\n\n\nselect pn.pizza_name, pt.topping_name\nfrom pizza_names pn inner join new_pizza_recipes np\non pn.pizza_id = np.pizza_id\ninner join pizza_toppings pt on pt.topping_id::text = ANY (np.toppings)\n\n\nDisplaying records 1 - 10\n\n\npizza_name\ntopping_name\n\n\n\n\nMeatlovers\nBacon\n\n\nMeatlovers\nBBQ Sauce\n\n\nMeatlovers\nBeef\n\n\nMeatlovers\nCheese\n\n\nMeatlovers\nChicken\n\n\nMeatlovers\nMushrooms\n\n\nMeatlovers\nPepperoni\n\n\nMeatlovers\nSalami\n\n\nVegetarian\nCheese\n\n\nVegetarian\nMushrooms\n\n\n\n\n\n\n\n\nD. Pricing and Ratings\n\n1. If a Meat Lovers pizza costs $12 and Vegetarian costs $10 and there were no charges for changes, how much money has Pizza Runner made so far if there are no delivery fees?\n\n-- Creating a view of cleaned data\nCREATE OR REPLACE VIEW clean_runner_orders AS\nselect order_id,\n    runner_id,\n    CASE\n        WHEN pickup_time = 'null' THEN NULL\n        ELSE pickup_time::TIMESTAMP\n    END,\n    cast(substring(distance, '[0-9\\-+\\.]+') as float) as distance,\n    cast(substring(duration, '[0-9\\-+\\.]+') as float) as duration,\n    nullif(cancellation, '') || nullif(cancellation, 'null') as cancellation\nfrom runner_orders;\n--\n\n\nwith cte1 as (\n    select co.pizza_id,\n        count(co.pizza_id) as quantity_sold\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n    GROUP BY 1\n)\nselect pizza_id,\n    quantity_sold * price as revenue\nfrom (\n        select *,\n            CASE\n                WHEN cte1.pizza_id = 1 THEN 12\n                WHEN cte1.pizza_id = 2 THEN 10\n            END AS price\n        from cte1\n    ) sq\n\n\n2 records\n\n\npizza_id\nrevenue\n\n\n\n\n1\n108\n\n\n2\n30\n\n\n\n\n\n\n-- Creating a random integer generator\ncreate or replace function random_between(low int, high int) returns int as $$ begin return floor(random() * (high - low + 1) + low);\nend;\n$$ language 'plpgsql' STRICT;\n\n\n\n2. If a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre travelled, how much money does Pizza Runner have left over after these deliveries?\n\nwith cte as (\n    select co.pizza_id,\n        ro.distance * 0.3 as runner_cost,\n        CASE\n            WHEN co.pizza_id = 1 THEN 12\n            WHEN co.pizza_id = 2 THEN 10\n        END AS price\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n)\nselect sum(price) revenue,\n    round(sum(runner_cost)::numeric, 2) cost,\n    round(sum(price) - sum(runner_cost)::numeric, 2) profit\nfrom cte\n\n\n1 records\n\n\nrevenue\ncost\nprofit\n\n\n\n\n138\n64.62\n73.38"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#closing-the-connection",
    "href": "posts/2023-11-21-BDA-1/index.html#closing-the-connection",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html",
    "title": "Data Bank",
    "section": "",
    "text": "Data Bank runs just like any other digital bank - but with a distributed data storage platform. Customers are allocated cloud data storage limits which are directly linked to how much money they have in their accounts. The management team at Data Bank want to increase their total customer base - but also need some help tracking just how much data storage their customers will need. This case study is all about calculating metrics, growth and helping the business analyse their data in a smart way to better forecast and plan for their future developments!\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"data_bank\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "title": "Data Bank",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#database-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#database-connection",
    "title": "Data Bank",
    "section": "Database Connection",
    "text": "Database Connection"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#queries",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#queries",
    "title": "Data Bank",
    "section": "Queries",
    "text": "Queries"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#introduction",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#introduction",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "title": "Pizza Runner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#database-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#database-connection",
    "title": "Pizza Runner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"pizza_runner\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#queries",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#queries",
    "title": "Pizza Runner",
    "section": "Queries",
    "text": "Queries\n\nA. Pizza Metrics\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\nselect count(pizza_id)\nfrom customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n14\n\n\n\n\n\n\n\n2. How many unique customer orders were made?\n\n\nSELECT count(DISTINCT order_id) from customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n10\n\n\n\n\n\n\n\n3. How many successful orders were delivered by each runner?\n\n\nselect runner_id,\n    count(order_id) as count_of_successful_orders\nfrom runner_orders\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP BY runner_id;\n\n\n3 records\n\n\nrunner_id\ncount_of_successful_orders\n\n\n\n\n3\n1\n\n\n2\n3\n\n\n1\n4\n\n\n\n\n\n\n\n4. How many of each type of pizza was delivered?\n\n\nselect pn.pizza_name as pizza,\n    count(co.pizza_id) as count_of_pizza_delivered\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id\n    INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP by pn.pizza_name;\n\n\n2 records\n\n\npizza\ncount_of_pizza_delivered\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\n\n5. How many Vegetarian and Meatlovers were ordered by each customer?\n\n\nselect co.customer_id as customers, count(co.pizza_id) as count_of_pizza_ordered from customer_orders co INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id group by 1;\n\n\n5 records\n\n\ncustomers\ncount_of_pizza_ordered\n\n\n\n\n101\n3\n\n\n103\n4\n\n\n104\n3\n\n\n105\n1\n\n\n102\n3\n\n\n\n\n\n\n\n6. What was the maximum number of pizzas delivered in a single order?\n\n\nselect ro.order_id as order,\n    count(co.pizza_id) as number_of_pizzas\nfrom runner_orders ro\n    INNER JOIN customer_orders co ON ro.order_id = co.order_id\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 1;\n\n\n1 records\n\n\norder\nnumber_of_pizzas\n\n\n\n\n4\n3\n\n\n\n\n\n\n\n7. For each customer, how many delivered pizzas had at least 1 change and how many? and no changes?\n\n-- PART 1\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with at least 1 changes\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n        or (\n            exclusions not in ('', 'null')\n            or extras not in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n4 records\n\n\ncustomer_id\ndelivered_pizzas_with_changes\n\n\n\n\n102\n1\n\n\n105\n1\n\n\n104\n3\n\n\n103\n3\n\n\n\n\n\n\n-- PART 2: orders with no changes\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions = ''\n        and extras = ''\n        or (\n            exclusions in ('', 'null')\n            or extras in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_no_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n5 records\n\n\ncustomer_id\ndelivered_pizzas_with_no_changes\n\n\n\n\n101\n2\n\n\n102\n3\n\n\n103\n3\n\n\n104\n2\n\n\n105\n1\n\n\n\n\n\n\n\n8. How many pizzas were delivered that had both exclusions and extras?\n\n\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with both exclusions and extras\ncte2 as (\n    select *\n    from customer_orders\n    where (\n            exclusions &lt;&gt; 'null'\n            and extras &lt;&gt; 'null'\n        )\n        and exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n)\nselect *\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id;\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\norder_id..7\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10minutes\nnull\n10\n104\n1\n2, 6\n1, 4\n2020-01-11 18:34:49\n\n\n\n\n\n\n\n9. What was the total volume of pizzas ordered for each hour of the day?\n\n\nselect EXTRACT (\n        hour\n        from order_time\n    ) as hour_of_day,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n6 records\n\n\nhour_of_day\npizza_volume\n\n\n\n\n18\n3\n\n\n21\n3\n\n\n23\n3\n\n\n13\n3\n\n\n19\n1\n\n\n11\n1\n\n\n\n\n\n\n\n10. What was the volume of orders for each day of the week?\n\n-- select EXTRACT (\n--        dow\n--        from order_time\n--    ) as day_of_week,\n--    count(pizza_id) as pizza_volume\n-- from customer_orders\n-- GROUP BY 1;\n-- or\nselect to_char(order_time, 'Day') as day_of_week,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n4 records\n\n\nday_of_week\npizza_volume\n\n\n\n\nSaturday\n5\n\n\nThursday\n3\n\n\nFriday\n1\n\n\nWednesday\n5\n\n\n\n\n\n\n\n\nB. Runner and Customer Experience\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\n\nselect EXTRACT(\n        week\n        from registration_date\n    ),\n    count(runner_id)\nfrom runners\nGROUP BY 1;\n\n\n3 records\n\n\nextract\ncount\n\n\n\n\n1\n1\n\n\n53\n2\n\n\n2\n1\n\n\n\n\n\n\n\n2. What was the average time in minutes it took for each runner to arrive at the Pizza – Runner HQ to pickup the order?\n\nselect case\n        when ro.pickup_time = 'null' then null\n        else round(\n            EXTRACT(\n                epoch\n                from (\n                        ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                    )\n            ) / 60,\n            2\n        )\n    end as duration_till_pickup,\n    ro.pickup_time,\n    co.order_time\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id; \n\n\nDisplaying records 1 - 10\n\n\nduration_till_pickup\npickup_time\norder_time\n\n\n\n\n10.53\n2020-01-01 18:15:34\n2020-01-01 18:05:02\n\n\n10.03\n2020-01-01 19:10:54\n2020-01-01 19:00:52\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n10.47\n2020-01-08 21:10:57\n2020-01-08 21:00:29\n\n\nNA\nnull\n2020-01-08 21:03:13\n\n\n10.27\n2020-01-08 21:30:45\n2020-01-08 21:20:29\n\n\n\n\n\nThis query calculates the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pick up the order. It first joins the customer_orders and runner_orders tables to get the order information and the runner’s pickup time. Then, it checks if the pickup_time is null, and if so, it sets the corresponding duration_till_pickup to null. Otherwise, it calculates the time difference between the pickup_time and the order_time in seconds using the EXTRACT() and TIMESTAMP()functions. It then converts the time difference to minutes and rounds it to two decimal places. Finally, it selects the duration_till_pickup, pickup_time, and order_time for each order.\n\n\n3. What was the average time in minutes it took for each runner to deliver pizzas?\n\n\nselect runner_id,\n    round(\n        avg(\n            case\n                when left(duration, 2) ~ '^\\d+$' THEN cast(left(duration, 2) as integer)\n                else null\n            end\n        ),\n        2\n    ) as extracted_minutes\nfrom runner_orders\ngroup by runner_id;\n\n\n3 records\n\n\nrunner_id\nextracted_minutes\n\n\n\n\n3\n15.00\n\n\n2\n26.67\n\n\n1\n22.25\n\n\n\n\n\n\n\n5. What was the difference between the longest and shortest delivery times for all?\n\n\n-- If we define delivery times as the duration between ro.pickup_time - co.order_time + ro.duration\n-- then:\nwith cte1 as (\n    select case\n            when ro.pickup_time = 'null' then null\n            else round(\n                EXTRACT(\n                    epoch\n                    from (\n                            ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                        )\n                ) / 60,\n                2\n            )\n        end as duration_till_pickup,\n        ro.pickup_time,\n        co.order_time,\n        round(\n            case\n                when left(ro.duration, 2) ~ '^\\d+$' THEN cast(left(ro.duration, 2) as integer)\n                else null\n            end,\n            2\n        ) as cleaned_duration_minutes\n    from customer_orders co\n        INNER JOIN runner_orders ro on co.order_id = ro.order_id\n)\nselect max(duration_till_pickup + cleaned_duration_minutes) as longest_delivery_time,\n    min(duration_till_pickup + cleaned_duration_minutes) as shortest_delivery_time,\n    max(duration_till_pickup + cleaned_duration_minutes) - min(duration_till_pickup + cleaned_duration_minutes) as difference\nfrom cte1;\n\n\n1 records\n\n\nlongest_delivery_time\nshortest_delivery_time\ndifference\n\n\n\n\n69.28\n25.47\n43.81\n\n\n\n\n\nNow, this is a monstrous looking one 😅 .\nA (CTE) named cte1 is created, extracting relevant information such as duration_till_pickup, pickup_time, order_time, and cleaned_duration_minutes from the customer_orders (co) and runner_orders (ro) tables. The duration_till_pickup is calculated as the time between pickup and order in minutes. The cleaned_duration_minutes extracts the duration in minutes from the ro.duration field.\nThe main query then computes the maximum, minimum, and the difference between the sum of duration_till_pickup and cleaned_duration_minutes. These values represent the longest, shortest, and the time difference between delivery times for all orders.\n\n\n6. What was the average speed for each runner for each delivery and do you notice – any trend for these values?\n\n\nwith cte as (\n    select runner_id,\n        case\n            when distance ~ '.*' THEN cast(substring(distance, '[0-9\\-+\\.]+') as float)\n            else null\n        end as cleaned_distance_km,\n        case\n            when duration ~ '.*' THEN cast(substring(duration, '[0-9\\-+\\.]+') as float) / 60\n            else null\n        end as cleaned_duration_hr\n    from runner_orders\n)\nselect runner_id,\n    avg(cleaned_distance_km / cleaned_duration_hr) as speed_km_hr\nfrom cte\ngroup by 1;\n\n\n3 records\n\n\nrunner_id\nspeed_km_hr\n\n\n\n\n3\n40.00000\n\n\n2\n62.90000\n\n\n1\n45.53611\n\n\n\n\n\nThis psql query calculates the average speed for each runner for each delivery. It uses a CTE named cte to clean and extract relevant information such as cleaned_distance_km and cleaned_duration_hr from the runner_orders table. The main query then computes the average speed (cleaned_distance_km / cleaned_duration_hr) for each runner and presents the results grouped by runner_id. This allows you to observe trends in the average speed of each runner across their deliveries.\n\n\n7. What is the successful delivery percentage for each runner?\n\nwith part as (\n    select cte.runner_id,\n        count(*) as part_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    where cancel is null\n    group by runner_id\n),\nwhole as (\n    select runner_id,\n        count(*) as whole_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    group by runner_id\n)\nselect p.runner_id,\n    case\n        when w.whole_cancel = 0 then null\n        else round(\n            (p.part_cancel::numeric / w.whole_cancel) * 100,\n            2\n        )\n    end as percent\nfrom part p\n    inner join whole w on p.runner_id = w.runner_id;\n\n\n3 records\n\n\nrunner_id\npercent\n\n\n\n\n3\n50\n\n\n2\n75\n\n\n1\n100\n\n\n\n\n\n\n\n\nC. Ingredient Optimisation\n\n1. What are the standard ingredients for each pizza?\n\n\nselect pn.pizza_name, pt.topping_name\nfrom pizza_names pn inner join new_pizza_recipes np\non pn.pizza_id = np.pizza_id\ninner join pizza_toppings pt on pt.topping_id::text = ANY (np.toppings)\n\n\nDisplaying records 1 - 10\n\n\npizza_name\ntopping_name\n\n\n\n\nMeatlovers\nBacon\n\n\nMeatlovers\nBBQ Sauce\n\n\nMeatlovers\nBeef\n\n\nMeatlovers\nCheese\n\n\nMeatlovers\nChicken\n\n\nMeatlovers\nMushrooms\n\n\nMeatlovers\nPepperoni\n\n\nMeatlovers\nSalami\n\n\nVegetarian\nCheese\n\n\nVegetarian\nMushrooms\n\n\n\n\n\n\n\n\nD. Pricing and Ratings\n\n1. If a Meat Lovers pizza costs $12 and Vegetarian costs $10 and there were no charges for changes, how much money has Pizza Runner made so far if there are no delivery fees?\n\n-- Creating a view of cleaned data\nCREATE OR REPLACE VIEW clean_runner_orders AS\nselect order_id,\n    runner_id,\n    CASE\n        WHEN pickup_time = 'null' THEN NULL\n        ELSE pickup_time::TIMESTAMP\n    END,\n    cast(substring(distance, '[0-9\\-+\\.]+') as float) as distance,\n    cast(substring(duration, '[0-9\\-+\\.]+') as float) as duration,\n    nullif(cancellation, '') || nullif(cancellation, 'null') as cancellation\nfrom runner_orders;\n--\n\n\nwith cte1 as (\n    select co.pizza_id,\n        count(co.pizza_id) as quantity_sold\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n    GROUP BY 1\n)\nselect pizza_id,\n    quantity_sold * price as revenue\nfrom (\n        select *,\n            CASE\n                WHEN cte1.pizza_id = 1 THEN 12\n                WHEN cte1.pizza_id = 2 THEN 10\n            END AS price\n        from cte1\n    ) sq\n\n\n2 records\n\n\npizza_id\nrevenue\n\n\n\n\n1\n108\n\n\n2\n30\n\n\n\n\n\n\n-- Creating a random integer generator\ncreate or replace function random_between(low int, high int) returns int as $$ begin return floor(random() * (high - low + 1) + low);\nend;\n$$ language 'plpgsql' STRICT;\n\n\n\n2. If a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre travelled, how much money does Pizza Runner have left over after these deliveries?\n\nwith cte as (\n    select co.pizza_id,\n        ro.distance * 0.3 as runner_cost,\n        CASE\n            WHEN co.pizza_id = 1 THEN 12\n            WHEN co.pizza_id = 2 THEN 10\n        END AS price\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n)\nselect sum(price) revenue,\n    round(sum(runner_cost)::numeric, 2) cost,\n    round(sum(price) - sum(runner_cost)::numeric, 2) profit\nfrom cte\n\n\n1 records\n\n\nrevenue\ncost\nprofit\n\n\n\n\n138\n64.62\n73.38"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#closing-the-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#closing-the-connection",
    "title": "Pizza Runner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#introduction",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#introduction",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "title": "Danny’s Diner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram\n\n\nSales\nThe sales table captures all customer_id level purchases with an corresponding order_date and product_id information for when and what menu items were ordered.\n\n\nMenu\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\n\n\nMembers\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\nkey concepts: CTEs, window functions"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#database-connection",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#database-connection",
    "title": "Danny’s Diner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"danny_diners\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#queries",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#queries",
    "title": "Danny’s Diner",
    "section": "Queries",
    "text": "Queries\nNow let’s convert business questions into SQL queries!\n\n1. What is the total amount each customer spent at the restaurant?\n\n\nselect s.customer_id as customer,\n   sum(m.price) as total_amount\nfrom sales s\n   inner join menu m on s.product_id = m.product_id\ngroup by customer;\n\n\n3 records\n\n\ncustomer\ntotal_amount\n\n\n\n\nB\n74\n\n\nC\n36\n\n\nA\n76\n\n\n\n\n\n\n\n2. How many days has each customer visited the restaurant?\n\n\nselect customer_id,\n   count(\n      distinct extract(\n         day\n         from order_date\n      )\n   ) as no_of_days_visited\nfrom sales\ngroup by customer_id\norder by customer_id;\n\n\n3 records\n\n\ncustomer_id\nno_of_days_visited\n\n\n\n\nA\n4\n\n\nB\n5\n\n\nC\n2\n\n\n\n\n\nThis SQL query calculates the number of days each customer has visited a restaurant by grouping the sales transactions by customer ID, extracting the day from the order date, counting the distinct days, and ordering the results by customer ID.\n\n\n3. What was the first item from the menu purchased by each customer?\n\n\nwith cte as (\n   select customer_id,\n      order_date,\n      row_number() over (\n         partition by customer_id\n         order by order_date\n      ) as order_rank,\n      product_id\n   from sales\n)\nselect c.customer_id,\n   c.order_date,\n   c.order_rank,\n   m.product_name\nfrom cte c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\norder_date\norder_rank\nproduct_name\n\n\n\n\nA\n2021-01-01\n1\nsushi\n\n\nB\n2021-01-01\n1\ncurry\n\n\nC\n2021-01-01\n1\nramen\n\n\n\n\n\nWithin the CTE, the row_number() function is employed to assign a ranking to each order for each customer based on the order_date. The ranking starts from 1, indicating the first order for each customer. The main query then selects the customer ID, order date, order rank, and product name from the CTE, joining the menu table to retrieve the product name corresponding to the product ID. Finally, it filters the results to include only records with an order rank of 1, effectively selecting the first item purchased for each customer.\n\n\n4. What is the most purchased item on the menu and how many times was it purchased by all customers?\n\n\nwith cte as(\n   select product_id,\n      count(product_id)\n   from sales\n   group by product_id\n   order by count(product_id) desc\n   limit 1\n) -- Most purchased item on the menu is the product with the id 3 which is ramen, according to cte\nselect customer_id,\n   count(product_id) as count_of_most_purchased_product\nfrom sales\nwhere product_id in (\n      select product_id\n      from cte\n   )\ngroup by customer_id;\n\n\n3 records\n\n\ncustomer_id\ncount_of_most_purchased_product\n\n\n\n\nA\n3\n\n\nB\n2\n\n\nC\n3\n\n\n\n\n\nThe query utilizes a common table expression (CTE) named cte to generate a temporary result set. Within the CTE, the count() function is employed to count the number of times each product ID appears in the sales table. The results are then sorted in descending order based on the product count. The limit 1 clause restricts the output to the top row, effectively identifying the product ID with the highest count.\nThe main query then selects the customer ID and the count of the most purchased product for each customer. It filters the sales table to include only records where the product_id matches the one identified in the CTE, effectively focusing on the most purchased item. Finally, it groups the results by customer_id to determine how many times each customer purchased the most popular item.\n\n\n5. Which item was the most popular for each customer?\n\n\nwith cte_1 as (\n   select customer_id,\n      product_id,\n      count(product_id) as count_of_item\n   from sales\n   group by customer_id,\n      product_id\n),\ncte_2 as (\n   select *,\n      row_number() over (\n         partition by customer_id\n         order by count_of_item desc\n      ) as order_rank\n   from cte_1\n)\nselect c.customer_id,\n   m.product_name,\n   c.count_of_item\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\nproduct_name\ncount_of_item\n\n\n\n\nC\nramen\n3\n\n\nB\nramen\n2\n\n\nA\nramen\n3\n\n\n\n\n\nThis query aims to identify the most popular item for each customer. The query utilizes two common table expressions to process the data and generate the desired output. The first CTE, named cte_1, calculates the count of each product purchased by each customer. It groups the rows in the sales table by customer_id and product_id, and then counts the occurrences of each product ID for each customer. This step determines the frequency of each product purchase for each customer.\nThe second CTE, named cte_2, assigns a ranking to each product for each customer based on the purchase frequency calculated in cte_1. It uses the row_number() function and partitions the data by customer_id, sorting within each partition by the count_of_item in descending order. This step effectively identifies the product with the highest purchase frequency (i.e., the most popular item) for each customer.\nThe main query then selects the customer ID, product name, and purchase count for each customer’s most popular item. It joins the menu table to obtain the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the most popular item for each customer is selected.\n\n\n6. Which item was purchased first by the customer after they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date\n      ) as order_rank\n   from cte_1\n   where order_date &gt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nramen\n2021-01-10\n1\n\n\nB\nsushi\n2021-01-11\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date, ensuring a chronological order of transactions. The second CTE, named cte_2, focuses on purchases made after each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is later than the join_date (membership start date).\nAdditionally, it assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date. This identifies the first purchase (order_rank = 1) made by each customer after becoming a member. The main query then selects the customer ID, product name, order date, and order rank for each customer’s first purchase after becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the first purchase is selected.\n\n\n7. Which item was purchased just before the customer became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      join_date,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date desc\n      ) as order_rank\n   from cte_1\n   where order_date &lt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nsushi\n2021-01-01\n1\n\n\nB\nsushi\n2021-01-04\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date in ascending order, ensuring a chronological sequence of transactions.\nThe second CTE, cte_2, focuses on purchases made before each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date).\nIt assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date in descending order. This identifies the last purchase (order_rank = 1) made by each customer before becoming a member.\nThe main query then selects the customer ID, product name, order date, and order rank for each customer’s last purchase before becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the last purchase is selected.\n\n\n8. What is the total items and amount spent for each member before they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n      natural join menu\n   order by order_date\n)\nselect customer_id,\n   count(distinct product_id) as count_of_products,\n   sum(price) as total_amount_spent\nfrom cte_1\nwhere order_date &lt; join_date\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ncount_of_products\ntotal_amount_spent\n\n\n\n\nA\n2\n25\n\n\nB\n2\n40\n\n\n\n\n\nThe query utilizes a CTE named cte_1 to prepare the data and simplifies the aggregation in the main query.\nThe main query then aggregates the data for each customer based on their membership status. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date). For each customer, it counts the distinct product_id values to determine the total number of unique items purchased and calculates the sum of price values to determine the total amount spent. The results are grouped by customer_id to provide individual summaries for each member.\n\n\n9. If each $1 spent equates to 10 points and sushi has a 2x points multiplier - how many points would each customer have?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points\n   from members\n      natural join sales\n      natural join menu\n)\nselect customer_id,\n   sum(points) as total_points\nfrom cte\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n860\n\n\nB\n940\n\n\n\n\n\nThe CTE combines the members, sales, and menu tables, providing a comprehensive view of customer memberships, their purchases, and the corresponding product names.\nThe main query then summarizes the points earned for each customer. It groups the data from the CTE by customer_id and calculates the sum of points values for each group, effectively determining the total points earned by each customer.\n\n\n10. In the first week after a customer joins the program (including their join date) they earn 2x points on all items, – not just sushi - how many points do customer A and B have at the end of January?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points,\n      case\n         when order_date - join_date &lt;= 7 then 2\n         else 1\n      end multiplier\n   from members\n      natural join sales\n      natural join menu\n),\ncte_2 as (\n   select *,\n      points * multiplier as total_points\n   from cte\n)\nselect customer_id,\n   sum(total_points) as total_points\nfrom cte_2\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n1720\n\n\nB\n1760\n\n\n\n\n\nThe first CTE, named cte, joins the members, sales, and menu tables, and calculates the points earned for each purchase using a conditional CASE expression, similar to the previous query.\nAdditionally, it assigns a multiplier to each purchase based on whether it falls within the first week after the customer’s join date. For purchases within the first week, the multiplier is 2 (double points); for purchases outside the first week, the multiplier is 1 (standard points).\nThe second CTE, named cte_2, simplifies the calculation by multiplying the points and multiplier columns for each purchase, effectively determining the total points earned per transaction. The main query then summarizes the points earned for each customer, including the double points accrued during the first week. It groups the data from cte_2 by customer_id and calculates the sum of total_points values for each group, providing the total points earned by customer A and customer B at the end of January\n\n\n11. Recreate the following table output using the available data:\n\n\nselect s.customer_id,\n    s.order_date,\n    men.product_name,\n    men.price,\n    CASE\n        WHEN s.order_date &gt;= m.join_date THEN 'Y'\n        ELSE 'N'\n    END\nfrom sales s\n    LEFT JOIN menu men ON s.product_id = men.product_id\n    LEFT JOIN members m on m.customer_id = s.customer_id\nORDER BY s.customer_id,\n    s.order_date;\n\n\nDisplaying records 1 - 10\n\n\ncustomer_id\norder_date\nproduct_name\nprice\ncase\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#closing-the-connection",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#closing-the-connection",
    "title": "Danny’s Diner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html",
    "href": "posts/2023-11-18_sleep-disorders/index.html",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "",
    "text": "The goal is to build a casual model using linear regression that explains quality of sleep, and logistic regression to explain sleep disorders\n\nlibrary(tidyverse)\n\n# descriptives\nlibrary(datawizard)\nlibrary(kableExtra)\nlibrary(skimr)\nlibrary(qqplotr)\nlibrary(gt)\n\n# os\nlibrary(here)"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#data-cleaning",
    "href": "posts/2023-11-18_sleep-disorders/index.html#data-cleaning",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Data cleaning",
    "text": "Data cleaning\nI observed the following issues in the data\n\nOccupation variable having levels with very few counts. Those with few counts can be consolidated with similar categories.\nThe BMI category variable has both “Normal” and “Normal Weight” as levels. This is most likely as data entry error.\nThe heart rate variable is not in a suitable format to work with. I’ll extract the systolic blood pressure from the variable as an integer. I would not need the diastolic blood pressure as I observed strong multicollinearity between both diastolic and systolic blood pressure.\n\n\n# observe occupations with low frequency counts\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# observe data entry error in bmi_category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# regrouping occupation variable so that occupations with lower counts are consolidated with other similar occupations. Simultaneously correcting data entry error in bmi_category variable\nsleep &lt;- sleep %&gt;% mutate( \n  occupation=ifelse(sleep$occupation %in% c(\"Software Engineer\", \"Scientist\", \"Engineer\"),\"Technical\", \nifelse(sleep$occupation %in% c(\"Salesperson\", \"Sales Representative\", \"Manager\"), \"Sales\", ifelse(sleep$occupation %in% c(\"Doctor\", \"Nurse\"), \"Medical\",\n                                                          as.character(occupation)))),\nbmi_category = ifelse(sleep$bmi_category %in% c(\"Normal\", \"Normal Weight\"), \"Normal\", as.character(sleep$bmi_category)))\n\n\n# extracting systolic blood pressure from the blood_pressure variable as it such a numeric variable is easier to work with. Subsequently dropping hear_rate variable\n\nsleep &lt;- sleep %&gt;% mutate(\nbp_sys= parse_number(str_sub(sleep$blood_pressure, 1, 3)),\nperson_id = as.character(person_id)\n) %&gt;% select(!heart_rate)\n\n\n# confirming changes\nhead(sleep)  %&gt;% gt() %&gt;% opt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      person_id\n      gender\n      age\n      occupation\n      sleep_duration\n      quality_of_sleep\n      physical_activity_level\n      stress_level\n      bmi_category\n      blood_pressure\n      daily_steps\n      sleep_disorder\n      bp_sys\n    \n  \n  \n    1\nMale\n27\nTechnical\n6.1\n6\n42\n6\nOverweight\n126/83\n4200\nNone\n126\n    2\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    3\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    4\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    5\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    6\nMale\n28\nTechnical\n5.9\n4\n30\n8\nObese\n140/90\n3000\nInsomnia\n140\n  \n  \n  \n\n\n\n\n\n# confirming changes to BMI category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# confirming changes to occupation variable\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#associations",
    "href": "posts/2023-11-18_sleep-disorders/index.html#associations",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Associations",
    "text": "Associations\n\nResearch Question: “What strong associations exists between quality_of_sleep and other variables in the dataset?\n\n# | warning: false\n# | message: false\n# packages required for associations\nlibrary(correlation)\nlibrary(GGally)\n\n\n# plotting the grid plot of scatterplots of numeric variables\nggscatmat(sleep %&gt;% select_if(is.numeric)) + theme_light()\n\n\n\n\n\n# showing relationships with strong correlations (greater than |+-0.4|)\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation() %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs)) %&gt;% filter(abs(r) &gt; 0.4) %&gt;% arrange(r) \ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.899\n0.000\n    sleep_duration\nstress_level\n−0.811\n0.000\n    age\nstress_level\n−0.422\n0.000\n    age\nquality_of_sleep\n0.474\n0.000\n    age\nbp_sys\n0.606\n0.000\n    physical_activity_level\ndaily_steps\n0.773\n0.000\n    sleep_duration\nquality_of_sleep\n0.883\n0.000\n  \n  \n  \n\n\n\n\n\n# robust correlation analysis since the data are skewed. Results is filtered to include only relationships involving quality_of_sleep. Sorted ascendingly by p-value\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation(method=\"percentage\") %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs))  %&gt;% arrange(p)  %&gt;% filter( Parameter1 == \"quality_of_sleep\" | Parameter2 == \"quality_of_sleep\") %&gt;% arrange(p) %&gt;% filter(p &lt; 0.05)\ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.910\n0.000\n    sleep_duration\nquality_of_sleep\n0.892\n0.000\n    age\nquality_of_sleep\n0.432\n0.000\n    quality_of_sleep\nphysical_activity_level\n0.178\n0.005\n  \n  \n  \n\n\n\n\n\nResults\nThe most significant relationships (at alpha = .05) are the relationships between:\n\nquality of sleep and stress level, r(372) =-0.91, p &lt; 0.0001\nquality of sleep and sleep duration, r(372) = 0.89, p &lt; 0.0001\nquality of sleep and age, r(372) =0.43, p &lt; 0.0001\nquality of sleep and physical activity level, r(372) =0.18, p = 0.0054\n\nThese are the variables we would fit to our linear regression model."
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#research-questions",
    "href": "posts/2023-11-18_sleep-disorders/index.html#research-questions",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research questions:",
    "text": "Research questions:\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\nIs the relationship between stress level and quality of sleep moderated by occupation?\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\n\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\n\n# Visualizing the relationship of the three variables\nggplot(sleep, aes(x = sleep_duration, y = quality_of_sleep, color = sleep_disorder)) +\ngeom_jitter(size=1, alpha=0.8) +\nlabs(x = \"Sleep duration\", y = \"Quality of sleep\", color = \"Sleep disorder\") +\ngeom_smooth(method = \"lm\", se = FALSE, linewidth=0.9) + theme_bw()\n\n\n\n\nThe plot shows that there might be some moderation effects.\n\n# making sleep_disorder a factor variable and setting the base level to \"None\"\nsleep &lt;- sleep %&gt;% mutate(\n  sleep_disorder = as_factor(sleep_disorder) %&gt;% fct_relevel(\"None\")\n)\n\n\nqs_lm_03 &lt;- lm(quality_of_sleep ~ sleep_duration * sleep_disorder, data=sleep)\nqs_lm_03 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-0.5750673\n0.36703899\n-1.566774\n1.180269e-01\n    sleep_duration\n1.1144522\n0.04963586\n22.452560\n6.124844e-71\n    sleep_disorderSleep Apnea\n-3.4341228\n0.57712441\n-5.950403\n6.233752e-09\n    sleep_disorderInsomnia\n-3.2040380\n1.11193745\n-2.881491\n4.190005e-03\n    sleep_duration:sleep_disorderSleep Apnea\n0.4802913\n0.08000178\n6.003508\n4.632039e-09\n    sleep_duration:sleep_disorderInsomnia\n0.4503706\n0.16657925\n2.703642\n7.176641e-03\n  \n  \n  \n\n\n\n\n\nInterpretation\n\n\n\n\n\n\nTip\n\n\n\nThe effect of increasing sleep duration in someone with no sleep disorder on sleep quality is 1.11\nThe effect of sleep apnea on sleep quality compared to someone with no disorders is -3.43\nThe effect of Insomnia on sleep quality compared to someone with no disorders is -3.20\nThe effect of increasing sleep duration in someone with sleep anea on sleep quality compared to someone with no disorders is 0.48\nThe effect of increasing sleep duration in someone with insomnia on sleep quality compared to someone with no disorders is 0.45\n\n\n\ninteractions::sim_slopes(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of sleep_duration when sleep_disorder = Insomnia: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.56   0.15   1.26    1.86    10.26   0.00\n\nSlope of sleep_duration when sleep_disorder = Sleep Apnea: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.59   0.06   1.48    1.71    27.35   0.00\n\nSlope of sleep_duration when sleep_disorder = None: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.11   0.03   1.05    1.17    36.50   0.00\n\n\nThe slopes for the three regressions are not significantly different\n\ninteractions::interact_plot(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Sleep disorder\"\n  )\n\n\n\n\nLooking at the slope, we can that the moderation effect is not significant.\n\n\nConclusion\nSleep disorder is not a moderator of the relationship between sleep duration and quality of sleep\n\n\n\nIs the relationship between stress level and quality of sleep moderated by occupation?\n\nqs_lm_04 &lt;- lm(quality_of_sleep ~ stress_level * occupation, data=sleep)\nqs_lm_04 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n9.3661417\n0.22464236\n41.6935685\n2.847324e-140\n    stress_level\n-0.3208661\n0.04725953\n-6.7894487\n4.624546e-11\n    occupationLawyer\n3.3611310\n1.08304354\n3.1034126\n2.063488e-03\n    occupationMedical\n1.3637168\n0.24305144\n5.6108156\n4.008125e-08\n    occupationSales\n3.5481440\n1.02746628\n3.4532948\n6.193572e-04\n    occupationTeacher\n-0.2842770\n0.33263086\n-0.8546322\n3.933200e-01\n    occupationTechnical\n2.0623438\n0.25972133\n7.9406026\n2.559029e-14\n    stress_level:occupationLawyer\n-0.6336793\n0.21426067\n-2.9575158\n3.304916e-03\n    stress_level:occupationMedical\n-0.2851357\n0.04939422\n-5.7726532\n1.679151e-08\n    stress_level:occupationSales\n-0.6791339\n0.15058782\n-4.5098857\n8.775327e-06\n    stress_level:occupationTeacher\n-0.1447393\n0.07087144\n-2.0422799\n4.184795e-02\n    stress_level:occupationTechnical\n-0.4718021\n0.05572917\n-8.4659819\n6.415721e-16\n  \n  \n  \n\n\n\n\n\ninteractions::sim_slopes(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of stress_level when occupation = Lawyer: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.95   0.03   -1.02   -0.89   -29.71   0.00\n\nSlope of stress_level when occupation = Accountant: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.32   0.04   -0.40   -0.24    -7.92   0.00\n\nSlope of stress_level when occupation = Teacher: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.47   0.09   -0.65   -0.28    -5.01   0.00\n\nSlope of stress_level when occupation = Sales: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -1.00   1.04   -3.04    1.04    -0.97   0.33\n\nSlope of stress_level when occupation = Medical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.61   0.01   -0.62   -0.59   -81.54   0.00\n\nSlope of stress_level when occupation = Technical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.79   0.07   -0.93   -0.65   -10.97   0.00\n\n\n\ninteractions::interact_plot(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Occupation\"\n  )\n\n\n\n\nAgain, we do not observe a significant moderation effect of occupation on the relationship between stress level and quality of sleep\n\n\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\nTo test this, we are going to run 3 regressions:\n\nThe total effect of physical_activity_level on quality_of_sleep.\nThe effect of physical_activity_level on quality_of_sleep that is mediated by daily_steps, a.k.a. the indirect effect\nThe effect of physical_activity_level on quality_of_sleep that is not mediated by daily_steps, a.k.a. the direct effect\n\n\nTotal effect\nWe first determine the total effect by running a regression of physical_activity_level on quality_of_sleep (without including daily_steps):\n\nlm(quality_of_sleep ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n6.657\n0.183\n36.307\n0.000\n    physical_activity_level\n0.011\n0.003\n3.792\n0.000\n  \n  \n  \n\n\n\n\nThe total effect is approximately 0.011, meaning that an increase of one unit in physical activity increases sleep quality by 0.001 units on average, not holding daily steps constant.\n\n\nMediated/Indirect Effect\nThe effect of physical_activity_level on quality_of_sleep mediated by daily_steps can be obtained by multiplying together the effect of physical_activity_level on daily_steps and the effect of daily_steps on quality_of_sleep.\n\n1. physical_activity_level on daily_steps (a)\n\nlm(daily_steps ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n3265.57619\n160.321889\n20.36887\n1.718735e-62\n    physical_activity_level\n60.01692\n2.556092\n23.47995\n1.968256e-75\n  \n  \n  \n\n\n\n\nOne unit increase in physical_activity_level increases daily_steps by an average of 60 steps\n\n\n2. daily_steps on quality_of_sleep\n\nlm(quality_of_sleep ~ daily_steps + physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n7.4500815511\n2.609352e-01\n28.551459\n1.151887e-95\n    daily_steps\n-0.0002428667\n5.802065e-05\n-4.185867\n3.551189e-05\n    physical_activity_level\n0.0256601109\n4.506428e-03\n5.694113\n2.524969e-08\n  \n  \n  \n\n\n\n\n(a * b) = 60.07 * -0.00024 = -0.0144\n\n\n\nDirect Effect\nThis is simply the coefficient of physical_activity_level in the above regression = 0.0257\nAccording to Zhao et al., what we have here isCompetitive Mediation (Regularly Partial Mediation)\nIn the competitive partial mediation hypothesis, it is assumed that the intermediate variable (daily_steps) could sometimes increase and at times decrease the relationship between the independent and dependent variables. i.e an “inconsistent” model.\n\n\n\nDecision tree for determining mediation"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "href": "posts/2023-11-18_sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research question: are significant differences in the mean sleep quality of both male and female participants",
    "text": "Research question: are significant differences in the mean sleep quality of both male and female participants\n\n# summary statistics of quality_of_sleep by gender\n\nby(cbind(data=sleep$quality_of_sleep), sleep$gender, psych::describe)\n\nINDICES: Female\n     vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 185 7.66 1.28      8    7.76 1.48   4   9     5 -0.49    -0.83 0.09\n------------------------------------------------------------ \nINDICES: Male\n     vars   n mean sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 189 6.97  1      7    7.01 1.48   4   9     5 -0.35    -0.59 0.07\n\n\nThe mean of female participants is one point higher than males. But is this difference significant?\n\n# plotting a violin-errorbar plot to visualise the relationship\nggplot(sleep, aes(gender,quality_of_sleep))+\ngeom_violin() +\nstat_summary(fun.data=\"mean_cl_normal\") + \ntheme_minimal()\n\n\n\n\nSince the errorbars do not overlap, we can be fairly confident that the difference is significant. Lets confirm this hunch with a Welsh’s t-test.\n\n t.test(quality_of_sleep ~ gender,\n                    data = sleep,\n                    paired = FALSE,\n                    var.equal = FALSE,\n                    conf.level = 0.95,\n                    na.action = na.exclude)\n\n\n    Welch Two Sample t-test\n\ndata:  quality_of_sleep by gender\nt = 5.8593, df = 347.96, p-value = 1.078e-08\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n 0.4627786 0.9304432\nsample estimates:\nmean in group Female   mean in group Male \n            7.664865             6.968254 \n\n\nBecause the p-value = 1.078e-08 is less than our alpha of .05, we can conclude that the difference between both genders is significant. Also, since the confidence interval expressing the true difference in means does not cross zero, we can be confident that there exists a true difference, with a chance of us being wrong 5% of the time.\n\neffectsize::cohens_d(quality_of_sleep ~ gender, data = sleep) %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Cohens_d\n      CI\n      CI_low\n      CI_high\n    \n  \n  \n    0.608\n0.950\n0.400\n0.815\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOn average, female participants have better sleep quality (M = 7.66, SE = 0.09), than those not given a cloak (M = 6.97, SE = 0.07). Sleep quality is significantly different for both genders: the mean difference, M = 0.69, 95% CI [0.46, 0.93], was significantly different from 0, t(347.96) = 5.85, p = 01.078e-08. This effect was quite large, d= 0.61[0.4, 0.82]"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#conclusion-1",
    "href": "posts/2023-11-18_sleep-disorders/index.html#conclusion-1",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Conclusion",
    "text": "Conclusion\nI created this post as a way of solidifying my understanding of these concepts. I find that documenting the material helps it to stick iwth me better. I hope that you find this helpful. My deepest gratitude goes to Andy Field for his exceptional book on statistical analysis with R."
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#introduction",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#introduction",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "title": "Foodie Fi",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#database-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#database-connection",
    "title": "Foodie Fi",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"foodie_fi\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#queries",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#queries",
    "title": "Foodie Fi",
    "section": "Queries",
    "text": "Queries\n\n1. How many customers has Foodie-Fi ever had?\n\n\nSELECT count(DISTINCT customer_id)\nFROM subscriptions;\n\n\n1 records\n\n\ncount\n\n\n\n\n1000\n\n\n\n\n\n\n\n3. What is the monthly distribution of trial plan start_date values for our dataset use the start of the month as the group by value?\n\nSELECT upper(to_char(start_date, 'month')) as start_month,\n    count(*) frequency\nfrom subscriptions\nwhere plan_id = 0\ngroup by 1\nORDER BY 2 desc;\n\n\nDisplaying records 1 - 10\n\n\nstart_month\nfrequency\n\n\n\n\nMARCH\n94\n\n\nJULY\n89\n\n\nAUGUST\n88\n\n\nMAY\n88\n\n\nJANUARY\n88\n\n\nSEPTEMBER\n87\n\n\nDECEMBER\n84\n\n\nAPRIL\n81\n\n\nJUNE\n79\n\n\nOCTOBER\n79\n\n\n\n\n\n\n\n3. What plan start_date values occur after the year 2020 for our dataset? Show the breakdown by count of events for each plan_name ?\n\n\nSELECT p.plan_name,\n    count(*) as count_of_events_after_2020\nfrom subscriptions s\n    natural join plans p\nwhere EXTRACT(\n        year\n        from start_date\n    ) &gt; 2020\ngroup by 1;\n\n\n4 records\n\n\nplan_name\ncount_of_events_after_2020\n\n\n\n\npro annual\n63\n\n\nchurn\n71\n\n\npro monthly\n60\n\n\nbasic monthly\n8\n\n\n\n\n\n\n\n4. What is the customer count and percentage of customers who have churned rounded to 1 decimal place?\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id = 4\n)\nSELECT cte1.whole as total_customers,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_churned\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ntotal_customers\npct_churned\n\n\n\n\n2650\n12\n\n\n\n\n\n\n\n5. How many customers have churned straight after their initial free trial? what percentage is this rounded to the nearest whole number?\n\nwith cte as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\ncte2 as (\n    -- getting rows whose values satisfy the condition in the question\n    select *\n    from cte\n    where plan_id = 0\n        and lead_plan_id = 4\n) -- solution\nselect count(*) as count_of_customers_who_churned_after_free_trial\nfrom cte2 \n\n\n1 records\n\n\ncount_of_customers_who_churned_after_free_trial\n\n\n\n\n92\n\n\n\n\n\n\n\n6. What is the number and percentage of customer plans after their initial free trial?\n\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id &lt;&gt; 0\n)\nSELECT cte2.part as customer_count_after_trial_plan,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_ccatp\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ncustomer_count_after_trial_plan\npct_ccatp\n\n\n\n\n1650\n62\n\n\n\n\n\n\n\n7. How many customers have upgraded to an annual plan in 2020?\n\n\nwith cte1 as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\n-- filtering to only annual plans\ncte2 as (\n    select *,\n        lead_plan_id - plan_id as diff\n    from cte1\n    where lead_plan_id = 3\n) -- excluding churned customers and unupgraded plans\nselect count(DISTINCT customer_id) as upgraded_customers_2020_count\nfrom cte2\nwhere (diff &gt; 0)\n    and (lead_plan_id &lt;&gt; 4)\n    and EXTRACT(\n        year\n        from start_date\n    ) = 2020;\n\n\n1 records\n\n\nupgraded_customers_2020_count\n\n\n\n\n253\n\n\n\n\n\n\n\n8. How many days on average does it take for a customer to upgrade to an annual plan from the – day they join Foodie-Fi?\n\nwith cte1 as (\n    select *,\n        max(plan_id) over (partition by customer_id) as highest_plan_suscribed,\n        max(start_date) over (partition by customer_id) as date_of_hps,\n        min(start_date) over (partition by customer_id) as date_of_lps,\n        row_number() over (partition by customer_id) as sn\n    from subscriptions\n    order by customer_id,\n        start_date,\n        plan_id\n),\ncte2 as(\n    select *,\n        date_of_hps - date_of_lps as diff_in_days\n    from cte1\n    where highest_plan_suscribed = 3\n        and sn = 1\n)\nselect round(avg(diff_in_days)::numeric, 2) as avg_days_to_upgrade_to_annual\nfrom cte2\n\n\n1 records\n\n\navg_days_to_upgrade_to_annual\n\n\n\n\n105.95\n\n\n\n\n\n\n\n9. How many customers downgraded from a pro monthly to a basic monthly plan in 2020?\n\nwith cte1 as(\n        select *,\n            lead(plan_id) over(partition by customer_id) as lead_plan_id\n        from subscriptions\n        order by customer_id,\n            start_date,\n            plan_id\n    ),\n    cte2 as (\n        select customer_id,\n            plan_id,\n            lead_plan_id,\n            start_date\n            from cte1\n        where plan_id = 2\n            and lead_plan_id = 1\n            and EXTRACT(\n                year\n                from start_date\n            ) = 2020\n    )\nselect count(*) as number_of_customers_downgrade_from_prom_basm\nfrom cte2\n\n\n1 records\n\n\nnumber_of_customers_downgrade_from_prom_basm\n\n\n\n\n0"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#closing-the-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#closing-the-connection",
    "title": "Foodie Fi",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts_index.html",
    "href": "posts_index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Causal-Behavioral Framework for Data Analysis\n\n\nIntroductory part to a series on Florent Buisson’s book, Behavioral Data Analysis with R & Python\n\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPizza Runner\n\n\nSecond part of a series of SQL case studies where I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoodie Fi\n\n\nThird part of a series of SQL case studies\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Bank\n\n\nFourth part of a series of SQL case studies… more CTEs!\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDanny’s Diner\n\n\nIn this first part of a series of SQL case studies, I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships of Variables in Sleep Data\n\n\nIn this post, I use regression, with an emphasis on understanding the casual relationships between variables\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSales Monitoring Dashboard\n\n\nMy take on the AdventureWorks dataset; featuring database creation, data modeling in Power BI and python scripting\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising COVID-19’s Impact in Nigeria\n\n\nTracking a pandemic with data\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)\n\n\nFeaturing data modeling in Power BI, and Dax\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts_index.html#recent-posts-posts-recentes",
    "href": "posts_index.html#recent-posts-posts-recentes",
    "title": "Chamber of (data) secrets",
    "section": "",
    "text": "1 min\n\n\n\n\n\n\n  \n\n\n\n\n\nSecond part of a series of SQL case studies where I use Postgres SQL to answer a bunch of business questions\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 19, 2023\n\n\nEdun Joshua\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\n\nThird part of a series of SQL case studies\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 19, 2023\n\n\nEdun Joshua\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\nFourth part of a series of SQL case studies… more CTEs!\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 19, 2023\n\n\nEdun Joshua\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\nIn this first part of a series of SQL case studies, I use Postgres SQL to answer a bunch of business questions\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 18, 2023\n\n\nEdun Joshua\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\n\nIn this post, I use regression, with an emphasis on understanding the casual relationships between variables\n\n\n\n\nstats\n\n\ncausal regression modelling\n\n\n \n\n\n\n\nNov 18, 2023\n\n\nEdun Joshua\n\n\n16 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all/Ver todos"
  },
  {
    "objectID": "posts_index.html#recent-posts",
    "href": "posts_index.html#recent-posts",
    "title": "Chamber of (data) secrets",
    "section": "",
    "text": "See all"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html",
    "href": "posts/2023-11-30_BDA-1/index.html",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "",
    "text": "Please Note: The ideas presented in this article are not my own original thoughts. This material is merely a condensed version of my study notes based on Florent Buisson’s excellent book. Due to the need for brevity, certain nuances of these concepts have been omitted. For a more comprehensive understanding, please refer to the original source.\nPredictive analytics is like having a super cool fortune teller 🔮who can predict what’s going to happen next. But causal analytics is like being a detective 🕵️‍♀️🕵️‍♂️, trying to figure out why things happen in the first place 🤔. In predictive analytics, we just want to find a bunch of clues 🕵️‍♀️🕵️‍♂️ that can help us guess what’s going to happen next, even if those clues aren’t actually causing it to happen. But in causal analytics, we’re not just interested in guessing 🔮, we want to understand the real reasons behind things 🧠.\nImagine you want to know if eating more fruits and veggies 🍎🥦 will make you healthier 💪. Predictive analytics might tell you that people who eat more fruits and veggies tend to be healthier, but that doesn’t mean the fruits and veggies are actually causing the better health. It could be that healthier people just tend to make healthier choices overall 🏃‍♀️🚴🥗.\nSo, how do we really know if eating more fruits and veggies is making us healthier? That’s where causal analytics comes in 🕵️‍♀️🕵️‍♂️. We need to carefully choose the clues we use to solve the mystery, making sure they’re not just guessing 🔮but actually helping us understand the real reasons behind things 🧠.\ndf1 &lt;- read_csv(\"chap1-stand_data.csv\")\nhead(df1) %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      icecream_sales\n      iced_coffee_sales\n      summer_months\n      temps\n    \n  \n  \n    25649.78\n28592.18\n0\n28.59314\n    32694.93\n37152.84\n0\n37.13064\n    26467.80\n24074.19\n0\n24.11190\n    43438.79\n49169.47\n0\n49.21807\n    52452.63\n47249.40\n0\n47.25766\n    34130.66\n34228.50\n0\n34.25746\nImagine you’re an ice cream 🍦 seller and you want to know if hotter days ☀️mean more ice cream sales 🤑. You track the temperature and your sales for a while and find that for every degree the temperature rises, your sales go up by $1,171!\nsummary(lm(\"icecream_sales ~ temps\",data=df1))\n\n\nCall:\nlm(formula = \"icecream_sales ~ temps\", data = df1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-30272  -5197   -494   3787  37430 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6169.844    531.506  -11.61   &lt;2e-16 ***\ntemps        1171.335      9.027  129.76   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8730 on 2398 degrees of freedom\nMultiple R-squared:  0.8753,    Adjusted R-squared:  0.8753 \nF-statistic: 1.684e+04 on 1 and 2398 DF,  p-value: &lt; 2.2e-16\nYou’re excited and think you’ve found the perfect formula for success!🤑 But then something strange happens. October comes around and it’s a scorcher 🥵, but your sales actually drop! 😨 What gives?\nAfter some sleuthing, you realize that the culprit is school 🏫. In the summer, kids are out of school and have more time to enjoy ice cream 🍦. But in October, they’re back in class and have less time for frozen treats.\nSo, even though the temperature was high, your sales dropped because of the school year. This is called a “confounder” 😵. A confounder is something that can make it look like something else is causing a (third) something when it’s actually not.\nIn this case, the school year made it look like the temperature was causing sales to drop, when it was really the school year all along 🏫. So, next time you’re trying to figure out what’s causing something, be sure to watch out for confounders! 👀👀\nRemember that time when we added a bunch of random ingredients to our recipe and ended up with a weird-tasting soup? It’s the same with causal modelling.\nTo be fair, if your goal is only to predict a variable, you have a model that is carefully designed to generalize adequately beyond your testing data, and you don’t care about why the predicted variable is taking a certain value, then that’s a perfectly valid stance.\nLet’s demonstrate this with our example by adding a variable that we might be inclined to include but will bias our regression. The variable IcedCoffeeSales is correlated with Temperature but not with SummerMonth. Let’s look at what happens to our regression if we add this variable in addition to Temperature and SummerMonth (a binary 1/0 variable that indicates if the month was July or August (1) or any other month (0)):\nsummary(lm(icecream_sales ~ iced_coffee_sales + temps + summer_months, data=df1))\n\n\nCall:\nlm(formula = icecream_sales ~ iced_coffee_sales + temps + summer_months, \n    data = df1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25763.1  -3364.4     78.6   3317.5  27512.0 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -15.827    374.581  -0.042    0.966    \niced_coffee_sales    -1.701      2.083  -0.817    0.414    \ntemps              2702.788   2083.161   1.297    0.195    \nsummer_months     19548.168    361.572  54.064   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5859 on 2396 degrees of freedom\nMultiple R-squared:  0.9439,    Adjusted R-squared:  0.9438 \nF-statistic: 1.344e+04 on 3 and 2396 DF,  p-value: &lt; 2.2e-16\nOnly summer_months has a statistically significant influence on icecream_sales . The other variable are not good predictors of the target variable and only confound the model.\nImagine you’re running an ice cream stand 🍦🍨 and you want to know if people who like chocolate ice cream 🍫are more likely to buy from your shop than people who like vanilla ice. You ask a bunch of people how much they like chocolate and vanilla ice cream (on a scale of 1 to 10, of course), and whether they’ve ever bought from your shop. You find out that some people like one but not the other, some like both equally, and some like one more than the other.\nChecking the association between our variables we discover 2 things:\nAll good so far! 👍 If we want to understand how both vanilla and chocolate ice cream preferences affect whether someone is a customer or not, then it makes sense to include both variables in our logistic regression model. 📈 But what if we want to know something different? What if we want to know how much the people who like vanilla ice cream also like chocolate ice cream? 🍦🍫 In other words, we’re interested in the relationship between how much someone likes vanilla ice cream and how much they like chocolate ice cream, but only for people who have actually bought from the ice cream stand. 🛍️ To do this, we can plot a graph of how much customers who have bought ice cream like chocolate ice cream, based on how much they like vanilla ice cream.\nHmm, something’s not right! 🤨 When we looked at the relationship between how much vanilla ice cream customers like and how much chocolate ice cream they like, we found a weird pattern: people who like one flavor tend to dislike the other. 🤯 But don’t worry, vanilla lovers aren’t suddenly becoming chocolate haters, and vice versa! 👎 This strange correlation was actually caused by the way we looked at the data.\nWe only looked at people who had bought ice cream from the stand, and this left out a lot of people who might have liked both flavours but didn’t buy any ice cream. If someone has a weak taste for both vanilla and chocolate ice creams, they are most likely not shoppers. if someone has a strong taste for vanilla, they might shop at your stand even if they don’t have a strong taste for chocolate. However, since you excluded non-shoppers, you removed a bunch of data points that should exist in the lower left quadrant of the scatterplot. If they were included, a positive relationship would be observed.\nSo, next time you’re looking at data, be sure to consider all of the data points, not just the ones that fit your expectations.😉\nUp next, chapter 2!"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-30_BDA-1/index.html#entity-relationship-diagram",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#causal-analytics-unraveling-the-mystery-behind-why-things-happen",
    "href": "posts/2023-11-30_BDA-1/index.html#causal-analytics-unraveling-the-mystery-behind-why-things-happen",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Causal Analytics: 🕵️‍♂️Unraveling the Mystery Behind Why Things Happen",
    "text": "Causal Analytics: 🕵️‍♂️Unraveling the Mystery Behind Why Things Happen"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#data",
    "href": "posts/2023-11-30_BDA-1/index.html#data",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "href": "posts/2023-11-30_BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Why Correlation Is Not Causation: A Confounder in Action",
    "text": "Why Correlation Is Not Causation: A Confounder in Action"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#too-many-variables-can-spoil-the-broth",
    "href": "posts/2023-11-30_BDA-1/index.html#too-many-variables-can-spoil-the-broth",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Too Many Variables Can Spoil the Broth",
    "text": "Too Many Variables Can Spoil the Broth"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#another-example",
    "href": "posts/2023-11-30_BDA-1/index.html#another-example",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Another example",
    "text": "Another example"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html",
    "href": "posts/2022_05_28_UK-accidents/index.html",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "",
    "text": "&lt;/div&gt;\n&lt;/section&gt;\n&lt;section id=\"requirement-gathering\" class=\"level2\"&gt;\n&lt;h2&gt;Requirement Gathering&lt;/h2&gt;\n&lt;p&gt;Client wants to create a dashboard on road accidents for the year 2021 and 2022.&lt;/p&gt;\n&lt;section id=\"a.-primary-kpis\" class=\"level4\"&gt;\n&lt;h4&gt;a. Primary KPIs&lt;/h4&gt;\n&lt;ul&gt;\n&lt;li&gt;Total Casualties and Total Accident Values:\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;li&gt;Year-over-Year (YoY) Growth&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Total Casualties by Accident Severity:\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;li&gt;Year-over-Year (YoY) Growth&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"b.-secondary-kpis\" class=\"level4\"&gt;\n&lt;h4&gt;b. Secondary KPIs&lt;/h4&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Total Casualties with Respect to Vehicle Type:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Monthly Trend Comparison of Casualties:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year vs. Previous Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Casualties by Road Type:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Current Year Casualties by Area/Location &amp; Day/Night&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Total Casualties and Total Accidents by Location Total casualties and total accidents by location&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n&lt;section id=\"identifying-stakeholders\" class=\"level2\"&gt;\n&lt;h2&gt;Identifying Stakeholders 🧑🏽‍💼&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Emergency Services Departments&lt;/li&gt;\n&lt;li&gt;Road Safety Corps&lt;/li&gt;\n&lt;li&gt;Traffic Management Agencies&lt;/li&gt;\n&lt;li&gt;Police Force&lt;/li&gt;\n&lt;li&gt;General public&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"understanding-raw-data\" class=\"level2\"&gt;\n&lt;h2&gt;Understanding Raw Data 🥩&lt;/h2&gt;\n&lt;p&gt;Data Grain: A single instance of a reported accident event in the UK in 2021 and 2022&lt;/p&gt;\n&lt;table&gt;\n&lt;colgroup&gt;\n&lt;col style=\"width: 31%\" /&gt;\n&lt;col style=\"width: 68%\" /&gt;\n&lt;/colgroup&gt;\n&lt;thead&gt;\n&lt;tr class=\"header\"&gt;\n&lt;th&gt;Fields&lt;/th&gt;\n&lt;th&gt;description (datatype)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Accident_Index&lt;/td&gt;\n&lt;td&gt;unique row identifier (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Accident Date&lt;/td&gt;\n&lt;td&gt;date of accident event (date)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Day_of_Week&lt;/td&gt;\n&lt;td&gt;day of the week (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Junction_Control&lt;/td&gt;\n&lt;td&gt;mechanism of traffic control at junction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Junction_Detail&lt;/td&gt;\n&lt;td&gt;type of junction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Accident_Severity&lt;/td&gt;\n&lt;td&gt;severity of accident (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Latitude&lt;/td&gt;\n&lt;td&gt;latitude (float)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Light_Conditions&lt;/td&gt;\n&lt;td&gt;light conditions at accident scene (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Local_Authority_(District)&lt;/td&gt;\n&lt;td&gt;name of district (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Carriageway_Hazards&lt;/td&gt;\n&lt;td&gt;hazards on the road if any&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Number_of_Casualties&lt;/td&gt;\n&lt;td&gt;number of causalities (integer)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Number_of_Vehicles&lt;/td&gt;\n&lt;td&gt;number of vehicles involved in the accident (integer)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Police_Force&lt;/td&gt;\n&lt;td&gt;Police force jurisdiction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Road_Surface_Conditions&lt;/td&gt;\n&lt;td&gt;condition of road surface (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Road_Type&lt;/td&gt;\n&lt;td&gt;road type (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Speed_limit&lt;/td&gt;\n&lt;td&gt;road speed limit in mph (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Time&lt;/td&gt;\n&lt;td&gt;time of day accident occurred (time)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Urban_or_Rural_Area&lt;/td&gt;\n&lt;td&gt;Whether accident occurred in an urban or rural area (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Weather_Conditions&lt;/td&gt;\n&lt;td&gt;weather condition at time of accident (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Vehicle_Type&lt;/td&gt;\n&lt;td&gt;type of vehicle involved in the accident&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n&lt;/section&gt;\n&lt;section id=\"data-cleansing\" class=\"level2\"&gt;\n&lt;h2&gt;Data cleansing 🧹&lt;/h2&gt;\n&lt;p&gt;Issues - Junction_Control column has, among others, two values “Auto traffic sigl” and “Auto traffic signal” of which the former is a misspelling of the latter&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Instances where “Fatal” is misspelled as “Fetal” in Accident_Severity column&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;“Time” column was represented as a datetime data type in Power BI&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;Fixes - Used the Replace Values feature to correct the misspellings of Issues 1 &amp; 2 - Changed the data type to time accordingly - All fixes were done in Power Query.&lt;/p&gt;\n&lt;section id=\"data-processing\" class=\"level3\"&gt;\n&lt;h3&gt;Data Processing ⚙️&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;The data needs a calendar table so as to use Time Intelligence functions further down the line.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Using the CALENDAR function, a new table with calculated, dynamic columns of Date, Month and Year was generated.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"data-modeling\" class=\"level3\"&gt;\n&lt;h3&gt;Data Modeling 🏛&lt;/h3&gt;\n&lt;p&gt;a one-to-many active relationship was established between the calendar table and the data table &lt;img src=\"data-model.png\" class=\"img-fluid\" alt=\"Schema\" /&gt;&lt;/p&gt;\n&lt;/section&gt;\n&lt;section id=\"data-visualization\" class=\"level3\"&gt;\n&lt;h3&gt;Data Visualization 🎨&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;Power BI magic!✨ The report pbix file is available in this repo to explore design decisions The following DAX measures were used:&lt;/li&gt;\n&lt;/ul&gt;\n&lt;pre&gt;&lt;code&gt;Count of Accidents = DISTINCTCOUNT(Data[Accident_Index])\n\nCurrent Year Accident Count = TOTALYTD(COUNT(Data[Accident_Index]),&#39;Calendar&#39;[Date])\n\nCurrent Year Casualties = TOTALYTD(SUM(Data[Number_of_Casualties]),&#39;Calendar&#39;[Date])\n\nPrevious Year Accident Counts = CALCULATE(COUNT(Data[Accident_Index]), SAMEPERIODLASTYEAR(&#39;Calendar&#39;[Date]))\n\nPrevious Year Casualties = CALCULATE(SUM(Data[Number_of_Casualties]),\nSAMEPERIODLASTYEAR(&#39;Calendar&#39;[Date]))\n\nYoY Accident Count = DIVIDE([Current Year Accident Count] - [Previous Year Accident Counts], [Previous Year Accident Counts])\n\nYoY Casualties = (DIVIDE([Current Year Casualties] - [Previous Year Casualties],[Previous Year Casualties]))&lt;/code&gt;&lt;/pre&gt;\n&lt;/section&gt;\n&lt;section id=\"deriving-insights\" class=\"level3\"&gt;\n&lt;h3&gt;Deriving Insights&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;The vehicle type most involved in accidents was cars. This can be explained by the fact that most vehicles plying UK roads are cars&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There is a general decrease in casualty counts in 2022 compared to 2021&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n&lt;section id=\"contact\" class=\"level2\"&gt;\n&lt;h2&gt;Contact&lt;/h2&gt;\n&lt;p&gt;Edun Joshua Olubori - &lt;a href=\"https://www.linkedin.com/in/joshua-edun\"&gt;connect on linkedin&lt;/a&gt; - joshuaolubori@gmail.com&lt;/p&gt;\n&lt;p&gt;Live Demo: &lt;a href=\"https://app.powerbi.com/view?r=eyJrIjoiYjZiNDYwMTctY2EwOC00NGQ2LWJmOGEtMzVlN2I2YTFlZTEwIiwidCI6ImRmODY3OWNkLWE4MGUtNDVkOC05OWFjLWM4M2VkN2ZmOTVhMCJ9&amp;embedImagePlaceholder=true\"&gt;here&lt;/a&gt;&lt;/p&gt;\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Home\"&gt;Home&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.html\"&gt;/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Posts\"&gt;Posts&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/posts_index.html\"&gt;/posts_index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"footer-left\"&gt;Blog made with 💚 and &lt;a href=\"https://quarto.org/\"&gt;Quarto&lt;/a&gt;, by Edun Joshua. License: &lt;a href=\"https://creativecommons.org/licenses/by-sa/2.0/\"&gt;CC BY-SA 2.0&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;Featuring data modeling in Power BI, and Dax&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;Featuring data modeling in Power BI, and Dax&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;script src=\"https://giscus.app/client.js\"\n        data-repo=\"JoshuaOlubori/qblog\"\n        data-repo-id=\"R_kgDOKvhXew\"\n        data-category=\"General\"\n        data-category-id=\"DIC_kwDOKvhXe84CbFeH\"\n        data-mapping=\"title\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"top\"\n        data-theme=\"light\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async&gt;\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n&lt;footer class=\"footer\"&gt;\n  &lt;div class=\"nav-footer\"&gt;\n    &lt;div class=\"nav-footer-left\"&gt;\n      &lt;div class='footer-contents'&gt;Blog made with 💚 and [Quarto](https://quarto.org/),  by Edun Joshua.  License: [CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/).&lt;/div&gt;  \n    &lt;/div&gt;   \n    &lt;div class=\"nav-footer-center\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n    &lt;div class=\"nav-footer-right\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#about-the-project",
    "href": "posts/2022_05_28_UK-accidents/index.html#about-the-project",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "",
    "text": "&lt;/div&gt;\n&lt;/section&gt;\n&lt;section id=\"requirement-gathering\" class=\"level2\"&gt;\n&lt;h2&gt;Requirement Gathering&lt;/h2&gt;\n&lt;p&gt;Client wants to create a dashboard on road accidents for the year 2021 and 2022.&lt;/p&gt;\n&lt;section id=\"a.-primary-kpis\" class=\"level4\"&gt;\n&lt;h4&gt;a. Primary KPIs&lt;/h4&gt;\n&lt;ul&gt;\n&lt;li&gt;Total Casualties and Total Accident Values:\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;li&gt;Year-over-Year (YoY) Growth&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Total Casualties by Accident Severity:\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;li&gt;Year-over-Year (YoY) Growth&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"b.-secondary-kpis\" class=\"level4\"&gt;\n&lt;h4&gt;b. Secondary KPIs&lt;/h4&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Total Casualties with Respect to Vehicle Type:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Monthly Trend Comparison of Casualties:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year vs. Previous Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Casualties by Road Type:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Current Year Casualties by Area/Location &amp; Day/Night&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Total Casualties and Total Accidents by Location Total casualties and total accidents by location&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n&lt;section id=\"identifying-stakeholders\" class=\"level2\"&gt;\n&lt;h2&gt;Identifying Stakeholders 🧑🏽‍💼&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Emergency Services Departments&lt;/li&gt;\n&lt;li&gt;Road Safety Corps&lt;/li&gt;\n&lt;li&gt;Traffic Management Agencies&lt;/li&gt;\n&lt;li&gt;Police Force&lt;/li&gt;\n&lt;li&gt;General public&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"understanding-raw-data\" class=\"level2\"&gt;\n&lt;h2&gt;Understanding Raw Data 🥩&lt;/h2&gt;\n&lt;p&gt;Data Grain: A single instance of a reported accident event in the UK in 2021 and 2022&lt;/p&gt;\n&lt;table&gt;\n&lt;colgroup&gt;\n&lt;col style=\"width: 31%\" /&gt;\n&lt;col style=\"width: 68%\" /&gt;\n&lt;/colgroup&gt;\n&lt;thead&gt;\n&lt;tr class=\"header\"&gt;\n&lt;th&gt;Fields&lt;/th&gt;\n&lt;th&gt;description (datatype)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Accident_Index&lt;/td&gt;\n&lt;td&gt;unique row identifier (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Accident Date&lt;/td&gt;\n&lt;td&gt;date of accident event (date)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Day_of_Week&lt;/td&gt;\n&lt;td&gt;day of the week (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Junction_Control&lt;/td&gt;\n&lt;td&gt;mechanism of traffic control at junction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Junction_Detail&lt;/td&gt;\n&lt;td&gt;type of junction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Accident_Severity&lt;/td&gt;\n&lt;td&gt;severity of accident (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Latitude&lt;/td&gt;\n&lt;td&gt;latitude (float)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Light_Conditions&lt;/td&gt;\n&lt;td&gt;light conditions at accident scene (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Local_Authority_(District)&lt;/td&gt;\n&lt;td&gt;name of district (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Carriageway_Hazards&lt;/td&gt;\n&lt;td&gt;hazards on the road if any&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Number_of_Casualties&lt;/td&gt;\n&lt;td&gt;number of causalities (integer)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Number_of_Vehicles&lt;/td&gt;\n&lt;td&gt;number of vehicles involved in the accident (integer)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Police_Force&lt;/td&gt;\n&lt;td&gt;Police force jurisdiction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Road_Surface_Conditions&lt;/td&gt;\n&lt;td&gt;condition of road surface (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Road_Type&lt;/td&gt;\n&lt;td&gt;road type (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Speed_limit&lt;/td&gt;\n&lt;td&gt;road speed limit in mph (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Time&lt;/td&gt;\n&lt;td&gt;time of day accident occurred (time)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Urban_or_Rural_Area&lt;/td&gt;\n&lt;td&gt;Whether accident occurred in an urban or rural area (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Weather_Conditions&lt;/td&gt;\n&lt;td&gt;weather condition at time of accident (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Vehicle_Type&lt;/td&gt;\n&lt;td&gt;type of vehicle involved in the accident&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n&lt;/section&gt;\n&lt;section id=\"data-cleansing\" class=\"level2\"&gt;\n&lt;h2&gt;Data cleansing 🧹&lt;/h2&gt;\n&lt;p&gt;Issues - Junction_Control column has, among others, two values “Auto traffic sigl” and “Auto traffic signal” of which the former is a misspelling of the latter&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Instances where “Fatal” is misspelled as “Fetal” in Accident_Severity column&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;“Time” column was represented as a datetime data type in Power BI&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;Fixes - Used the Replace Values feature to correct the misspellings of Issues 1 &amp; 2 - Changed the data type to time accordingly - All fixes were done in Power Query.&lt;/p&gt;\n&lt;section id=\"data-processing\" class=\"level3\"&gt;\n&lt;h3&gt;Data Processing ⚙️&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;The data needs a calendar table so as to use Time Intelligence functions further down the line.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Using the CALENDAR function, a new table with calculated, dynamic columns of Date, Month and Year was generated.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"data-modeling\" class=\"level3\"&gt;\n&lt;h3&gt;Data Modeling 🏛&lt;/h3&gt;\n&lt;p&gt;a one-to-many active relationship was established between the calendar table and the data table &lt;img src=\"data-model.png\" class=\"img-fluid\" alt=\"Schema\" /&gt;&lt;/p&gt;\n&lt;/section&gt;\n&lt;section id=\"data-visualization\" class=\"level3\"&gt;\n&lt;h3&gt;Data Visualization 🎨&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;Power BI magic!✨ The report pbix file is available in this repo to explore design decisions The following DAX measures were used:&lt;/li&gt;\n&lt;/ul&gt;\n&lt;pre&gt;&lt;code&gt;Count of Accidents = DISTINCTCOUNT(Data[Accident_Index])\n\nCurrent Year Accident Count = TOTALYTD(COUNT(Data[Accident_Index]),&#39;Calendar&#39;[Date])\n\nCurrent Year Casualties = TOTALYTD(SUM(Data[Number_of_Casualties]),&#39;Calendar&#39;[Date])\n\nPrevious Year Accident Counts = CALCULATE(COUNT(Data[Accident_Index]), SAMEPERIODLASTYEAR(&#39;Calendar&#39;[Date]))\n\nPrevious Year Casualties = CALCULATE(SUM(Data[Number_of_Casualties]),\nSAMEPERIODLASTYEAR(&#39;Calendar&#39;[Date]))\n\nYoY Accident Count = DIVIDE([Current Year Accident Count] - [Previous Year Accident Counts], [Previous Year Accident Counts])\n\nYoY Casualties = (DIVIDE([Current Year Casualties] - [Previous Year Casualties],[Previous Year Casualties]))&lt;/code&gt;&lt;/pre&gt;\n&lt;/section&gt;\n&lt;section id=\"deriving-insights\" class=\"level3\"&gt;\n&lt;h3&gt;Deriving Insights&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;The vehicle type most involved in accidents was cars. This can be explained by the fact that most vehicles plying UK roads are cars&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There is a general decrease in casualty counts in 2022 compared to 2021&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n&lt;section id=\"contact\" class=\"level2\"&gt;\n&lt;h2&gt;Contact&lt;/h2&gt;\n&lt;p&gt;Edun Joshua Olubori - &lt;a href=\"https://www.linkedin.com/in/joshua-edun\"&gt;connect on linkedin&lt;/a&gt; - joshuaolubori@gmail.com&lt;/p&gt;\n&lt;p&gt;Live Demo: &lt;a href=\"https://app.powerbi.com/view?r=eyJrIjoiYjZiNDYwMTctY2EwOC00NGQ2LWJmOGEtMzVlN2I2YTFlZTEwIiwidCI6ImRmODY3OWNkLWE4MGUtNDVkOC05OWFjLWM4M2VkN2ZmOTVhMCJ9&amp;embedImagePlaceholder=true\"&gt;here&lt;/a&gt;&lt;/p&gt;\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Home\"&gt;Home&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.html\"&gt;/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Posts\"&gt;Posts&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/posts_index.html\"&gt;/posts_index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"footer-left\"&gt;Blog made with 💚 and &lt;a href=\"https://quarto.org/\"&gt;Quarto&lt;/a&gt;, by Edun Joshua. License: &lt;a href=\"https://creativecommons.org/licenses/by-sa/2.0/\"&gt;CC BY-SA 2.0&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;Featuring data modeling in Power BI, and Dax&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;Featuring data modeling in Power BI, and Dax&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;script src=\"https://giscus.app/client.js\"\n        data-repo=\"JoshuaOlubori/qblog\"\n        data-repo-id=\"R_kgDOKvhXew\"\n        data-category=\"General\"\n        data-category-id=\"DIC_kwDOKvhXe84CbFeH\"\n        data-mapping=\"title\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"top\"\n        data-theme=\"light\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async&gt;\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n&lt;footer class=\"footer\"&gt;\n  &lt;div class=\"nav-footer\"&gt;\n    &lt;div class=\"nav-footer-left\"&gt;\n      &lt;div class='footer-contents'&gt;Blog made with 💚 and [Quarto](https://quarto.org/),  by Edun Joshua.  License: [CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/).&lt;/div&gt;  \n    &lt;/div&gt;   \n    &lt;div class=\"nav-footer-center\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n    &lt;div class=\"nav-footer-right\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#requirement-gathering",
    "href": "posts/2022_05_28_UK-accidents/index.html#requirement-gathering",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "Requirement Gathering",
    "text": "Requirement Gathering\nClient wants to create a dashboard on road accidents for the year 2021 and 2022.\n\na. Primary KPIs\n\nTotal Casualties and Total Accident Values:\n\nCurrent Year\nYear-over-Year (YoY) Growth\n\nTotal Casualties by Accident Severity:\n\nCurrent Year\nYear-over-Year (YoY) Growth\n\n\n\n\nb. Secondary KPIs\n\nTotal Casualties with Respect to Vehicle Type:\n\nCurrent Year\n\nMonthly Trend Comparison of Casualties:\n\nCurrent Year vs. Previous Year\n\nCasualties by Road Type:\n\nCurrent Year\n\nCurrent Year Casualties by Area/Location & Day/Night\nTotal Casualties and Total Accidents by Location Total casualties and total accidents by location"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#identifying-stakeholders",
    "href": "posts/2022_05_28_UK-accidents/index.html#identifying-stakeholders",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "Identifying Stakeholders 🧑🏽‍💼",
    "text": "Identifying Stakeholders 🧑🏽‍💼\n\nEmergency Services Departments\nRoad Safety Corps\nTraffic Management Agencies\nPolice Force\nGeneral public"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#understanding-raw-data",
    "href": "posts/2022_05_28_UK-accidents/index.html#understanding-raw-data",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "Understanding Raw Data 🥩",
    "text": "Understanding Raw Data 🥩\nData Grain: A single instance of a reported accident event in the UK in 2021 and 2022\n\n\n\n\n\n\n\nFields\ndescription (datatype)\n\n\n\n\nAccident_Index\nunique row identifier (string)\n\n\nAccident Date\ndate of accident event (date)\n\n\nDay_of_Week\nday of the week (string)\n\n\nJunction_Control\nmechanism of traffic control at junction (string)\n\n\nJunction_Detail\ntype of junction (string)\n\n\nAccident_Severity\nseverity of accident (string)\n\n\nLatitude\nlatitude (float)\n\n\nLight_Conditions\nlight conditions at accident scene (string)\n\n\nLocal_Authority_(District)\nname of district (string)\n\n\nCarriageway_Hazards\nhazards on the road if any\n\n\nNumber_of_Casualties\nnumber of causalities (integer)\n\n\nNumber_of_Vehicles\nnumber of vehicles involved in the accident (integer)\n\n\nPolice_Force\nPolice force jurisdiction (string)\n\n\nRoad_Surface_Conditions\ncondition of road surface (string)\n\n\nRoad_Type\nroad type (string)\n\n\nSpeed_limit\nroad speed limit in mph (string)\n\n\nTime\ntime of day accident occurred (time)\n\n\nUrban_or_Rural_Area\nWhether accident occurred in an urban or rural area (string)\n\n\nWeather_Conditions\nweather condition at time of accident (string)\n\n\nVehicle_Type\ntype of vehicle involved in the accident"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#data-cleansing",
    "href": "posts/2022_05_28_UK-accidents/index.html#data-cleansing",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "Data cleansing 🧹",
    "text": "Data cleansing 🧹\nIssues - Junction_Control column has, among others, two values “Auto traffic sigl” and “Auto traffic signal” of which the former is a misspelling of the latter\n\nInstances where “Fatal” is misspelled as “Fetal” in Accident_Severity column\n“Time” column was represented as a datetime data type in Power BI\n\nFixes - Used the Replace Values feature to correct the misspellings of Issues 1 & 2 - Changed the data type to time accordingly - All fixes were done in Power Query.\n\nData Processing ⚙️\n\nThe data needs a calendar table so as to use Time Intelligence functions further down the line.\nUsing the CALENDAR function, a new table with calculated, dynamic columns of Date, Month and Year was generated.\n\n\n\nData Modeling 🏛\na one-to-many active relationship was established between the calendar table and the data table \n\n\nData Visualization 🎨\n\nPower BI magic!✨ The report pbix file is available in this repo to explore design decisions The following DAX measures were used:\n\nCount of Accidents = DISTINCTCOUNT(Data[Accident_Index])\n\nCurrent Year Accident Count = TOTALYTD(COUNT(Data[Accident_Index]),'Calendar'[Date])\n\nCurrent Year Casualties = TOTALYTD(SUM(Data[Number_of_Casualties]),'Calendar'[Date])\n\nPrevious Year Accident Counts = CALCULATE(COUNT(Data[Accident_Index]), SAMEPERIODLASTYEAR('Calendar'[Date]))\n\nPrevious Year Casualties = CALCULATE(SUM(Data[Number_of_Casualties]),\nSAMEPERIODLASTYEAR('Calendar'[Date]))\n\nYoY Accident Count = DIVIDE([Current Year Accident Count] - [Previous Year Accident Counts], [Previous Year Accident Counts])\n\nYoY Casualties = (DIVIDE([Current Year Casualties] - [Previous Year Casualties],[Previous Year Casualties]))\n\n\nDeriving Insights\n\nThe vehicle type most involved in accidents was cars. This can be explained by the fact that most vehicles plying UK roads are cars\nThere is a general decrease in casualty counts in 2022 compared to 2021"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#contact",
    "href": "posts/2022_05_28_UK-accidents/index.html#contact",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "Contact",
    "text": "Contact\nEdun Joshua Olubori - connect on linkedin - joshuaolubori@gmail.com\nLive Demo: here"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/nga_covid19.html",
    "href": "posts/2022_05_28_COVID-NGA-analysis/nga_covid19.html",
    "title": "Chamber of (data) secrets",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot\nimport seaborn as sns\nimport matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nImporting the neccesary libraries..\nReading the files and inspecting the data…\n\nnga = pd.read_csv(\"nga_subnational_covid19_hera.csv\", sep=\";\")\n\n\n#checking the columns\nnga.columns\n\nIndex(['ID', 'DATE', 'ISO_3', 'PAYS', 'ID_PAYS', 'REGION', 'ID_REGION',\n       'CONTAMINES', 'DECES', 'GUERIS', 'CONTAMINES_FEMME', 'CONTAMINES_HOMME',\n       'CONTAMINES_GENRE_NON_SPECIFIE', 'SOURCE'],\n      dtype='object')\n\n\n\nnga.shape\n\n(26866, 14)\n\n\n\nprint(nga.head(10))\n\n   ID        DATE ISO_3     PAYS  ID_PAYS       REGION  ID_REGION  CONTAMINES  \\\n0   1  27/02/2020   NGA  Nigéria       15         Abia        170         0.0   \n1   2  27/02/2020   NGA  Nigéria       15      Adamawa        171         0.0   \n2   3  27/02/2020   NGA  Nigéria       15    Akwa Ibom        172         0.0   \n3   4  27/02/2020   NGA  Nigéria       15      Anambra        173         0.0   \n4   5  27/02/2020   NGA  Nigéria       15       Bauchi        174         0.0   \n5   6  27/02/2020   NGA  Nigéria       15      Bayelsa        175         0.0   \n6   7  27/02/2020   NGA  Nigéria       15        Benue        176         0.0   \n7   8  27/02/2020   NGA  Nigéria       15        Borno        177         0.0   \n8   9  27/02/2020   NGA  Nigéria       15  Cross River        178         0.0   \n9  10  27/02/2020   NGA  Nigéria       15        Delta        179         0.0   \n\n   DECES  GUERIS  CONTAMINES_FEMME  CONTAMINES_HOMME  \\\n0    0.0     0.0               0.0               0.0   \n1    0.0     0.0               0.0               0.0   \n2    0.0     0.0               0.0               0.0   \n3    0.0     0.0               0.0               0.0   \n4    0.0     0.0               0.0               0.0   \n5    0.0     0.0               0.0               0.0   \n6    0.0     0.0               0.0               0.0   \n7    0.0     0.0               0.0               0.0   \n8    0.0     0.0               0.0               0.0   \n9    0.0     0.0               0.0               0.0   \n\n   CONTAMINES_GENRE_NON_SPECIFIE                              SOURCE  \n0                            0.0  Nigeria Centre for Disease Control  \n1                            0.0  Nigeria Centre for Disease Control  \n2                            0.0  Nigeria Centre for Disease Control  \n3                            0.0  Nigeria Centre for Disease Control  \n4                            0.0  Nigeria Centre for Disease Control  \n5                            0.0  Nigeria Centre for Disease Control  \n6                            0.0  Nigeria Centre for Disease Control  \n7                            0.0  Nigeria Centre for Disease Control  \n8                            0.0  Nigeria Centre for Disease Control  \n9                            0.0  Nigeria Centre for Disease Control  \n\n\n\nnga.tail()\n\n\n\n\n\n\n\n\nID\nDATE\nISO_3\nPAYS\nID_PAYS\nREGION\nID_REGION\nCONTAMINES\nDECES\nGUERIS\nCONTAMINES_FEMME\nCONTAMINES_HOMME\nCONTAMINES_GENRE_NON_SPECIFIE\nSOURCE\n\n\n\n\n26861\n26862\n02/02/2022\nNGA\nNigéria\n15\nSokoto\n203\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n26862\n26863\n02/02/2022\nNGA\nNigéria\n15\nTaraba\n204\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n26863\n26864\n02/02/2022\nNGA\nNigéria\n15\nYobe\n205\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n26864\n26865\n02/02/2022\nNGA\nNigéria\n15\nZamfara\n206\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n26865\n26866\n02/02/2022\nNGA\nNigéria\n15\nNon spécifié\n207\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n\n\n\n\n\n\nnga.describe()\n#negative numbers present in contamines,gueris, homme, non_specifie columns\n#\n\n\n\n\n\n\n\n\nID\nID_PAYS\nID_REGION\nCONTAMINES\nDECES\nGUERIS\nCONTAMINES_FEMME\nCONTAMINES_HOMME\nCONTAMINES_GENRE_NON_SPECIFIE\n\n\n\n\ncount\n26866.000000\n26866.0\n26866.00000\n26802.000000\n26463.000000\n26280.000000\n17734.000000\n17734.000000\n23663.000000\n\n\nmean\n13433.500000\n15.0\n188.50000\n9.440452\n0.118467\n8.714574\n2.197023\n1.259276\n8.102481\n\n\nstd\n7755.690502\n0.0\n10.96606\n45.747271\n0.942725\n107.723565\n25.365332\n15.169063\n46.874095\n\n\nmin\n1.000000\n15.0\n170.00000\n-1.000000\n0.000000\n-46.000000\n0.000000\n-90.000000\n-1.000000\n\n\n25%\n6717.250000\n15.0\n179.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n13433.500000\n15.0\n188.50000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n20149.750000\n15.0\n198.00000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n26866.000000\n15.0\n207.00000\n3393.000000\n79.000000\n10946.000000\n696.000000\n493.000000\n3393.000000\n\n\n\n\n\n\n\nChecking for null values…\n\nnga.isna().sum()\n\nID                                  0\nDATE                                0\nISO_3                               0\nPAYS                                0\nID_PAYS                             0\nREGION                              0\nID_REGION                           0\nCONTAMINES                         64\nDECES                             403\nGUERIS                            586\nCONTAMINES_FEMME                 9132\nCONTAMINES_HOMME                 9132\nCONTAMINES_GENRE_NON_SPECIFIE    3203\nSOURCE                              0\ndtype: int64\n\n\nFilling in the null values with the median values per column\n\nnga['CONTAMINES']=nga.CONTAMINES.fillna(nga.CONTAMINES.median())\nnga['DECES']=nga.DECES.fillna(nga.DECES.median())\nnga['GUERIS']=nga.GUERIS.fillna(nga.GUERIS.median())\nnga['CONTAMINES_FEMME']=nga.CONTAMINES_FEMME.fillna(nga.CONTAMINES_FEMME.median())\nnga['CONTAMINES_HOMME']=nga.CONTAMINES_HOMME.fillna(nga.CONTAMINES_HOMME.median())\nnga['CONTAMINES_GENRE_NON_SPECIFIE']=nga.CONTAMINES_GENRE_NON_SPECIFIE.fillna(nga.CONTAMINES_GENRE_NON_SPECIFIE.median())\n\n\n#checking for null values\nnga.isna().sum()\n\nID                               0\nDATE                             0\nISO_3                            0\nPAYS                             0\nID_PAYS                          0\nREGION                           0\nID_REGION                        0\nCONTAMINES                       0\nDECES                            0\nGUERIS                           0\nCONTAMINES_FEMME                 0\nCONTAMINES_HOMME                 0\nCONTAMINES_GENRE_NON_SPECIFIE    0\nSOURCE                           0\ndtype: int64\n\n\n\nnga.describe()\n\n\n\n\n\n\n\n\nID\nID_PAYS\nID_REGION\nCONTAMINES\nDECES\nGUERIS\nCONTAMINES_FEMME\nCONTAMINES_HOMME\nCONTAMINES_GENRE_NON_SPECIFIE\n\n\n\n\ncount\n26866.000000\n26866.0\n26866.00000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n\n\nmean\n13433.500000\n15.0\n188.50000\n9.417963\n0.116690\n8.524492\n1.450234\n0.831237\n7.136492\n\n\nstd\n7755.690502\n0.0\n10.96606\n45.695065\n0.935739\n106.549817\n20.634374\n12.338562\n44.069426\n\n\nmin\n1.000000\n15.0\n170.00000\n-1.000000\n0.000000\n-46.000000\n0.000000\n-90.000000\n-1.000000\n\n\n25%\n6717.250000\n15.0\n179.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n13433.500000\n15.0\n188.50000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n20149.750000\n15.0\n198.00000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n26866.000000\n15.0\n207.00000\n3393.000000\n79.000000\n10946.000000\n696.000000\n493.000000\n3393.000000\n\n\n\n\n\n\n\nI observe from the code above that some columns contain negative values. These are probable input errors. I replace them with the median values per column.\n\nimport numpy as np\nnga.GUERIS = np.abs(nga.GUERIS)\nnga.CONTAMINES = np.abs(nga.CONTAMINES)\nnga.CONTAMINES_HOMME = np.abs(nga.CONTAMINES_HOMME)\nnga.CONTAMINES = np.abs(nga.CONTAMINES)\nnga.CONTAMINES_GENRE_NON_SPECIFIE = np.abs(nga.CONTAMINES_GENRE_NON_SPECIFIE)\n\n\nnga.describe()\n\n\n\n\n\n\n\n\nID\nID_PAYS\nID_REGION\nCONTAMINES\nDECES\nGUERIS\nCONTAMINES_FEMME\nCONTAMINES_HOMME\nCONTAMINES_GENRE_NON_SPECIFIE\n\n\n\n\ncount\n26866.000000\n26866.0\n26866.00000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n\n\nmean\n13433.500000\n15.0\n188.50000\n9.418038\n0.116690\n8.528735\n1.450234\n0.837936\n7.136567\n\n\nstd\n7755.690502\n0.0\n10.96606\n45.695049\n0.935739\n106.549477\n20.634374\n12.338109\n44.069414\n\n\nmin\n1.000000\n15.0\n170.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n6717.250000\n15.0\n179.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n13433.500000\n15.0\n188.50000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n20149.750000\n15.0\n198.00000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n26866.000000\n15.0\n207.00000\n3393.000000\n79.000000\n10946.000000\n696.000000\n493.000000\n3393.000000\n\n\n\n\n\n\n\n\nnga.head()\n\n\n\n\n\n\n\n\nID\nDATE\nISO_3\nPAYS\nID_PAYS\nREGION\nID_REGION\nCONTAMINES\nDECES\nGUERIS\nCONTAMINES_FEMME\nCONTAMINES_HOMME\nCONTAMINES_GENRE_NON_SPECIFIE\nSOURCE\n\n\n\n\n0\n1\n27/02/2020\nNGA\nNigéria\n15\nAbia\n170\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n1\n2\n27/02/2020\nNGA\nNigéria\n15\nAdamawa\n171\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n2\n3\n27/02/2020\nNGA\nNigéria\n15\nAkwa Ibom\n172\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n3\n4\n27/02/2020\nNGA\nNigéria\n15\nAnambra\n173\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n4\n5\n27/02/2020\nNGA\nNigéria\n15\nBauchi\n174\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n\n\n\n\n\nNow the values are all integers\nThe columns are in French. I rename each accordingly below\n\nnga.rename(columns={\n    'ID': 'id',\n    'DATE': 'date',\n    'ISO_3': 'iso_3',\n    'PAYS': 'country',\n    'ID_PAYS': 'country_id',\n    'REGION': 'state',\n    'ID_REGION': 'state_id',\n    'CONTAMINES': 'confirmed',\n    'DECES': 'deaths',\n    'GUERIS': 'recovered',\n    'CONTAMINES_FEMME': 'confirmed_female',\n    'CONTAMINES_HOMME': 'confirmed_male',\n    'CONTAMINES_GENRE_NON_SPECIFIE': 'confirmed_gender_ns',\n    'SOURCE': 'source'\n}, inplace=True)\n\n\n\nnga.head()\n\n\n\n\n\n\n\n\nid\ndate\niso_3\ncountry\ncountry_id\nstate\nstate_id\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\nsource\n\n\n\n\n0\n1\n27/02/2020\nNGA\nNigéria\n15\nAbia\n170\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n1\n2\n27/02/2020\nNGA\nNigéria\n15\nAdamawa\n171\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n2\n3\n27/02/2020\nNGA\nNigéria\n15\nAkwa Ibom\n172\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n3\n4\n27/02/2020\nNGA\nNigéria\n15\nAnambra\n173\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n4\n5\n27/02/2020\nNGA\nNigéria\n15\nBauchi\n174\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n\n\n\n\n\n‘Nigeria’ is misspelt. Correcting the error.\n\nnga.country=nga.country.replace(['Nigéria'],'Nigeria')\nnga.head()\n\n\n\n\n\n\n\n\nid\ndate\niso_3\ncountry\ncountry_id\nstate\nstate_id\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\nsource\n\n\n\n\n0\n1\n27/02/2020\nNGA\nNigeria\n15\nAbia\n170\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n1\n2\n27/02/2020\nNGA\nNigeria\n15\nAdamawa\n171\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n2\n3\n27/02/2020\nNGA\nNigeria\n15\nAkwa Ibom\n172\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n3\n4\n27/02/2020\nNGA\nNigeria\n15\nAnambra\n173\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n4\n5\n27/02/2020\nNGA\nNigeria\n15\nBauchi\n174\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n\n\n\n\n\n\n# Check that there is only one instance of each country\nnga['country'].value_counts()\n\nNigeria    26866\nName: country, dtype: int64\n\n\n\n#checking that there are 36 states\nstates = nga['state']\nprint(states)\n\n0                Abia\n1             Adamawa\n2           Akwa Ibom\n3             Anambra\n4              Bauchi\n             ...     \n26861          Sokoto\n26862          Taraba\n26863            Yobe\n26864         Zamfara\n26865    Non spécifié\nName: state, Length: 26866, dtype: object\n\n\n\n\nprint(states)\n\n0                Abia\n1             Adamawa\n2           Akwa Ibom\n3             Anambra\n4              Bauchi\n             ...     \n26861          Sokoto\n26862          Taraba\n26863            Yobe\n26864         Zamfara\n26865    Non spécifié\nName: state, Length: 26866, dtype: object\n\n\n\n#checking for this unwanted value\nnga.state[26865]\n\n'Non spécifié'\n\n\n\n#converting date datattype to datetime\nnga.date = pd.to_datetime(nga.date)\n\n\n#checking if conversion is successfull\nnga.dtypes\n\nid                              int64\ndate                   datetime64[ns]\niso_3                          object\ncountry                        object\ncountry_id                      int64\nstate                          object\nstate_id                        int64\nconfirmed                     float64\ndeaths                        float64\nrecovered                     float64\nconfirmed_female              float64\nconfirmed_male                float64\nconfirmed_gender_ns           float64\nsource                         object\ndtype: object\n\n\n\n#dropping unwanted columns\nnga.drop(['iso_3', 'country_id', 'state_id', 'id'], axis=1, inplace=True)\n\n\n#removing an error value (row)\nnga.drop([26865], inplace=True)\n\n\n#removing an unwanted column\nnga.drop(['source'], axis=1, inplace=True)\n\n\nprint(nga.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 26865 entries, 0 to 26864\nData columns (total 9 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26865 non-null  datetime64[ns]\n 1   country              26865 non-null  object        \n 2   state                26865 non-null  object        \n 3   confirmed            26865 non-null  float64       \n 4   deaths               26865 non-null  float64       \n 5   recovered            26865 non-null  float64       \n 6   confirmed_female     26865 non-null  float64       \n 7   confirmed_male       26865 non-null  float64       \n 8   confirmed_gender_ns  26865 non-null  float64       \ndtypes: datetime64[ns](1), float64(6), object(2)\nmemory usage: 1.8+ MB\nNone\n\n\n\n#number of confirmed cases\nprint(nga.confirmed.sum())\nprint(nga.deaths.sum())\nprint(nga.recovered.sum())\n\n253025.0\n3135.0\n229133.0\n\n\n\n#checking most states with confirmed cases\nnga.sort_values(by=['confirmed'], ascending=False)\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\n\n\n\n\n25256\n2021-12-22\nNigeria\nLagos\n3393.0\n0.0\n0.0\n0.0\n0.0\n3393.0\n\n\n25218\n2021-12-21\nNigeria\nLagos\n1552.0\n0.0\n0.0\n0.0\n0.0\n1552.0\n\n\n12868\n2021-01-30\nNigeria\nLagos\n1040.0\n0.0\n461.0\n0.0\n0.0\n1040.0\n\n\n25066\n2021-12-17\nNigeria\nLagos\n906.0\n0.0\n0.0\n0.0\n0.0\n906.0\n\n\n12374\n2021-01-17\nNigeria\nLagos\n901.0\n2.0\n717.0\n0.0\n0.0\n901.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10456\n2020-11-28\nNigeria\nBenue\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n10455\n2020-11-28\nNigeria\nBayelsa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n10454\n2020-11-28\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n10453\n2020-11-28\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n26864\n2022-02-02\nNigeria\nZamfara\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n26865 rows × 9 columns\n\n\n\n\n#exporting the data\nout_nga = 'nga_covid19.csv'\nnga.to_csv(out_nga)\n\n\nnga.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 26865 entries, 0 to 26864\nData columns (total 9 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26865 non-null  datetime64[ns]\n 1   country              26865 non-null  object        \n 2   state                26865 non-null  object        \n 3   confirmed            26865 non-null  float64       \n 4   deaths               26865 non-null  float64       \n 5   recovered            26865 non-null  float64       \n 6   confirmed_female     26865 non-null  float64       \n 7   confirmed_male       26865 non-null  float64       \n 8   confirmed_gender_ns  26865 non-null  float64       \ndtypes: datetime64[ns](1), float64(6), object(2)\nmemory usage: 1.8+ MB\n\n\n\n#reading population table. The goal is to merge with nga\nimport pandas as pd\npop_nga = pd.read_csv(r\"C:\\Users\\eduna\\Documents\\state_pop.txt\", sep='\\s+')\npop_nga.head()\n\n\n\n\n\n\n\n\nstate\npopulation_(2022)\n\n\n\n\n0\nLagos\n21,000,000\n\n\n1\nKano\n12,550,598\n\n\n2\nKaduna\n8,252,366\n\n\n3\nOyo\n7,840,864\n\n\n4\nKatsina\n7,831,319\n\n\n\n\n\n\n\n\n#correcting some states misspellings\npop_nga.state=pop_nga.state.replace(['Akwa-Ibom'],'Akwa Ibom')\npop_nga.state=pop_nga.state.replace(['Cross-River'],'Cross River')\n\n\n#checking the correction\npop_nga.state[13]\n\n'Akwa Ibom'\n\n\n\n#checking that data is clean\npop_nga.isna().sum()\npop_nga.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 37 entries, 0 to 36\nData columns (total 2 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   state              37 non-null     object\n 1   population_(2022)  37 non-null     object\ndtypes: object(2)\nmemory usage: 720.0+ bytes\n\n\n\n#exporting the data\npop_nga.to_csv('pop_nga.csv')\n\n\nnga.head()\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\n\n\n\n\n0\n2020-02-27\nNigeria\nAbia\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n2020-02-27\nNigeria\nAdamawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n2020-02-27\nNigeria\nAkwa Ibom\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n2020-02-27\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n2020-02-27\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n#dropping rows with 'Non specifie'\nnga.drop(nga.index[nga['state'] == 'Non spécifié'], inplace=True)\nnga['state']\n\n0             Abia\n1          Adamawa\n2        Akwa Ibom\n3          Anambra\n4           Bauchi\n           ...    \n26860       Rivers\n26861       Sokoto\n26862       Taraba\n26863         Yobe\n26864      Zamfara\nName: state, Length: 26159, dtype: object\n\n\n\n#merging the data with population data\nnga_merged = pd.merge(nga, pop_nga, on='state', how='left')\nnga_merged.head(45)\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\n\n\n\n\n0\n2020-02-27\nNigeria\nAbia\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,727,347\n\n\n1\n2020-02-27\nNigeria\nAdamawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,248,436\n\n\n2\n2020-02-27\nNigeria\nAkwa Ibom\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,482,177\n\n\n3\n2020-02-27\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,527,809\n\n\n4\n2020-02-27\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6,537,314\n\n\n5\n2020-02-27\nNigeria\nBayelsa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2,277,961\n\n\n6\n2020-02-27\nNigeria\nBenue\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,741,815\n\n\n7\n2020-02-27\nNigeria\nBorno\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,860,183\n\n\n8\n2020-02-27\nNigeria\nCross River\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,866,269\n\n\n9\n2020-02-27\nNigeria\nDelta\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,663,362\n\n\n10\n2020-02-27\nNigeria\nEbonyi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2,880,383\n\n\n11\n2020-02-27\nNigeria\nEdo\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,235,595\n\n\n12\n2020-02-27\nNigeria\nEkiti\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,270,798\n\n\n13\n2020-02-27\nNigeria\nEnugu\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,411,119\n\n\n14\n2020-02-27\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n15\n2020-02-27\nNigeria\nGombe\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,256,962\n\n\n16\n2020-02-27\nNigeria\nImo\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,408,756\n\n\n17\n2020-02-27\nNigeria\nJigawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,828,163\n\n\n18\n2020-02-27\nNigeria\nKaduna\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n8,252,366\n\n\n19\n2020-02-27\nNigeria\nKano\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12,550,598\n\n\n20\n2020-02-27\nNigeria\nKatsina\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n7,831,319\n\n\n21\n2020-02-27\nNigeria\nKebbi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,440,050\n\n\n22\n2020-02-27\nNigeria\nKogi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,473,490\n\n\n23\n2020-02-27\nNigeria\nKwara\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,192,893\n\n\n24\n2020-02-27\nNigeria\nLagos\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n21,000,000\n\n\n25\n2020-02-27\nNigeria\nNasarawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2,523,395\n\n\n26\n2020-02-27\nNigeria\nNiger\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,556,247\n\n\n27\n2020-02-27\nNigeria\nOgun\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n5,217,716\n\n\n28\n2020-02-27\nNigeria\nOsun\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,705,589\n\n\n29\n2020-02-27\nNigeria\nOndo\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,671,695\n\n\n30\n2020-02-27\nNigeria\nOyo\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n7,840,864\n\n\n31\n2020-02-27\nNigeria\nPlateau\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,200,442\n\n\n32\n2020-02-27\nNigeria\nRivers\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n7,303,924\n\n\n33\n2020-02-27\nNigeria\nSokoto\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,998,090\n\n\n34\n2020-02-27\nNigeria\nTaraba\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,066,834\n\n\n35\n2020-02-27\nNigeria\nYobe\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,294,137\n\n\n36\n2020-02-27\nNigeria\nZamfara\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,515,427\n\n\n37\n2020-02-28\nNigeria\nAbia\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,727,347\n\n\n38\n2020-02-28\nNigeria\nAdamawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,248,436\n\n\n39\n2020-02-28\nNigeria\nAkwa Ibom\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,482,177\n\n\n40\n2020-02-28\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,527,809\n\n\n41\n2020-02-28\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6,537,314\n\n\n42\n2020-02-28\nNigeria\nBayelsa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2,277,961\n\n\n43\n2020-02-28\nNigeria\nBenue\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,741,815\n\n\n44\n2020-02-28\nNigeria\nBorno\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,860,183\n\n\n\n\n\n\n\n\n#exporting the data\nnga_merged.to_csv('nga_merged.csv')\n\nPermissionError: [Errno 13] Permission denied: 'nga_merged.csv'\n\n\n\n#checking the newly merged column\nnga_merged.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 26159 entries, 0 to 26158\nData columns (total 10 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26159 non-null  datetime64[ns]\n 1   country              26159 non-null  object        \n 2   state                26159 non-null  object        \n 3   confirmed            26159 non-null  float64       \n 4   deaths               26159 non-null  float64       \n 5   recovered            26159 non-null  float64       \n 6   confirmed_female     26159 non-null  float64       \n 7   confirmed_male       26159 non-null  float64       \n 8   confirmed_gender_ns  26159 non-null  float64       \n 9   population_(2022)    25452 non-null  object        \ndtypes: datetime64[ns](1), float64(6), object(3)\nmemory usage: 2.2+ MB\n\n\n\n#checking for the columns with null values\nnga_merged.isnull().sum()\n\ndate                     0\ncountry                  0\nstate                    0\nconfirmed                0\ndeaths                   0\nrecovered                0\nconfirmed_female         0\nconfirmed_male           0\nconfirmed_gender_ns      0\npopulation_(2022)      707\ndtype: int64\n\n\nThe population column has some missing values. Let’s check that out…\n\nnga_merged[nga_merged['population_(2022)'].isnull()]\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\n\n\n\n\n14\n2020-02-27\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n51\n2020-02-28\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n88\n2020-02-29\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n125\n2020-01-03\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n162\n2020-02-03\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n25988\n2022-01-29\nNigeria\nFederal Capital Territory\n3.0\n0.0\n0.0\n0.0\n0.0\n3.0\nNaN\n\n\n26025\n2022-01-30\nNigeria\nFederal Capital Territory\n6.0\n0.0\n245.0\n0.0\n0.0\n6.0\nNaN\n\n\n26062\n2022-01-31\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n26099\n2022-01-02\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n26136\n2022-02-02\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n\n\n707 rows × 10 columns\n\n\n\nThe problem seems to be that values for the Federal Capital Territory were not inputed during the merging\n\n#fixing the nans in the population column by inserting with the value of the population of the FCT\nnga_merged['population_(2022)'] = nga_merged['population_(2022)'].fillna(3564126)\n#confirming that no nulls are present\nnga_merged[nga_merged['population_(2022)'].isnull()]\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\n\n\n\n\n\n\n\n\n\n\nnga_merged.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 26159 entries, 0 to 26158\nData columns (total 10 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26159 non-null  datetime64[ns]\n 1   country              26159 non-null  object        \n 2   state                26159 non-null  object        \n 3   confirmed            26159 non-null  float64       \n 4   deaths               26159 non-null  float64       \n 5   recovered            26159 non-null  float64       \n 6   confirmed_female     26159 non-null  float64       \n 7   confirmed_male       26159 non-null  float64       \n 8   confirmed_gender_ns  26159 non-null  float64       \n 9   population_(2022)    26159 non-null  object        \ndtypes: datetime64[ns](1), float64(6), object(3)\nmemory usage: 2.2+ MB\n\n\nCreating separate columns for year, month and date in preparation for aggregation\n\n#Extracting the year from the date column\nnga_merged['year'] = pd.DatetimeIndex(nga_merged['date']).year\n#confirming\nnga_merged.head()\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\nyear\n\n\n\n\n0\n2020-02-27\nNigeria\nAbia\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,727,347\n2020\n\n\n1\n2020-02-27\nNigeria\nAdamawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,248,436\n2020\n\n\n2\n2020-02-27\nNigeria\nAkwa Ibom\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,482,177\n2020\n\n\n3\n2020-02-27\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,527,809\n2020\n\n\n4\n2020-02-27\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6,537,314\n2020\n\n\n\n\n\n\n\n\n#Extracting month\nnga_merged['month'] = pd.DatetimeIndex(nga_merged['date']).month\n#confirming\nnga_merged.head()\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\nyear\nmonth\n\n\n\n\n0\n2020-02-27\nNigeria\nAbia\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,727,347\n2020\n2\n\n\n1\n2020-02-27\nNigeria\nAdamawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,248,436\n2020\n2\n\n\n2\n2020-02-27\nNigeria\nAkwa Ibom\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,482,177\n2020\n2\n\n\n3\n2020-02-27\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,527,809\n2020\n2\n\n\n4\n2020-02-27\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6,537,314\n2020\n2\n\n\n\n\n\n\n\n\nnga_merged.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 26159 entries, 0 to 26158\nData columns (total 12 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26159 non-null  datetime64[ns]\n 1   country              26159 non-null  object        \n 2   state                26159 non-null  object        \n 3   confirmed            26159 non-null  float64       \n 4   deaths               26159 non-null  float64       \n 5   recovered            26159 non-null  float64       \n 6   confirmed_female     26159 non-null  float64       \n 7   confirmed_male       26159 non-null  float64       \n 8   confirmed_gender_ns  26159 non-null  float64       \n 9   population_(2022)    26159 non-null  object        \n 10  year                 26159 non-null  int64         \n 11  month                26159 non-null  int64         \ndtypes: datetime64[ns](1), float64(6), int64(2), object(3)\nmemory usage: 2.6+ MB\n\n\n\nnga_merged.to_csv(\"nga_merged_month_year.csv\")\n\n\nnga_merged['active_cases'] = nga_merged['confirmed']-(nga_merged['recovered']+nga_merged['deaths'])\nnga_merged.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 26159 entries, 0 to 26158\nData columns (total 13 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26159 non-null  datetime64[ns]\n 1   country              26159 non-null  object        \n 2   state                26159 non-null  object        \n 3   confirmed            26159 non-null  float64       \n 4   deaths               26159 non-null  float64       \n 5   recovered            26159 non-null  float64       \n 6   confirmed_female     26159 non-null  float64       \n 7   confirmed_male       26159 non-null  float64       \n 8   confirmed_gender_ns  26159 non-null  float64       \n 9   population_(2022)    26159 non-null  object        \n 10  year                 26159 non-null  int64         \n 11  month                26159 non-null  int64         \n 12  active_cases         26159 non-null  float64       \ndtypes: datetime64[ns](1), float64(7), int64(2), object(3)\nmemory usage: 2.8+ MB\n\n\n\nnga_merged[nga_merged['active_cases'].values &lt; 0]\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\nyear\nmonth\nactive_cases\n\n\n\n\n767\n2020-03-18\nNigeria\nOgun\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n5,217,716\n2020\n3\n-1.0\n\n\n878\n2020-03-21\nNigeria\nOgun\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n5,217,716\n2020\n3\n-1.0\n\n\n1233\n2020-03-31\nNigeria\nEkiti\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n3,270,798\n2020\n3\n-1.0\n\n\n1356\n2020-03-04\nNigeria\nLagos\n7.0\n0.0\n11.0\n0.0\n0.0\n0.0\n21,000,000\n2020\n3\n-4.0\n\n\n1467\n2020-06-04\nNigeria\nLagos\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n21,000,000\n2020\n6\n-1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26029\n2022-01-30\nNigeria\nKaduna\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n8,252,366\n2022\n1\n-7.0\n\n\n26030\n2022-01-30\nNigeria\nKano\n1.0\n0.0\n6.0\n0.0\n0.0\n1.0\n12,550,598\n2022\n1\n-5.0\n\n\n26038\n2022-01-30\nNigeria\nOgun\n0.0\n0.0\n3.0\n0.0\n0.0\n0.0\n5,217,716\n2022\n1\n-3.0\n\n\n26039\n2022-01-30\nNigeria\nOsun\n9.0\n0.0\n15.0\n0.0\n0.0\n9.0\n4,705,589\n2022\n1\n-6.0\n\n\n26043\n2022-01-30\nNigeria\nRivers\n2.0\n0.0\n11.0\n0.0\n0.0\n2.0\n7,303,924\n2022\n1\n-9.0\n\n\n\n\n3372 rows × 13 columns\n\n\n\nThe issue seems to be from the formula. And the negative values occur at dates when ‘recovered’ is greater than ‘confirmed’. At such cases, active cases should be zero\n\n#using clip function to correct this\nnga_merged['active_cases'] = nga_merged['active_cases'].clip(lower=0)\nnga_merged.describe()\n\n\n\n\n\n\n\n\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\nyear\nmonth\nactive_cases\n\n\n\n\ncount\n26159.000000\n26159.000000\n26159.000000\n26159.0\n26159.0\n26159.000000\n26159.000000\n26159.000000\n26159.000000\n\n\nmean\n9.672579\n0.119844\n8.759242\n0.0\n0.0\n7.329447\n2020.609618\n6.595474\n6.319584\n\n\nstd\n46.281864\n0.948101\n107.970440\n0.0\n0.0\n44.645168\n0.575629\n3.429761\n38.039798\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n2020.000000\n1.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n2020.000000\n4.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n2021.000000\n7.000000\n0.000000\n\n\n75%\n3.000000\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n2021.000000\n10.000000\n0.000000\n\n\nmax\n3393.000000\n79.000000\n10946.000000\n0.0\n0.0\n3393.000000\n2022.000000\n12.000000\n3393.000000\n\n\n\n\n\n\n\n\n#total deaths\nnga_merged['deaths'].sum()\n\n3135.0\n\n\n\n#total cases\nnga_merged['confirmed'].sum()\n\n253025.0\n\n\n\n#total recovered\nnga_merged['recovered'].sum()\n\n229133.0\n\n\nVisualizing the data\n\nsns.set(rc={'figure.figsize':(10,8)})\nsns.lineplot(x='date', y='confirmed', data=nga_merged)\n#number of confirmed cases peaked early 2022\n\n&lt;AxesSubplot:xlabel='date', ylabel='confirmed'&gt;\n\n\n\n\n\n\nsns.set(rc={'figure.figsize':(10,8)})\nsns.lineplot(x='date', y='deaths', data=nga_merged)\n#number of deaths peaked mid 2021\n\n&lt;AxesSubplot:xlabel='date', ylabel='deaths'&gt;\n\n\n\n\n\n\nsns.set(rc={'figure.figsize':(30,16)})\nsns.barplot(x='state', y='confirmed', data=nga_merged)\n\n&lt;AxesSubplot:xlabel='state', ylabel='confirmed'&gt;\n\n\n\n\n\n\nsns.set(rc={'figure.figsize':(30,16)})\nsns.barplot(x='state', y='deaths', data=nga_merged)\n#Lagos by far suffered the most fatalities. Not surprising given its massive population.\n\n&lt;AxesSubplot:xlabel='state', ylabel='deaths'&gt;\n\n\n\n\n\nGrouping the data\n\nnga_20 = nga_merged2[nga_merged2['year']==2020] # Considering data for the year 2020 only\nnga_21 = nga_merged2[nga_merged2['year']==2021] # Considering data for the year 2021 only\n#nga_22 = nga_merged2[nga_merged2['year']==2022] # Considering data for the year 2022 only\n\nGrouping by confirmed cases, recovered and deaths by month for each year since the pandemic\nFor 2020…\n\nnga_confirmed_20 = nga_20['confirmed'].groupby(nga_20['month']).sum()\nnga_confirmed_20\n\n\nplt.plot(nga_confirmed_20)\n\n\n\n\n\nnga_rec_20 = nga_20['recovered'].groupby(nga_20['month']).sum()\nnga_rec_20.head()\n\n\nplt.plot(nga_rec_20)\n\n\n\n\n\nnga_dea_20 = nga_20['deaths'].groupby(nga_20['month']).sum()\nnga_dea_20.head()\n\n\nplt.plot(nga_dea_20)\n\n\n\n\nFor 2021…\n\nnga_confirmed_21 = nga_21['confirmed'].groupby(nga_21['month']).sum()\nnga_confirmed_21.head()\n\n\nplt.plot(nga_confirmed_21)\n\n\n\n\n\nnga_rec_21 = nga_21['recovered'].groupby(nga_21['month']).sum()\nnga_rec_21.head()\n\n\nplt.plot(nga_rec_21)\n\n\n\n\n\nnga_dea_21 = nga_21['deaths'].groupby(nga_21['month']).sum()\nnga_dea_21.head\n\n\nplt.plot(nga_dea_21)\n\n\n\n\nFindings: 2020 &gt; Cases surged in the middle of year, spiking at the end of the year\n\nMosts of the deaths occurred during the middle of the year\n\nFindings: 2021 &gt; The beginning saw high number of cases, sharply falling and surging for a while in the middle of the year before rising sharply again towards the end of the year\n\nMosts of the deaths occurred during the in the beginning of the year, paralleling the rise in cases. Deaths lowered throughout the first quarter until August, its peak."
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/index.html",
    "href": "posts/2022_05_28_Adventure-works/index.html",
    "title": "Sales Monitoring Dashboard",
    "section": "",
    "text": "Client wants a dashboard monitoring sales of company’s product\n\n\n\nTop ten customers, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\nTop ten products by sales, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\n\n\n\n\n\nTotal sales by category\nDaily sales trend"
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/index.html#about-the-project",
    "href": "posts/2022_05_28_Adventure-works/index.html#about-the-project",
    "title": "Sales Monitoring Dashboard",
    "section": "",
    "text": "Client wants a dashboard monitoring sales of company’s product\n\n\n\nTop ten customers, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\nTop ten products by sales, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\n\n\n\n\n\nTotal sales by category\nDaily sales trend"
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/index.html#identifying-stakeholders",
    "href": "posts/2022_05_28_Adventure-works/index.html#identifying-stakeholders",
    "title": "Sales Monitoring Dashboard",
    "section": "Identifying Stakeholders 🧑🏽‍💼",
    "text": "Identifying Stakeholders 🧑🏽‍💼\n\nSales team\nMarketing Department\n\n\nUnderstanding Raw Data 🥩\nThe raw data exists in 9 CSV files representing 1 fact table and 8 dimension tables\n\n\n\n\n\n\n\nTable\ndescription (datatype)\n\n\n\n\nBudget\ndetails budget of each product and product categories per period\n\n\nBudgetPeriod\ndate table for the budget table\n\n\ncustomer\neach row records a customer information\n\n\ndimdates\ndate table for the fact table\n\n\ndimProductCategory\nrecords the various product categories available\n\n\ndimProductSubCategory\nlists the product subcategories available\n\n\nproduct\nlists individual products\n\n\nSales\nThe fact table: records individual sales transaction\n\n\nTerritory\nLists regional locations\n\n\n\n\n\nConverting CSV to SQL INSERT statements with Python 🐍\nI wrote a Python script using the csv and os modules, that takes in a csv file and outputs an SQL file with INSERT statements for each row in the CSV.\n\nimport csv\nimport os\n\ndef escape_quotes(value):\n    # Escape single quotes within a value by doubling them\n    return value.replace(\"'\", \"''\")\n\ndef convert_csv_to_sql(csv_file):\n    try:\n        # Extract table name from CSV file name\n        table_name = os.path.splitext(os.path.basename(csv_file))[0].lower()\n        \n        # Open the CSV file for reading\n        with open(csv_file, 'r') as csvfile:\n            # Create a CSV reader object\n            reader = csv.reader(csvfile)\n            \n            # Extract the header row and column names\n            headers = next(reader)\n            columns = ', '.join(headers)\n            \n            # Generate the output SQL file name\n            sql_file = f\"{table_name}.sql\"\n            \n            # Create the results folder if it doesn't exist\n            os.makedirs('results', exist_ok=True)\n            \n            # Update the SQL file path to include the results folder\n            sql_file = os.path.join('results', sql_file)\n            \n            # Create the SQL file for writing\n            with open(sql_file, 'w') as sqlfile:\n                # Initialize a list to store values for each row\n                all_values = []\n                \n                # Iterate over the CSV rows\n                for row in reader:\n                    # Prepare the values for the insert statement, escaping single quotes\n                    values = []\n                    for header, value in zip(headers, row):\n                        if header.lower() == 'id' or header.lower().endswith('_id') or header.lower() == 'quantity' or header.lower() == 'unit_price':\n                            # For 'id' columns and columns ending with '_id', insert the value as is (without quotes):\n                            values.append(value)\n                        elif value == '':\n                            # Insert NULL for blank values\n                            values.append('NULL')\n                        else:\n                            # Escape single quotes for other columns\n                            values.append(f\"'{escape_quotes(value)}'\")\n                    \n                    # Append the values for the current row to the list\n                    all_values.append(f\"({', '.join(values)})\")\n\n                # Generate the final SQL insert statement with multiple value sets\n                sql_insert = f\"INSERT INTO {table_name} ({columns}) VALUES {', '.join(all_values)};\\n\"\n                \n                # Write the SQL insert statement to the SQL file\n                sqlfile.write(sql_insert)\n                \n                print(f\"Conversion completed successfully. SQL file '{sql_file}' generated.\")\n    \n    except FileNotFoundError:\n        print(\"The specified CSV file does not exist.\")\n    \n    except Exception as e:\n        print(f\"An error occurred during the conversion: {str(e)}\")\n\n\nfiles = [\"customer.csv\",\"department.csv\",\"employee.csv\",\"employee_product.csv\",\"orders.csv\",\"product.csv\"]\n\nfor file in files:\n    convert_csv_to_sql(file)\nprint(\"\\nConversion Completed\")\n\nThe specified CSV file does not exist.\nThe specified CSV file does not exist.\nThe specified CSV file does not exist.\nThe specified CSV file does not exist.\nThe specified CSV file does not exist.\nThe specified CSV file does not exist.\n\nConversion Completed\n\n\n\n\nLoading into database 🧺\na MySQL database was created to host the data. The ERD is shown below and using MySQL’s built-in GUI tool, it was converted into a database schema\n\n\n\n\n\nTo lessen the load on the BI tool further down the pipeline, and to make modelling easier, I created 4 views on top of the data (dim_customer, dim_calendar, dim_product and fact_sales)\n\n\nData cleansing 🧹\nIssues - No major cleaning issues; data was loaded directly into the BI tool\n\n\nData Modeling 🏛\nThe following star schema was created for the data model in Power BI\n\n\n\n\n\n\n\nData Visualization 🎨\n\nPower BI magic!✨\nThe following DAX measures were used:\n\nNon-Null Days = \nCALCULATE(\n    COUNTROWS('awdb dim_calendar'),\n    NOT(ISBLANK('awdb dim_calendar'[Day]))\n)\n\nTop 10 Customers by Sales = \nIF(RANKX(ALL('awdb dim_customer'[fullname]), [Total Sales],,DESC)&lt;=10,[Total Sales], BLANK())\n\nTop 10 Products by Sales = \nIF(RANKX(ALL('awdb dim_product'[product_name]), [Total Sales],,DESC)&lt;=10,[Total Sales], BLANK())\n\nTotal Sales = \nSUM('awdb fact_sales'[SalesAmount])"
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/Adw README 3.html#about-the-project",
    "href": "posts/2022_05_28_Adventure-works/Adw README 3.html#about-the-project",
    "title": "Chamber of (data) secrets",
    "section": "About The Project 🍪",
    "text": "About The Project 🍪\n\n\n\nDashboard\n\n\nLive Demo: Here\nTracking Accidents and Casualties across UK Roads in 2021 - 2022  ### Requirement Gathering\nClient wants a dashboard monitoring sales of company’s product\n\na. Primary KPIs\n\nTop ten customers, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\nTop ten products by sales, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\n\n\n\nb. Secondary KPIs\n\nTotal sales by category\nDaily sales trend"
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/Adw README 3.html#identifying-stakeholders",
    "href": "posts/2022_05_28_Adventure-works/Adw README 3.html#identifying-stakeholders",
    "title": "Chamber of (data) secrets",
    "section": "Identifying Stakeholders 🧑🏽‍💼",
    "text": "Identifying Stakeholders 🧑🏽‍💼\n\nSales team\nMarketing Department\n\n\n\nUnderstanding Raw Data 🥩\nThe raw data exists in 9 CSV files representing 1 fact table and 8 dimension tables\n\n\n\n\n\n\n\nTable\ndescription (datatype)\n\n\n\n\nBudget\ndetails budget of each product and product categories per period\n\n\nBudgetPeriod\ndate table for the budget table\n\n\ncustomer\neach row records a customer information\n\n\ndimdates\ndate table for the fact table\n\n\ndimProductCategory\nrecords the various product categories available\n\n\ndimProductSubCategory\nlists the product subcategories available\n\n\nproduct\nlists individual products\n\n\nSales\nThe fact table: records individual sales transaction\n\n\nTerritory\nLists regional locations\n\n\n\n\n\n\nConverting CSV to SQL INSERT statements with Python 🐍\nI wrote a Python  script using the csv and os modules, that takes in a csv file and outputs an SQL file with INSERT statements for each row in the CSV.\n\n\n\nLoading into database 🧺\na MySQL database was created to host the data. The ERD is shown below and using MySQL’s built-in GUI tool, it was converted into a database schema\n\n\n\n\nCreating views 🌁\nTo lessen the load on the BI tool further down the pipeline, and to make modeling easier, I created 4 views on top of the data  (dim_customer, dim_calendar, dim_product and fact_sales)\n\n\n\nData cleansing 🧹\nIssues - No major cleaning issues; data was loaded directly into the BI tool\n ### Data Modeling 🏛\nThe following star schema was created for the data model in Power BI\n\n\n\nSchema\n\n\n\n\n\nData Visualization 🎨\n\nPower BI magic!✨ The report pbix file is available in this repo to explore design decisions\nThe following DAX measures were used:\n\n``` Non-Null Days = CALCULATE( COUNTROWS(‘awdb dim_calendar’), NOT(ISBLANK(‘awdb dim_calendar’[Day])) )\nTop 10 Customers by Sales = IF(RANKX(ALL(‘awdb dim_customer’[fullname]), [Total Sales],,DESC)&lt;=10,[Total Sales], BLANK())\nTop 10 Products by Sales = IF(RANKX(ALL(‘awdb dim_product’[product_name]), [Total Sales],,DESC)&lt;=10,[Total Sales], BLANK())\nTotal Sales = SUM(‘awdb fact_sales’[SalesAmount]) ```\n\n\n\nDeriving Insights\n\nThe primary source of revenue for the company is from the sales of bikes with 96% of sales coming from that category alone\nSales peaked on the 22nd of the month"
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/Adw README 3.html#contact",
    "href": "posts/2022_05_28_Adventure-works/Adw README 3.html#contact",
    "title": "Chamber of (data) secrets",
    "section": "Contact",
    "text": "Contact\nEdun Joshua Olubori - connect on linkedin - joshuaolubori@gmail.com\nLive Demo: Here\n\n(back to top)"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#footnotes",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#footnotes",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKatherine Unger Baillie, Credits, Baillie, K. U., Berger, M. W., Brockmeier, E. K., Writers, Date October 28, Date, 28, O., Subtopics Data Science, Subtopics, Science, D., Schools School of Engineering & Applied Science, Schools, School of Engineering & Applied Science, Share Twitter Facebook, Share, & Twitter Facebook. (1970, May 26). The role of data in a world reshaped by covid-19. Penn Today. Retrieved May 30, 2022, from https://penntoday.upenn.edu/news/role-data-world-reshaped-covid-19↩︎\nNCDC coronavirus COVID-19 microsite. Retrieved May 30, 2022, from https://covid19.ncdc.gov.ng/advisory/↩︎"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#task",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#task",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Task 📌",
    "text": "Task 📌\nTo design a Tableau dashboard to clearly visualize the COVID-19 pandemic in Nigeria since the start of the pandemic in 2020. Key metrics to show include total number of cases, infection rate, fatality rate e.t.c. My audience is the general public and therefore the viz is to be hosted on Tableau Public."
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#prepare",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#prepare",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Prepare 🧹",
    "text": "Prepare 🧹\nWhere was the data located and organized?\nThe data was sourced from the NCDC (Nigerian Centre for Disease Control) and accessed via link on the 15th of May, 2022. The dataset was contained in a CSV file, arranged in 14 columns with the headers written in French. That had to be taken care of in the data cleaning process.\n#Are there issues with bias or credibility in this data? There are concerns that the data from Nigeria may not completely reflective of the true situation on ground due to low testing rates compared to Western countries. #Are there any problems with the data? Problems of note include the columns headers being in French and a few redundant columns."
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#process",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#process",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Process ⚙️",
    "text": "Process ⚙️\nWhat tools are you choosing and why?\nFor data cleaning, exploratory data analysis (EDA) and visualization, I choose Python’s pandas with Jupyter Notebooks. I like pandas versatility with handling data and together with Jupyter Notebook’s flexible running of code in cells, it makes for an efficient workflow. #What steps have you taken to ensure that your data is clean? I opened a folder for the project on my PC and named the file with an appropriate naming convention. Then I imported the dataset using pandas and inspected the first few rows, the last few rows, the columns, the shape and some summary statistics of the data to confirm that it was imported correctly and to have a feel of the data. Using a mapping function, I renamed the column headers appropriately. Where null values where present, I filled in with zero. The Jupyter Notebook file delineates the whole data preparation process\nOn preliminary exploration on Excel, I discovered that date values on some of the tables were inconsistently formatted. Therefore all date fields have to be homogenized going forward\nI did all data cleaning and wrangling with Python’s pandas on Jupyter Notebooks. The codes can be accessed checked here"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#insights",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#insights",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Insights 🕵🏽‍♂️",
    "text": "Insights 🕵🏽‍♂️\n\nThe states of Lagos, Federal Capital Territory (Abuja), and Rivers have the highest confirmed cases most likely due to their being port cities and Lagos’ high population density.\nThere are 246,889 confirmed cases, 223,634 recoveries and 3079 deaths.\nDecember 22nd, 2021 saw the highest number of confirmed cases at 4,006 while August 29th, 2021 was the peak of total deaths at 93.\n\nThe visual can be viewed and interacted with at link"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#contact",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#contact",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Contact",
    "text": "Contact\nEdun Joshua Olubori - connect on linkedin - joshuaolubori@gmail.com\nLive Demo: here"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#about-the-project",
    "href": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#about-the-project",
    "title": "Chamber of (data) secrets",
    "section": "About The Project 🍪",
    "text": "About The Project 🍪\n\n\n\nDashboard\n\n\nLive Demo: here\nTracking Accidents and Casualties across UK Roads in 2021 - 2022  ### Requirement Gathering\nClient wants to create a dashboard on road accidents for the year 2021 and 2022.\n\na. Primary KPIs\n\nTotal Casualties and Total Accident Values:\n\nCurrent Year\nYear-over-Year (YoY) Growth\n\nTotal Casualties by Accident Severity:\n\nCurrent Year\nYear-over-Year (YoY) Growth\n\n\n\n\nb. Secondary KPIs\n\nTotal Casualties with Respect to Vehicle Type:\n\nCurrent Year\n\nMonthly Trend Comparison of Casualties:\n\nCurrent Year vs. Previous Year\n\nCasualties by Road Type:\n\nCurrent Year\n\nCurrent Year Casualties by Area/Location & Day/Night\nTotal Casualties and Total Accidents by Location Total casualties and total accidents by location"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#identifying-stakeholders",
    "href": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#identifying-stakeholders",
    "title": "Chamber of (data) secrets",
    "section": "Identifying Stakeholders 🧑🏽‍💼",
    "text": "Identifying Stakeholders 🧑🏽‍💼\n\nEmergency Services Departments\nRoad Safety Corps\nTraffic Management Agencies\nPolice Force\nGeneral public\n\n\n\nUnderstanding Raw Data 🥩\nData Grain: A single instance of a reported accident event in the UK in 2021 amd 2022\n\n\n\n\n\n\n\nFields\ndescription (datatype)\n\n\n\n\nAccident_Index\nunique row indentifier (string)\n\n\nAccident Date\ndate of accident event (date)\n\n\nDay_of_Week\nday of the week (string)\n\n\nJunction_Control\nmechanism of traffic control at junction (string)\n\n\nJunction_Detail\ntype of junction (string)\n\n\nAccident_Severity\nseverity of accident (string)\n\n\nLatitude\nlatitude (float)\n\n\nLight_Conditions\nlight conditions at accident scene (string)\n\n\nLocal_Authority_(District)\nname of district (string)\n\n\nCarriageway_Hazards\nhazards on the road if any\n\n\nNumber_of_Casualties\nnumber of causaulties (integer)\n\n\nNumber_of_Vehicles\nnumber of vehicles involved in the accident (integer)\n\n\nPolice_Force\nPolice force jurisdiction (string)\n\n\nRoad_Surface_Conditions\ncondition of road surface (string)\n\n\nRoad_Type\nroad type (string)\n\n\nSpeed_limit\nroad speed limit in mph (string)\n\n\nTime\ntime of day accident occurred (time)\n\n\nUrban_or_Rural_Area\nWhether accident occurred in an urban or rural area (string)\n\n\nWeather_Conditions\nweather condition at time of accident (string)\n\n\nVehicle_Type\ntype of vehicle involved in the accident\n\n\n\n\n\n\nData cleansing 🧹\nIssues - Junction_Control column has, among others, two values “Auto traffic sigl” and “Auto traffic signal” of which the former is a misspelling of the latter\n\nInstances where “Fatal” is misspelled as “Fetal” in Accident_Severity column\n“Time” column was represented as a datetime data type in Power BI\n\nFixes - Used the Replace Values feature to correct the misspellings of Issues 1 & 2 - Changed the data type to time accordingly - All fixes were done in Power Query\n ### Data Processing ⚙️\n\nThe data needs a calendar table so as to use Time Intelligence functions further down the line.\nUsing the CALENDAR function, a new table with calculated, dynamic columns of Date, Month and Year was generated.\n ### Data Modeling 🏛\na one-to-many active relationship was established between the calendar table and the data table \n ### Data Visualization 🎨\nPower BI magic!✨ The report pbix file is available in this repo to explore design decisions The following DAX measures were used:\n\nCount of Accidents = DISTINCTCOUNT(Data[Accident_Index])\n\nCurrent Year Accident Count = TOTALYTD(COUNT(Data[Accident_Index]),'Calendar'[Date])\n\nCurrent Year Casualties = TOTALYTD(SUM(Data[Number_of_Casualties]),'Calendar'[Date])\n\nPrevious Year Accident Counts = CALCULATE(COUNT(Data[Accident_Index]), SAMEPERIODLASTYEAR('Calendar'[Date]))\n\nPrevious Year Casualties = CALCULATE(SUM(Data[Number_of_Casualties]),\nSAMEPERIODLASTYEAR('Calendar'[Date]))\n\nYoY Accident Count = DIVIDE([Current Year Accident Count] - [Previous Year Accident Counts], [Previous Year Accident Counts])\n\nYoY Casualties = (DIVIDE([Current Year Casualties] - [Previous Year Casualties],[Previous Year Casualties]))\n\n\n\nDeriving Insights\n\nThe vehicle type most involved in accidents was cars. This can be explained by the fact that most vehicles plying UK roads are cars\nThere is a general decrease in casualty counts in 2022 compared to 2021"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#contact",
    "href": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#contact",
    "title": "Chamber of (data) secrets",
    "section": "Contact",
    "text": "Contact\nEdun Joshua Olubori - connect on linkedin - joshuaolubori@gmail.com\nLive Demo: here\n\n(back to top)"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "",
    "text": "Live Demo: here\n\n\n\n\n\n\n\n   \n\n\n\n\n\nThe year 2020 will be remembered for the global impact of a virus that had infected over 40 million people by late October. While drawing comparisons to the 1918 flu pandemic, one notable difference is the role of technology in providing real-time insights into the spread, mutations, and economic effects of the virus.\nThis enhanced understanding of COVID-19 is made possible by vast amounts of data, including hundreds of genomes and millions of tweets, coupled with advancements in computing power and analytical methods. (Credits Katherine Unger Baillie et al., 2020)1\nThe Nigeria Centre for Disease Control (NCDC) and the National Emergency Operations Centre (EOC) have been at the forefront of Nigeria’s public health response, working in coordination with the Presidential Task Force on COVID-19 (PTF-COVID-19). The NCDC also collaborates closely with all states of the Federation to support their pandemic response efforts. (NCDC coronavirus COVID-19 advisory, 2022)2\nGiven these circumstances, data collection and analysis are vital for informing appropriate responses, planning, and resource allocation.\nThis interactive dashboard provides information on the COVID-19 pandemic in Nigeria, offering insights into the disease and its impact on the Nigerian population.This is an interactive dashboard on the COVID-19 pandemic in Nigeria, reflecting information emerging on the disease and its impact on the Nigerian population."
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#about-the-project",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#about-the-project",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "",
    "text": "Live Demo: here\n\n\n\n\n\n\n\n   \n\n\n\n\n\nThe year 2020 will be remembered for the global impact of a virus that had infected over 40 million people by late October. While drawing comparisons to the 1918 flu pandemic, one notable difference is the role of technology in providing real-time insights into the spread, mutations, and economic effects of the virus.\nThis enhanced understanding of COVID-19 is made possible by vast amounts of data, including hundreds of genomes and millions of tweets, coupled with advancements in computing power and analytical methods. (Credits Katherine Unger Baillie et al., 2020)1\nThe Nigeria Centre for Disease Control (NCDC) and the National Emergency Operations Centre (EOC) have been at the forefront of Nigeria’s public health response, working in coordination with the Presidential Task Force on COVID-19 (PTF-COVID-19). The NCDC also collaborates closely with all states of the Federation to support their pandemic response efforts. (NCDC coronavirus COVID-19 advisory, 2022)2\nGiven these circumstances, data collection and analysis are vital for informing appropriate responses, planning, and resource allocation.\nThis interactive dashboard provides information on the COVID-19 pandemic in Nigeria, offering insights into the disease and its impact on the Nigerian population.This is an interactive dashboard on the COVID-19 pandemic in Nigeria, reflecting information emerging on the disease and its impact on the Nigerian population."
  }
]