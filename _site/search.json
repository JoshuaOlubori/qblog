[
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDeveloping a Modern Data Pipeline — Part 1\n\n\nA 3-part series featuring web-scraping, data pipelines, orchestration and containerisation\n\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a Modern Data Pipeline — Part 2\n\n\nA 3-part series featuring web-scraping, data pipelines, orchestration and containerisation\n\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a Modern Data Pipeline — Part 3\n\n\nA 3-part series featuring web-scraping, data pipelines, orchestration and containerisation\n\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPushing the Boundaries of Excel\n\n\nA Stunning Dashboard with Pivot Tables and Charts\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAqua Vista College Database\n\n\nA comprehensive case study in database design\n\n\n\nJan 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Causal-Behavioral Framework for Data Analysis\n\n\nIntroductory part to a series on Florent Buisson’s book, Behavioral Data Analysis with R & Python\n\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPizza Runner\n\n\nSecond part of a series of SQL case studies where I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoodie Fi\n\n\nThird part of a series of SQL case studies\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Bank\n\n\nFourth part of a series of SQL case studies… more CTEs!\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDanny’s Diner\n\n\nIn this first part of a series of SQL case studies, I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships of Variables in Sleep Data\n\n\nIn this post, I use regression, with an emphasis on understanding the casual relationships between variables\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeads Acquisition Analysis\n\n\nAn assessment for a Report Developer role at Toptal\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising call centre data trends\n\n\n… as part of PwC’s Virtual Internship program\n\n\n\nMar 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject: Analyzing Trends in Gun Sales Data across the U.S.\n\n\nAn project for Udacity’s Nanodegree\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving insights from bike traffic data\n\n\nSecond case study as part of my Udacity Nanodegree\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTweet Data Analysis\n\n\nHardcore python wrangling of Twitter data\n\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSales Monitoring Dashboard\n\n\nMy take on the AdventureWorks dataset; featuring database creation, data modeling in Power BI and python scripting\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising COVID-19’s Impact in Nigeria\n\n\nTracking a pandemic with data\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)\n\n\nFeaturing data modeling in Power BI, and Dax\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#introduction",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#introduction",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "title": "Foodie Fi",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#database-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#database-connection",
    "title": "Foodie Fi",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"foodie_fi\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#queries",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#queries",
    "title": "Foodie Fi",
    "section": "Queries",
    "text": "Queries\n\n1. How many customers has Foodie-Fi ever had?\n\n\nSELECT count(DISTINCT customer_id)\nFROM subscriptions;\n\n\n1 records\n\n\ncount\n\n\n\n\n1000\n\n\n\n\n\n\n\n3. What is the monthly distribution of trial plan start_date values for our dataset use the start of the month as the group by value?\n\nSELECT upper(to_char(start_date, 'month')) as start_month,\n    count(*) frequency\nfrom subscriptions\nwhere plan_id = 0\ngroup by 1\nORDER BY 2 desc;\n\n\nDisplaying records 1 - 10\n\n\nstart_month\nfrequency\n\n\n\n\nMARCH\n94\n\n\nJULY\n89\n\n\nAUGUST\n88\n\n\nMAY\n88\n\n\nJANUARY\n88\n\n\nSEPTEMBER\n87\n\n\nDECEMBER\n84\n\n\nAPRIL\n81\n\n\nJUNE\n79\n\n\nOCTOBER\n79\n\n\n\n\n\n\n\n3. What plan start_date values occur after the year 2020 for our dataset? Show the breakdown by count of events for each plan_name ?\n\n\nSELECT p.plan_name,\n    count(*) as count_of_events_after_2020\nfrom subscriptions s\n    natural join plans p\nwhere EXTRACT(\n        year\n        from start_date\n    ) &gt; 2020\ngroup by 1;\n\n\n4 records\n\n\nplan_name\ncount_of_events_after_2020\n\n\n\n\npro annual\n63\n\n\nchurn\n71\n\n\npro monthly\n60\n\n\nbasic monthly\n8\n\n\n\n\n\n\n\n4. What is the customer count and percentage of customers who have churned rounded to 1 decimal place?\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id = 4\n)\nSELECT cte1.whole as total_customers,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_churned\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ntotal_customers\npct_churned\n\n\n\n\n2650\n12\n\n\n\n\n\n\n\n5. How many customers have churned straight after their initial free trial? what percentage is this rounded to the nearest whole number?\n\nwith cte as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\ncte2 as (\n    -- getting rows whose values satisfy the condition in the question\n    select *\n    from cte\n    where plan_id = 0\n        and lead_plan_id = 4\n) -- solution\nselect count(*) as count_of_customers_who_churned_after_free_trial\nfrom cte2 \n\n\n1 records\n\n\ncount_of_customers_who_churned_after_free_trial\n\n\n\n\n92\n\n\n\n\n\n\n\n6. What is the number and percentage of customer plans after their initial free trial?\n\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id &lt;&gt; 0\n)\nSELECT cte2.part as customer_count_after_trial_plan,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_ccatp\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ncustomer_count_after_trial_plan\npct_ccatp\n\n\n\n\n1650\n62\n\n\n\n\n\n\n\n7. How many customers have upgraded to an annual plan in 2020?\n\n\nwith cte1 as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\n-- filtering to only annual plans\ncte2 as (\n    select *,\n        lead_plan_id - plan_id as diff\n    from cte1\n    where lead_plan_id = 3\n) -- excluding churned customers and unupgraded plans\nselect count(DISTINCT customer_id) as upgraded_customers_2020_count\nfrom cte2\nwhere (diff &gt; 0)\n    and (lead_plan_id &lt;&gt; 4)\n    and EXTRACT(\n        year\n        from start_date\n    ) = 2020;\n\n\n1 records\n\n\nupgraded_customers_2020_count\n\n\n\n\n253\n\n\n\n\n\n\n\n8. How many days on average does it take for a customer to upgrade to an annual plan from the – day they join Foodie-Fi?\n\nwith cte1 as (\n    select *,\n        max(plan_id) over (partition by customer_id) as highest_plan_suscribed,\n        max(start_date) over (partition by customer_id) as date_of_hps,\n        min(start_date) over (partition by customer_id) as date_of_lps,\n        row_number() over (partition by customer_id) as sn\n    from subscriptions\n    order by customer_id,\n        start_date,\n        plan_id\n),\ncte2 as(\n    select *,\n        date_of_hps - date_of_lps as diff_in_days\n    from cte1\n    where highest_plan_suscribed = 3\n        and sn = 1\n)\nselect round(avg(diff_in_days)::numeric, 2) as avg_days_to_upgrade_to_annual\nfrom cte2\n\n\n1 records\n\n\navg_days_to_upgrade_to_annual\n\n\n\n\n105.95\n\n\n\n\n\n\n\n9. How many customers downgraded from a pro monthly to a basic monthly plan in 2020?\n\nwith cte1 as(\n        select *,\n            lead(plan_id) over(partition by customer_id) as lead_plan_id\n        from subscriptions\n        order by customer_id,\n            start_date,\n            plan_id\n    ),\n    cte2 as (\n        select customer_id,\n            plan_id,\n            lead_plan_id,\n            start_date\n            from cte1\n        where plan_id = 2\n            and lead_plan_id = 1\n            and EXTRACT(\n                year\n                from start_date\n            ) = 2020\n    )\nselect count(*) as number_of_customers_downgrade_from_prom_basm\nfrom cte2\n\n\n1 records\n\n\nnumber_of_customers_downgrade_from_prom_basm\n\n\n\n\n0"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#closing-the-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#closing-the-connection",
    "title": "Foodie Fi",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html",
    "href": "posts/2023-11-18-sleep-disorders/index.html",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "",
    "text": "The goal is to build a casual model using linear regression that explains quality of sleep, and logistic regression to explain sleep disorders\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n# descriptives\nlibrary(datawizard)\n\nWarning: package 'datawizard' was built under R version 4.3.2\n\nlibrary(kableExtra)\nlibrary(skimr)\nlibrary(qqplotr)\n\nWarning: package 'qqplotr' was built under R version 4.3.2\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.2\n\n# os\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.3.2"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#data-cleaning",
    "href": "posts/2023-11-18-sleep-disorders/index.html#data-cleaning",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Data cleaning",
    "text": "Data cleaning\nI observed the following issues in the data\n\nOccupation variable having levels with very few counts. Those with few counts can be consolidated with similar categories.\nThe BMI category variable has both “Normal” and “Normal Weight” as levels. This is most likely as data entry error.\nThe heart rate variable is not in a suitable format to work with. I’ll extract the systolic blood pressure from the variable as an integer. I would not need the diastolic blood pressure as I observed strong multicollinearity between both diastolic and systolic blood pressure.\n\n\n# observe occupations with low frequency counts\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# observe data entry error in bmi_category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# regrouping occupation variable so that occupations with lower counts are consolidated with other similar occupations. Simultaneously correcting data entry error in bmi_category variable\nsleep &lt;- sleep %&gt;% mutate( \n  occupation=ifelse(sleep$occupation %in% c(\"Software Engineer\", \"Scientist\", \"Engineer\"),\"Technical\", \nifelse(sleep$occupation %in% c(\"Salesperson\", \"Sales Representative\", \"Manager\"), \"Sales\", ifelse(sleep$occupation %in% c(\"Doctor\", \"Nurse\"), \"Medical\",\n                                                          as.character(occupation)))),\nbmi_category = ifelse(sleep$bmi_category %in% c(\"Normal\", \"Normal Weight\"), \"Normal\", as.character(sleep$bmi_category)))\n\n\n# extracting systolic blood pressure from the blood_pressure variable as it such a numeric variable is easier to work with. Subsequently dropping hear_rate variable\n\nsleep &lt;- sleep %&gt;% mutate(\nbp_sys= parse_number(str_sub(sleep$blood_pressure, 1, 3)),\nperson_id = as.character(person_id)\n) %&gt;% select(!heart_rate)\n\n\n# confirming changes\nhead(sleep)  %&gt;% gt() %&gt;% opt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      person_id\n      gender\n      age\n      occupation\n      sleep_duration\n      quality_of_sleep\n      physical_activity_level\n      stress_level\n      bmi_category\n      blood_pressure\n      daily_steps\n      sleep_disorder\n      bp_sys\n    \n  \n  \n    1\nMale\n27\nTechnical\n6.1\n6\n42\n6\nOverweight\n126/83\n4200\nNone\n126\n    2\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    3\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    4\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    5\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    6\nMale\n28\nTechnical\n5.9\n4\n30\n8\nObese\n140/90\n3000\nInsomnia\n140\n  \n  \n  \n\n\n\n\n\n# confirming changes to BMI category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# confirming changes to occupation variable\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#associations",
    "href": "posts/2023-11-18-sleep-disorders/index.html#associations",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Associations",
    "text": "Associations\n\nResearch Question: “What strong associations exists between quality_of_sleep and other variables in the dataset?\n\n# | warning: false\n# | message: false\n# packages required for associations\nlibrary(correlation)\nlibrary(GGally)\n\n\n# plotting the grid plot of scatterplots of numeric variables\nggscatmat(sleep %&gt;% select_if(is.numeric)) + theme_light()\n\n\n\n\n\n# showing relationships with strong correlations (greater than |+-0.4|)\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation() %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs)) %&gt;% filter(abs(r) &gt; 0.4) %&gt;% arrange(r) \ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.899\n0.000\n    sleep_duration\nstress_level\n−0.811\n0.000\n    age\nstress_level\n−0.422\n0.000\n    age\nquality_of_sleep\n0.474\n0.000\n    age\nbp_sys\n0.606\n0.000\n    physical_activity_level\ndaily_steps\n0.773\n0.000\n    sleep_duration\nquality_of_sleep\n0.883\n0.000\n  \n  \n  \n\n\n\n\n\n# robust correlation analysis since the data are skewed. Results is filtered to include only relationships involving quality_of_sleep. Sorted ascendingly by p-value\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation(method=\"percentage\") %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs))  %&gt;% arrange(p)  %&gt;% filter( Parameter1 == \"quality_of_sleep\" | Parameter2 == \"quality_of_sleep\") %&gt;% arrange(p) %&gt;% filter(p &lt; 0.05)\ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.910\n0.000\n    sleep_duration\nquality_of_sleep\n0.892\n0.000\n    age\nquality_of_sleep\n0.432\n0.000\n    quality_of_sleep\nphysical_activity_level\n0.178\n0.005\n  \n  \n  \n\n\n\n\n\nResults\nThe most significant relationships (at alpha = .05) are the relationships between:\n\nquality of sleep and stress level, r(372) =-0.91, p &lt; 0.0001\nquality of sleep and sleep duration, r(372) = 0.89, p &lt; 0.0001\nquality of sleep and age, r(372) =0.43, p &lt; 0.0001\nquality of sleep and physical activity level, r(372) =0.18, p = 0.0054\n\nThese are the variables we would fit to our linear regression model."
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#research-questions",
    "href": "posts/2023-11-18-sleep-disorders/index.html#research-questions",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research questions:",
    "text": "Research questions:\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\nIs the relationship between stress level and quality of sleep moderated by occupation?\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\n\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\n\n# Visualizing the relationship of the three variables\nggplot(sleep, aes(x = sleep_duration, y = quality_of_sleep, color = sleep_disorder)) +\ngeom_jitter(size=1, alpha=0.8) +\nlabs(x = \"Sleep duration\", y = \"Quality of sleep\", color = \"Sleep disorder\") +\ngeom_smooth(method = \"lm\", se = FALSE, linewidth=0.9) + theme_bw()\n\n\n\n\nThe plot shows that there might be some moderation effects.\n\n# making sleep_disorder a factor variable and setting the base level to \"None\"\nsleep &lt;- sleep %&gt;% mutate(\n  sleep_disorder = as_factor(sleep_disorder) %&gt;% fct_relevel(\"None\")\n)\n\n\nqs_lm_03 &lt;- lm(quality_of_sleep ~ sleep_duration * sleep_disorder, data=sleep)\nqs_lm_03 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-0.5750673\n0.36703899\n-1.566774\n1.180269e-01\n    sleep_duration\n1.1144522\n0.04963586\n22.452560\n6.124844e-71\n    sleep_disorderSleep Apnea\n-3.4341228\n0.57712441\n-5.950403\n6.233752e-09\n    sleep_disorderInsomnia\n-3.2040380\n1.11193745\n-2.881491\n4.190005e-03\n    sleep_duration:sleep_disorderSleep Apnea\n0.4802913\n0.08000178\n6.003508\n4.632039e-09\n    sleep_duration:sleep_disorderInsomnia\n0.4503706\n0.16657925\n2.703642\n7.176641e-03\n  \n  \n  \n\n\n\n\n\nInterpretation\n\n\n\n\n\n\nTip\n\n\n\nThe effect of increasing sleep duration in someone with no sleep disorder on sleep quality is 1.11\nThe effect of sleep apnea on sleep quality compared to someone with no disorders is -3.43\nThe effect of Insomnia on sleep quality compared to someone with no disorders is -3.20\nThe effect of increasing sleep duration in someone with sleep anea on sleep quality compared to someone with no disorders is 0.48\nThe effect of increasing sleep duration in someone with insomnia on sleep quality compared to someone with no disorders is 0.45\n\n\n\ninteractions::sim_slopes(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of sleep_duration when sleep_disorder = Insomnia: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.56   0.15   1.26    1.86    10.26   0.00\n\nSlope of sleep_duration when sleep_disorder = Sleep Apnea: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.59   0.06   1.48    1.71    27.35   0.00\n\nSlope of sleep_duration when sleep_disorder = None: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.11   0.03   1.05    1.17    36.50   0.00\n\n\nThe slopes for the three regressions are not significantly different\n\ninteractions::interact_plot(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Sleep disorder\"\n  )\n\n\n\n\nLooking at the slope, we can that the moderation effect is not significant.\n\n\nConclusion\nSleep disorder is not a moderator of the relationship between sleep duration and quality of sleep\n\n\n\nIs the relationship between stress level and quality of sleep moderated by occupation?\n\nqs_lm_04 &lt;- lm(quality_of_sleep ~ stress_level * occupation, data=sleep)\nqs_lm_04 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n9.3661417\n0.22464236\n41.6935685\n2.847324e-140\n    stress_level\n-0.3208661\n0.04725953\n-6.7894487\n4.624546e-11\n    occupationLawyer\n3.3611310\n1.08304354\n3.1034126\n2.063488e-03\n    occupationMedical\n1.3637168\n0.24305144\n5.6108156\n4.008125e-08\n    occupationSales\n3.5481440\n1.02746628\n3.4532948\n6.193572e-04\n    occupationTeacher\n-0.2842770\n0.33263086\n-0.8546322\n3.933200e-01\n    occupationTechnical\n2.0623438\n0.25972133\n7.9406026\n2.559029e-14\n    stress_level:occupationLawyer\n-0.6336793\n0.21426067\n-2.9575158\n3.304916e-03\n    stress_level:occupationMedical\n-0.2851357\n0.04939422\n-5.7726532\n1.679151e-08\n    stress_level:occupationSales\n-0.6791339\n0.15058782\n-4.5098857\n8.775327e-06\n    stress_level:occupationTeacher\n-0.1447393\n0.07087144\n-2.0422799\n4.184795e-02\n    stress_level:occupationTechnical\n-0.4718021\n0.05572917\n-8.4659819\n6.415721e-16\n  \n  \n  \n\n\n\n\n\ninteractions::sim_slopes(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of stress_level when occupation = Lawyer: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.95   0.03   -1.02   -0.89   -29.71   0.00\n\nSlope of stress_level when occupation = Accountant: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.32   0.04   -0.40   -0.24    -7.92   0.00\n\nSlope of stress_level when occupation = Teacher: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.47   0.09   -0.65   -0.28    -5.01   0.00\n\nSlope of stress_level when occupation = Sales: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -1.00   1.04   -3.04    1.04    -0.97   0.33\n\nSlope of stress_level when occupation = Medical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.61   0.01   -0.62   -0.59   -81.54   0.00\n\nSlope of stress_level when occupation = Technical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.79   0.07   -0.93   -0.65   -10.97   0.00\n\n\n\ninteractions::interact_plot(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Occupation\"\n  )\n\n\n\n\nAgain, we do not observe a significant moderation effect of occupation on the relationship between stress level and quality of sleep\n\n\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\nTo test this, we are going to run 3 regressions:\n\nThe total effect of physical_activity_level on quality_of_sleep.\nThe effect of physical_activity_level on quality_of_sleep that is mediated by daily_steps, a.k.a. the indirect effect\nThe effect of physical_activity_level on quality_of_sleep that is not mediated by daily_steps, a.k.a. the direct effect\n\n\nTotal effect\nWe first determine the total effect by running a regression of physical_activity_level on quality_of_sleep (without including daily_steps):\n\nlm(quality_of_sleep ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n6.657\n0.183\n36.307\n0.000\n    physical_activity_level\n0.011\n0.003\n3.792\n0.000\n  \n  \n  \n\n\n\n\nThe total effect is approximately 0.011, meaning that an increase of one unit in physical activity increases sleep quality by 0.001 units on average, not holding daily steps constant.\n\n\nMediated/Indirect Effect\nThe effect of physical_activity_level on quality_of_sleep mediated by daily_steps can be obtained by multiplying together the effect of physical_activity_level on daily_steps and the effect of daily_steps on quality_of_sleep.\n\n1. physical_activity_level on daily_steps (a)\n\nlm(daily_steps ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n3265.57619\n160.321889\n20.36887\n1.718735e-62\n    physical_activity_level\n60.01692\n2.556092\n23.47995\n1.968256e-75\n  \n  \n  \n\n\n\n\nOne unit increase in physical_activity_level increases daily_steps by an average of 60 steps\n\n\n2. daily_steps on quality_of_sleep\n\nlm(quality_of_sleep ~ daily_steps + physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n7.4500815511\n2.609352e-01\n28.551459\n1.151887e-95\n    daily_steps\n-0.0002428667\n5.802065e-05\n-4.185867\n3.551189e-05\n    physical_activity_level\n0.0256601109\n4.506428e-03\n5.694113\n2.524969e-08\n  \n  \n  \n\n\n\n\n(a * b) = 60.07 * -0.00024 = -0.0144\n\n\n\nDirect Effect\nThis is simply the coefficient of physical_activity_level in the above regression = 0.0257\nAccording to Zhao et al., what we have here isCompetitive Mediation (Regularly Partial Mediation)\nIn the competitive partial mediation hypothesis, it is assumed that the intermediate variable (daily_steps) could sometimes increase and at times decrease the relationship between the independent and dependent variables. i.e an “inconsistent” model.\n\n\n\nDecision tree for determining mediation"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "href": "posts/2023-11-18-sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research question: are significant differences in the mean sleep quality of both male and female participants",
    "text": "Research question: are significant differences in the mean sleep quality of both male and female participants\n\n# summary statistics of quality_of_sleep by gender\n\nby(cbind(data=sleep$quality_of_sleep), sleep$gender, psych::describe)\n\nINDICES: Female\n     vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 185 7.66 1.28      8    7.76 1.48   4   9     5 -0.49    -0.83 0.09\n------------------------------------------------------------ \nINDICES: Male\n     vars   n mean sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 189 6.97  1      7    7.01 1.48   4   9     5 -0.35    -0.59 0.07\n\n\nThe mean of female participants is one point higher than males. But is this difference significant?\n\n# plotting a violin-errorbar plot to visualise the relationship\nggplot(sleep, aes(gender,quality_of_sleep))+\ngeom_violin() +\nstat_summary(fun.data=\"mean_cl_normal\") + \ntheme_minimal()\n\n\n\n\nSince the errorbars do not overlap, we can be fairly confident that the difference is significant. Lets confirm this hunch with a Welsh’s t-test.\n\n t.test(quality_of_sleep ~ gender,\n                    data = sleep,\n                    paired = FALSE,\n                    var.equal = FALSE,\n                    conf.level = 0.95,\n                    na.action = na.exclude)\n\n\n    Welch Two Sample t-test\n\ndata:  quality_of_sleep by gender\nt = 5.8593, df = 347.96, p-value = 1.078e-08\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n 0.4627786 0.9304432\nsample estimates:\nmean in group Female   mean in group Male \n            7.664865             6.968254 \n\n\nBecause the p-value = 1.078e-08 is less than our alpha of .05, we can conclude that the difference between both genders is significant. Also, since the confidence interval expressing the true difference in means does not cross zero, we can be confident that there exists a true difference, with a chance of us being wrong 5% of the time.\n\neffectsize::cohens_d(quality_of_sleep ~ gender, data = sleep) %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Cohens_d\n      CI\n      CI_low\n      CI_high\n    \n  \n  \n    0.608\n0.950\n0.400\n0.815\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOn average, female participants have better sleep quality (M = 7.66, SE = 0.09), than those not given a cloak (M = 6.97, SE = 0.07). Sleep quality is significantly different for both genders: the mean difference, M = 0.69, 95% CI [0.46, 0.93], was significantly different from 0, t(347.96) = 5.85, p = 01.078e-08. This effect was quite large, d= 0.61[0.4, 0.82]"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#conclusion-1",
    "href": "posts/2023-11-18-sleep-disorders/index.html#conclusion-1",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Conclusion",
    "text": "Conclusion\nI created this post as a way of solidifying my understanding of these concepts. I find that documenting the material helps it to stick iwth me better. I hope that you find this helpful. My deepest gratitude goes to Andy Field for his exceptional book on statistical analysis with R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chamber of (data) secrets",
    "section": "",
    "text": "Developing a Modern Data Pipeline — Part 1\n\n\n\ndata engineering\n\n\nweb-scraping\n\n\n\n\n\n\n\nEdun Joshua\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a Modern Data Pipeline — Part 2\n\n\n\ndata engineering\n\n\napi\n\n\n\n\n\n\n\nEdun Joshua\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a Modern Data Pipeline — Part 3\n\n\n\ndata engineering\n\n\napi\n\n\n\n\n\n\n\nEdun Joshua\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPushing the Boundaries of Excel\n\n\n\nExcel\n\n\ndata visualisation\n\n\n\n\n\n\n\nEdun Joshua\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAqua Vista College Database\n\n\n\nSQL\n\n\ndata engineering\n\n\ndatabase design and management\n\n\n\n\n\n\n\nEdun Joshua\n\n\nJan 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Causal-Behavioral Framework for Data Analysis\n\n\n\ncausal regression modelling\n\n\nstatistics\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPizza Runner\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoodie Fi\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Bank\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDanny’s Diner\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships of Variables in Sleep Data\n\n\n\nstats\n\n\ncausal regression modelling\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeads Acquisition Analysis\n\n\n\nTableau\n\n\ndata visualisation\n\n\n\n\n\n\n\nEdun Joshua\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising call centre data trends\n\n\n\nTableau\n\n\ndata visualisation\n\n\n\n\n\n\n\nEdun Joshua\n\n\nMar 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject: Analyzing Trends in Gun Sales Data across the U.S.\n\n\n\npython\n\n\ndata wrangling\n\n\n\n\n\n\n\nEdun Joshua\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving insights from bike traffic data\n\n\n\npython\n\n\ndata visualisation\n\n\ndata wrangling\n\n\n\n\n\n\n\nEdun Joshua\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTweet Data Analysis\n\n\n\npython\n\n\ndata visualisation\n\n\ndata wrangling\n\n\n\n\n\n\n\nEdun Joshua\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSales Monitoring Dashboard\n\n\n\npython\n\n\ndata visualisation\n\n\ndata engineering\n\n\n\n\n\n\n\nEdun Joshua\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising COVID-19’s Impact in Nigeria\n\n\n\npython\n\n\ndata visualisation\n\n\n\n\n\n\n\nEdun Joshua\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)\n\n\n\npower bi\n\n\ndata visualisation\n\n\n\n\n\n\n\nEdun Joshua\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Musings of a data mage",
    "section": "",
    "text": "See all"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Edun Joshua is a data enthusiast and an environmental engineering student. Data science is a reprieve from the existential crises that haunt him."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nFederal University of Agriculture, Abeokuta | Alabata, Ogun State\nB.Eng. in Environmental Engineering | March 2017 - March 2023"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#introduction",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#introduction",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "title": "Danny’s Diner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram\n\n\nSales\nThe sales table captures all customer_id level purchases with an corresponding order_date and product_id information for when and what menu items were ordered.\n\n\nMenu\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\n\n\nMembers\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\nkey concepts: CTEs, window functions"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#database-connection",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#database-connection",
    "title": "Danny’s Diner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"danny_diners\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#queries",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#queries",
    "title": "Danny’s Diner",
    "section": "Queries",
    "text": "Queries\nNow let’s convert business questions into SQL queries!\n\n1. What is the total amount each customer spent at the restaurant?\n\n\nselect s.customer_id as customer,\n   sum(m.price) as total_amount\nfrom sales s\n   inner join menu m on s.product_id = m.product_id\ngroup by customer;\n\n\n3 records\n\n\ncustomer\ntotal_amount\n\n\n\n\nB\n74\n\n\nC\n36\n\n\nA\n76\n\n\n\n\n\n\n\n2. How many days has each customer visited the restaurant?\n\n\nselect customer_id,\n   count(\n      distinct extract(\n         day\n         from order_date\n      )\n   ) as no_of_days_visited\nfrom sales\ngroup by customer_id\norder by customer_id;\n\n\n3 records\n\n\ncustomer_id\nno_of_days_visited\n\n\n\n\nA\n4\n\n\nB\n5\n\n\nC\n2\n\n\n\n\n\nThis SQL query calculates the number of days each customer has visited a restaurant by grouping the sales transactions by customer ID, extracting the day from the order date, counting the distinct days, and ordering the results by customer ID.\n\n\n3. What was the first item from the menu purchased by each customer?\n\n\nwith cte as (\n   select customer_id,\n      order_date,\n      row_number() over (\n         partition by customer_id\n         order by order_date\n      ) as order_rank,\n      product_id\n   from sales\n)\nselect c.customer_id,\n   c.order_date,\n   c.order_rank,\n   m.product_name\nfrom cte c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\norder_date\norder_rank\nproduct_name\n\n\n\n\nA\n2021-01-01\n1\nsushi\n\n\nB\n2021-01-01\n1\ncurry\n\n\nC\n2021-01-01\n1\nramen\n\n\n\n\n\nWithin the CTE, the row_number() function is employed to assign a ranking to each order for each customer based on the order_date. The ranking starts from 1, indicating the first order for each customer. The main query then selects the customer ID, order date, order rank, and product name from the CTE, joining the menu table to retrieve the product name corresponding to the product ID. Finally, it filters the results to include only records with an order rank of 1, effectively selecting the first item purchased for each customer.\n\n\n4. What is the most purchased item on the menu and how many times was it purchased by all customers?\n\n\nwith cte as(\n   select product_id,\n      count(product_id)\n   from sales\n   group by product_id\n   order by count(product_id) desc\n   limit 1\n) -- Most purchased item on the menu is the product with the id 3 which is ramen, according to cte\nselect customer_id,\n   count(product_id) as count_of_most_purchased_product\nfrom sales\nwhere product_id in (\n      select product_id\n      from cte\n   )\ngroup by customer_id;\n\n\n3 records\n\n\ncustomer_id\ncount_of_most_purchased_product\n\n\n\n\nA\n3\n\n\nB\n2\n\n\nC\n3\n\n\n\n\n\nThe query utilizes a common table expression (CTE) named cte to generate a temporary result set. Within the CTE, the count() function is employed to count the number of times each product ID appears in the sales table. The results are then sorted in descending order based on the product count. The limit 1 clause restricts the output to the top row, effectively identifying the product ID with the highest count.\nThe main query then selects the customer ID and the count of the most purchased product for each customer. It filters the sales table to include only records where the product_id matches the one identified in the CTE, effectively focusing on the most purchased item. Finally, it groups the results by customer_id to determine how many times each customer purchased the most popular item.\n\n\n5. Which item was the most popular for each customer?\n\n\nwith cte_1 as (\n   select customer_id,\n      product_id,\n      count(product_id) as count_of_item\n   from sales\n   group by customer_id,\n      product_id\n),\ncte_2 as (\n   select *,\n      row_number() over (\n         partition by customer_id\n         order by count_of_item desc\n      ) as order_rank\n   from cte_1\n)\nselect c.customer_id,\n   m.product_name,\n   c.count_of_item\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\nproduct_name\ncount_of_item\n\n\n\n\nC\nramen\n3\n\n\nB\nramen\n2\n\n\nA\nramen\n3\n\n\n\n\n\nThis query aims to identify the most popular item for each customer. The query utilizes two common table expressions to process the data and generate the desired output. The first CTE, named cte_1, calculates the count of each product purchased by each customer. It groups the rows in the sales table by customer_id and product_id, and then counts the occurrences of each product ID for each customer. This step determines the frequency of each product purchase for each customer.\nThe second CTE, named cte_2, assigns a ranking to each product for each customer based on the purchase frequency calculated in cte_1. It uses the row_number() function and partitions the data by customer_id, sorting within each partition by the count_of_item in descending order. This step effectively identifies the product with the highest purchase frequency (i.e., the most popular item) for each customer.\nThe main query then selects the customer ID, product name, and purchase count for each customer’s most popular item. It joins the menu table to obtain the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the most popular item for each customer is selected.\n\n\n6. Which item was purchased first by the customer after they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date\n      ) as order_rank\n   from cte_1\n   where order_date &gt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nramen\n2021-01-10\n1\n\n\nB\nsushi\n2021-01-11\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date, ensuring a chronological order of transactions. The second CTE, named cte_2, focuses on purchases made after each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is later than the join_date (membership start date).\nAdditionally, it assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date. This identifies the first purchase (order_rank = 1) made by each customer after becoming a member. The main query then selects the customer ID, product name, order date, and order rank for each customer’s first purchase after becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the first purchase is selected.\n\n\n7. Which item was purchased just before the customer became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      join_date,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date desc\n      ) as order_rank\n   from cte_1\n   where order_date &lt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nsushi\n2021-01-01\n1\n\n\nB\nsushi\n2021-01-04\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date in ascending order, ensuring a chronological sequence of transactions.\nThe second CTE, cte_2, focuses on purchases made before each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date).\nIt assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date in descending order. This identifies the last purchase (order_rank = 1) made by each customer before becoming a member.\nThe main query then selects the customer ID, product name, order date, and order rank for each customer’s last purchase before becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the last purchase is selected.\n\n\n8. What is the total items and amount spent for each member before they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n      natural join menu\n   order by order_date\n)\nselect customer_id,\n   count(distinct product_id) as count_of_products,\n   sum(price) as total_amount_spent\nfrom cte_1\nwhere order_date &lt; join_date\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ncount_of_products\ntotal_amount_spent\n\n\n\n\nA\n2\n25\n\n\nB\n2\n40\n\n\n\n\n\nThe query utilizes a CTE named cte_1 to prepare the data and simplifies the aggregation in the main query.\nThe main query then aggregates the data for each customer based on their membership status. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date). For each customer, it counts the distinct product_id values to determine the total number of unique items purchased and calculates the sum of price values to determine the total amount spent. The results are grouped by customer_id to provide individual summaries for each member.\n\n\n9. If each $1 spent equates to 10 points and sushi has a 2x points multiplier - how many points would each customer have?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points\n   from members\n      natural join sales\n      natural join menu\n)\nselect customer_id,\n   sum(points) as total_points\nfrom cte\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n860\n\n\nB\n940\n\n\n\n\n\nThe CTE combines the members, sales, and menu tables, providing a comprehensive view of customer memberships, their purchases, and the corresponding product names.\nThe main query then summarizes the points earned for each customer. It groups the data from the CTE by customer_id and calculates the sum of points values for each group, effectively determining the total points earned by each customer.\n\n\n10. In the first week after a customer joins the program (including their join date) they earn 2x points on all items, – not just sushi - how many points do customer A and B have at the end of January?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points,\n      case\n         when order_date - join_date &lt;= 7 then 2\n         else 1\n      end multiplier\n   from members\n      natural join sales\n      natural join menu\n),\ncte_2 as (\n   select *,\n      points * multiplier as total_points\n   from cte\n)\nselect customer_id,\n   sum(total_points) as total_points\nfrom cte_2\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n1720\n\n\nB\n1760\n\n\n\n\n\nThe first CTE, named cte, joins the members, sales, and menu tables, and calculates the points earned for each purchase using a conditional CASE expression, similar to the previous query.\nAdditionally, it assigns a multiplier to each purchase based on whether it falls within the first week after the customer’s join date. For purchases within the first week, the multiplier is 2 (double points); for purchases outside the first week, the multiplier is 1 (standard points).\nThe second CTE, named cte_2, simplifies the calculation by multiplying the points and multiplier columns for each purchase, effectively determining the total points earned per transaction. The main query then summarizes the points earned for each customer, including the double points accrued during the first week. It groups the data from cte_2 by customer_id and calculates the sum of total_points values for each group, providing the total points earned by customer A and customer B at the end of January\n\n\n11. Recreate the following table output using the available data:\n\n\nselect s.customer_id,\n    s.order_date,\n    men.product_name,\n    men.price,\n    CASE\n        WHEN s.order_date &gt;= m.join_date THEN 'Y'\n        ELSE 'N'\n    END\nfrom sales s\n    LEFT JOIN menu men ON s.product_id = men.product_id\n    LEFT JOIN members m on m.customer_id = s.customer_id\nORDER BY s.customer_id,\n    s.order_date;\n\n\nDisplaying records 1 - 10\n\n\ncustomer_id\norder_date\nproduct_name\nprice\ncase\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#closing-the-connection",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#closing-the-connection",
    "title": "Danny’s Diner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#introduction",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#introduction",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "title": "Pizza Runner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#database-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#database-connection",
    "title": "Pizza Runner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"pizza_runner\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#queries",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#queries",
    "title": "Pizza Runner",
    "section": "Queries",
    "text": "Queries\n\nA. Pizza Metrics\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\nselect count(pizza_id)\nfrom customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n14\n\n\n\n\n\n\n\n2. How many unique customer orders were made?\n\n\nSELECT count(DISTINCT order_id) from customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n10\n\n\n\n\n\n\n\n3. How many successful orders were delivered by each runner?\n\n\nselect runner_id,\n    count(order_id) as count_of_successful_orders\nfrom runner_orders\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP BY runner_id;\n\n\n3 records\n\n\nrunner_id\ncount_of_successful_orders\n\n\n\n\n3\n1\n\n\n2\n3\n\n\n1\n4\n\n\n\n\n\n\n\n4. How many of each type of pizza was delivered?\n\n\nselect pn.pizza_name as pizza,\n    count(co.pizza_id) as count_of_pizza_delivered\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id\n    INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP by pn.pizza_name;\n\n\n2 records\n\n\npizza\ncount_of_pizza_delivered\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\n\n5. How many Vegetarian and Meatlovers were ordered by each customer?\n\n\nselect co.customer_id as customers, count(co.pizza_id) as count_of_pizza_ordered from customer_orders co INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id group by 1;\n\n\n5 records\n\n\ncustomers\ncount_of_pizza_ordered\n\n\n\n\n101\n3\n\n\n103\n4\n\n\n104\n3\n\n\n105\n1\n\n\n102\n3\n\n\n\n\n\n\n\n6. What was the maximum number of pizzas delivered in a single order?\n\n\nselect ro.order_id as order,\n    count(co.pizza_id) as number_of_pizzas\nfrom runner_orders ro\n    INNER JOIN customer_orders co ON ro.order_id = co.order_id\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 1;\n\n\n1 records\n\n\norder\nnumber_of_pizzas\n\n\n\n\n4\n3\n\n\n\n\n\n\n\n7. For each customer, how many delivered pizzas had at least 1 change and how many? and no changes?\n\n-- PART 1\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with at least 1 changes\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n        or (\n            exclusions not in ('', 'null')\n            or extras not in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n4 records\n\n\ncustomer_id\ndelivered_pizzas_with_changes\n\n\n\n\n102\n1\n\n\n105\n1\n\n\n104\n3\n\n\n103\n3\n\n\n\n\n\n\n-- PART 2: orders with no changes\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions = ''\n        and extras = ''\n        or (\n            exclusions in ('', 'null')\n            or extras in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_no_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n5 records\n\n\ncustomer_id\ndelivered_pizzas_with_no_changes\n\n\n\n\n101\n2\n\n\n102\n3\n\n\n103\n3\n\n\n104\n2\n\n\n105\n1\n\n\n\n\n\n\n\n8. How many pizzas were delivered that had both exclusions and extras?\n\n\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with both exclusions and extras\ncte2 as (\n    select *\n    from customer_orders\n    where (\n            exclusions &lt;&gt; 'null'\n            and extras &lt;&gt; 'null'\n        )\n        and exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n)\nselect *\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id;\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\norder_id..7\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10minutes\nnull\n10\n104\n1\n2, 6\n1, 4\n2020-01-11 18:34:49\n\n\n\n\n\n\n\n9. What was the total volume of pizzas ordered for each hour of the day?\n\n\nselect EXTRACT (\n        hour\n        from order_time\n    ) as hour_of_day,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n6 records\n\n\nhour_of_day\npizza_volume\n\n\n\n\n18\n3\n\n\n21\n3\n\n\n23\n3\n\n\n13\n3\n\n\n19\n1\n\n\n11\n1\n\n\n\n\n\n\n\n10. What was the volume of orders for each day of the week?\n\n-- select EXTRACT (\n--        dow\n--        from order_time\n--    ) as day_of_week,\n--    count(pizza_id) as pizza_volume\n-- from customer_orders\n-- GROUP BY 1;\n-- or\nselect to_char(order_time, 'Day') as day_of_week,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n4 records\n\n\nday_of_week\npizza_volume\n\n\n\n\nSaturday\n5\n\n\nThursday\n3\n\n\nFriday\n1\n\n\nWednesday\n5\n\n\n\n\n\n\n\n\nB. Runner and Customer Experience\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\n\nselect EXTRACT(\n        week\n        from registration_date\n    ),\n    count(runner_id)\nfrom runners\nGROUP BY 1;\n\n\n3 records\n\n\nextract\ncount\n\n\n\n\n1\n1\n\n\n53\n2\n\n\n2\n1\n\n\n\n\n\n\n\n2. What was the average time in minutes it took for each runner to arrive at the Pizza – Runner HQ to pickup the order?\n\nselect case\n        when ro.pickup_time = 'null' then null\n        else round(\n            EXTRACT(\n                epoch\n                from (\n                        ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                    )\n            ) / 60,\n            2\n        )\n    end as duration_till_pickup,\n    ro.pickup_time,\n    co.order_time\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id; \n\n\nDisplaying records 1 - 10\n\n\nduration_till_pickup\npickup_time\norder_time\n\n\n\n\n10.53\n2020-01-01 18:15:34\n2020-01-01 18:05:02\n\n\n10.03\n2020-01-01 19:10:54\n2020-01-01 19:00:52\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n10.47\n2020-01-08 21:10:57\n2020-01-08 21:00:29\n\n\nNA\nnull\n2020-01-08 21:03:13\n\n\n10.27\n2020-01-08 21:30:45\n2020-01-08 21:20:29\n\n\n\n\n\nThis query calculates the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pick up the order. It first joins the customer_orders and runner_orders tables to get the order information and the runner’s pickup time. Then, it checks if the pickup_time is null, and if so, it sets the corresponding duration_till_pickup to null. Otherwise, it calculates the time difference between the pickup_time and the order_time in seconds using the EXTRACT() and TIMESTAMP()functions. It then converts the time difference to minutes and rounds it to two decimal places. Finally, it selects the duration_till_pickup, pickup_time, and order_time for each order.\n\n\n3. What was the average time in minutes it took for each runner to deliver pizzas?\n\n\nselect runner_id,\n    round(\n        avg(\n            case\n                when left(duration, 2) ~ '^\\d+$' THEN cast(left(duration, 2) as integer)\n                else null\n            end\n        ),\n        2\n    ) as extracted_minutes\nfrom runner_orders\ngroup by runner_id;\n\n\n3 records\n\n\nrunner_id\nextracted_minutes\n\n\n\n\n3\n15.00\n\n\n2\n26.67\n\n\n1\n22.25\n\n\n\n\n\n\n\n5. What was the difference between the longest and shortest delivery times for all?\n\n\n-- If we define delivery times as the duration between ro.pickup_time - co.order_time + ro.duration\n-- then:\nwith cte1 as (\n    select case\n            when ro.pickup_time = 'null' then null\n            else round(\n                EXTRACT(\n                    epoch\n                    from (\n                            ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                        )\n                ) / 60,\n                2\n            )\n        end as duration_till_pickup,\n        ro.pickup_time,\n        co.order_time,\n        round(\n            case\n                when left(ro.duration, 2) ~ '^\\d+$' THEN cast(left(ro.duration, 2) as integer)\n                else null\n            end,\n            2\n        ) as cleaned_duration_minutes\n    from customer_orders co\n        INNER JOIN runner_orders ro on co.order_id = ro.order_id\n)\nselect max(duration_till_pickup + cleaned_duration_minutes) as longest_delivery_time,\n    min(duration_till_pickup + cleaned_duration_minutes) as shortest_delivery_time,\n    max(duration_till_pickup + cleaned_duration_minutes) - min(duration_till_pickup + cleaned_duration_minutes) as difference\nfrom cte1;\n\n\n1 records\n\n\nlongest_delivery_time\nshortest_delivery_time\ndifference\n\n\n\n\n69.28\n25.47\n43.81\n\n\n\n\n\nNow, this is a monstrous looking one 😅 .\nA (CTE) named cte1 is created, extracting relevant information such as duration_till_pickup, pickup_time, order_time, and cleaned_duration_minutes from the customer_orders (co) and runner_orders (ro) tables. The duration_till_pickup is calculated as the time between pickup and order in minutes. The cleaned_duration_minutes extracts the duration in minutes from the ro.duration field.\nThe main query then computes the maximum, minimum, and the difference between the sum of duration_till_pickup and cleaned_duration_minutes. These values represent the longest, shortest, and the time difference between delivery times for all orders.\n\n\n6. What was the average speed for each runner for each delivery and do you notice – any trend for these values?\n\n\nwith cte as (\n    select runner_id,\n        case\n            when distance ~ '.*' THEN cast(substring(distance, '[0-9\\-+\\.]+') as float)\n            else null\n        end as cleaned_distance_km,\n        case\n            when duration ~ '.*' THEN cast(substring(duration, '[0-9\\-+\\.]+') as float) / 60\n            else null\n        end as cleaned_duration_hr\n    from runner_orders\n)\nselect runner_id,\n    avg(cleaned_distance_km / cleaned_duration_hr) as speed_km_hr\nfrom cte\ngroup by 1;\n\n\n3 records\n\n\nrunner_id\nspeed_km_hr\n\n\n\n\n3\n40.00000\n\n\n2\n62.90000\n\n\n1\n45.53611\n\n\n\n\n\nThis psql query calculates the average speed for each runner for each delivery. It uses a CTE named cte to clean and extract relevant information such as cleaned_distance_km and cleaned_duration_hr from the runner_orders table. The main query then computes the average speed (cleaned_distance_km / cleaned_duration_hr) for each runner and presents the results grouped by runner_id. This allows you to observe trends in the average speed of each runner across their deliveries.\n\n\n7. What is the successful delivery percentage for each runner?\n\nwith part as (\n    select cte.runner_id,\n        count(*) as part_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    where cancel is null\n    group by runner_id\n),\nwhole as (\n    select runner_id,\n        count(*) as whole_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    group by runner_id\n)\nselect p.runner_id,\n    case\n        when w.whole_cancel = 0 then null\n        else round(\n            (p.part_cancel::numeric / w.whole_cancel) * 100,\n            2\n        )\n    end as percent\nfrom part p\n    inner join whole w on p.runner_id = w.runner_id;\n\n\n3 records\n\n\nrunner_id\npercent\n\n\n\n\n3\n50\n\n\n2\n75\n\n\n1\n100\n\n\n\n\n\n\n\n\nC. Ingredient Optimisation\n\n1. What are the standard ingredients for each pizza?\n\n\nselect pn.pizza_name, pt.topping_name\nfrom pizza_names pn inner join new_pizza_recipes np\non pn.pizza_id = np.pizza_id\ninner join pizza_toppings pt on pt.topping_id::text = ANY (np.toppings)\n\n\nDisplaying records 1 - 10\n\n\npizza_name\ntopping_name\n\n\n\n\nMeatlovers\nBacon\n\n\nMeatlovers\nBBQ Sauce\n\n\nMeatlovers\nBeef\n\n\nMeatlovers\nCheese\n\n\nMeatlovers\nChicken\n\n\nMeatlovers\nMushrooms\n\n\nMeatlovers\nPepperoni\n\n\nMeatlovers\nSalami\n\n\nVegetarian\nCheese\n\n\nVegetarian\nMushrooms\n\n\n\n\n\n\n\n\nD. Pricing and Ratings\n\n1. If a Meat Lovers pizza costs $12 and Vegetarian costs $10 and there were no charges for changes, how much money has Pizza Runner made so far if there are no delivery fees?\n\n-- Creating a view of cleaned data\nCREATE OR REPLACE VIEW clean_runner_orders AS\nselect order_id,\n    runner_id,\n    CASE\n        WHEN pickup_time = 'null' THEN NULL\n        ELSE pickup_time::TIMESTAMP\n    END,\n    cast(substring(distance, '[0-9\\-+\\.]+') as float) as distance,\n    cast(substring(duration, '[0-9\\-+\\.]+') as float) as duration,\n    nullif(cancellation, '') || nullif(cancellation, 'null') as cancellation\nfrom runner_orders;\n--\n\n\nwith cte1 as (\n    select co.pizza_id,\n        count(co.pizza_id) as quantity_sold\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n    GROUP BY 1\n)\nselect pizza_id,\n    quantity_sold * price as revenue\nfrom (\n        select *,\n            CASE\n                WHEN cte1.pizza_id = 1 THEN 12\n                WHEN cte1.pizza_id = 2 THEN 10\n            END AS price\n        from cte1\n    ) sq\n\n\n2 records\n\n\npizza_id\nrevenue\n\n\n\n\n1\n108\n\n\n2\n30\n\n\n\n\n\n\n-- Creating a random integer generator\ncreate or replace function random_between(low int, high int) returns int as $$ begin return floor(random() * (high - low + 1) + low);\nend;\n$$ language 'plpgsql' STRICT;\n\n\n\n2. If a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre travelled, how much money does Pizza Runner have left over after these deliveries?\n\nwith cte as (\n    select co.pizza_id,\n        ro.distance * 0.3 as runner_cost,\n        CASE\n            WHEN co.pizza_id = 1 THEN 12\n            WHEN co.pizza_id = 2 THEN 10\n        END AS price\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n)\nselect sum(price) revenue,\n    round(sum(runner_cost)::numeric, 2) cost,\n    round(sum(price) - sum(runner_cost)::numeric, 2) profit\nfrom cte\n\n\n1 records\n\n\nrevenue\ncost\nprofit\n\n\n\n\n138\n64.62\n73.38"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#closing-the-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#closing-the-connection",
    "title": "Pizza Runner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html",
    "title": "Data Bank",
    "section": "",
    "text": "Data Bank runs just like any other digital bank - but with a distributed data storage platform. Customers are allocated cloud data storage limits which are directly linked to how much money they have in their accounts. The management team at Data Bank want to increase their total customer base - but also need some help tracking just how much data storage their customers will need. This case study is all about calculating metrics, growth and helping the business analyse their data in a smart way to better forecast and plan for their future developments!\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"data_bank\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "title": "Data Bank",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#database-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#database-connection",
    "title": "Data Bank",
    "section": "Database Connection",
    "text": "Database Connection"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#queries",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#queries",
    "title": "Data Bank",
    "section": "Queries",
    "text": "Queries"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html",
    "href": "posts/2023-11-21-BDA-1/index.html",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "",
    "text": "#|message: false\n#|wwarning: false\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(boot)\n\nWarning: package 'boot' was built under R version 4.3.2\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.2\n\nlibrary(broom)\n# library(boot) #Required for Bootstrap simulations\n# library(rstudioapi)"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#variable-description",
    "href": "posts/2023-11-21-BDA-1/index.html#variable-description",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Variable description",
    "text": "Variable description"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "href": "posts/2023-11-21-BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Why Correlation Is Not Causation: A Confounder in Action",
    "text": "Why Correlation Is Not Causation: A Confounder in Action\n\n#Reading the data\nstand_tib &lt;- read_csv(\"chap1-stand_data.csv\")\n\nRows: 2400 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): icecream_sales, malt_sales, summer_months, temps\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(stand_tib) %&gt;% gt() %&gt;% opt_stylize(style = 6, color=\"cyan\")\n\n\n\n\n\n  \n    \n    \n      icecream_sales\n      malt_sales\n      summer_months\n      temps\n    \n  \n  \n    25649.78\n28592.18\n0\n28.59314\n    32694.93\n37152.84\n0\n37.13064\n    26467.80\n24074.19\n0\n24.11190\n    43438.79\n49169.47\n0\n49.21807\n    52452.63\n47249.40\n0\n47.25766\n    34130.66\n34228.50\n0\n34.25746\n  \n  \n  \n\n\n\n\n\n#Running linear regressions \n#Biased model (coeff should be 1,000)\nlm(icecream_sales ~ temps, data = stand_tib) %&gt;% \n  tidy() %&gt;% \n  gt() %&gt;% opt_stylize(style = 6, color =\"cyan\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-6169.844\n531.5062\n-11.60823\n2.385706e-30\n    temps\n1171.335\n9.0268\n129.76197\n0.000000e+00\n  \n  \n  \n\n\n\n\n\n#correct model for icecream (coeffs should be 1,000 and 20,000)\nsummary(lm(icecream_sales ~ temps + summer_months, data=stand_tib)) %&gt;% \n  tidy() %&gt;% \n  gt() %&gt;% opt_stylize(style = 6, color =\"cyan\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-7.068995\n374.401378\n-0.01888079\n0.9849378\n    temps\n1001.481846\n6.822242\n146.79657789\n0.0000000\n    summer_months\n19556.774716\n361.393093\n54.11496531\n0.0000000\n  \n  \n  \n\n\n\n\n\n#Model biased by extra controlling\n#note how the standard error of the estimate of temp increases a lot\n#note how the p-values for \nsummary(lm(icecream_sales ~ malt_sales + temps + summer_months, \n             data = stand_tib)) %&gt;% \n  tidy() %&gt;% \n  gt() %&gt;% opt_stylize(style = 6, color =\"cyan\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-15.827140\n374.580924\n-0.04225293\n0.9663006\n    malt_sales\n-1.701081\n2.082874\n-0.81669912\n0.4141816\n    temps\n2702.788460\n2083.160984\n1.29744580\n0.1946027\n    summer_months\n19548.167641\n361.571822\n54.06441117\n0.0000000\n  \n  \n  \n\n\n\n\n\nggplot(stand_tib, aes(x=icecream_sales, y=malt_sales)) + \n    geom_point() + labs(x='Malt sales', y='Iced coffee sales') + \n    theme_classic()"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#second-example",
    "href": "posts/2023-11-21-BDA-1/index.html#second-example",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Second Example",
    "text": "Second Example\n\n#Reading the data\nsurvey_tib &lt;- read_csv(\"chap1-survey_data.csv\")\n\nRows: 10000 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): vanilla, chocolate, shopped\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Reformatting shopped variable to binary\nsurvey_tib &lt;- survey_tib %&gt;%\n  mutate(shopped = as.factor(shopped) %&gt;% relevel(ref=\"0\"))\n\n\n## Figure 1-5. (a) Tastes for vanilla and chocolate are uncorrelated in the \n## overall population. (b) Taste for vanilla is higher for people who shop at \n## the ice-cream stand than for people who don’t. (c) Same thing with the taste for chocolate.\n\n#Scatterplot of chocolate versus vanilla taste \na &lt;- ggplot(survey_tib, aes(x=vanilla, y=chocolate)) + geom_point() +\n  xlim(c(0,28)) + ylim(c(0,28)) + geom_smooth(formula = 'y~x', method = lm, se = FALSE) +\n  labs(x='Taste for vanilla', y='Taste for chocolate') + theme_minimal()\n\n#Boxplot of vanilla taste against shopping behavior\nb &lt;- ggplot(survey_tib, aes(shopped, vanilla)) + geom_boxplot() +\n  labs(x='Shopped (Y/N)', y='Taste for vanilla') + ylim(c(0,30)) + theme_minimal()\n\n#Boxplot of chocolate taste against shopping behavior\nc &lt;- ggplot(survey_tib, aes(shopped, chocolate)) + geom_boxplot() +\n  labs(x='Shopped (Y/N)', y='Taste for chocolate') + ylim(c(0,30)) + theme_minimal()\n\nggpubr::ggarrange(a,b,c, ncol = 3)\n\n\n\n\n\n\n\n### Plotting same scatterplot but for shoppers only\n\n#Figure 1-6 Taste for vanilla and chocolate among shoppers. \nd &lt;- ggplot(survey_tib %&gt;% filter(shopped=='1'), aes(x=vanilla, y=chocolate)) + geom_point() + \n  labs(x='Taste for vanilla', y='Taste for chocolate') + geom_smooth(formula='y~x', method = lm,se = FALSE) + theme_minimal()\nd\n\n\n\n\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"pizza_runner\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#queries",
    "href": "posts/2023-11-21-BDA-1/index.html#queries",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Queries",
    "text": "Queries\n\nA. Pizza Metrics\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\nselect count(pizza_id)\nfrom customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n14\n\n\n\n\n\n\n\n2. How many unique customer orders were made?\n\n\nSELECT count(DISTINCT order_id) from customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n10\n\n\n\n\n\n\n\n3. How many successful orders were delivered by each runner?\n\n\nselect runner_id,\n    count(order_id) as count_of_successful_orders\nfrom runner_orders\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP BY runner_id;\n\n\n3 records\n\n\nrunner_id\ncount_of_successful_orders\n\n\n\n\n3\n1\n\n\n2\n3\n\n\n1\n4\n\n\n\n\n\n\n\n4. How many of each type of pizza was delivered?\n\n\nselect pn.pizza_name as pizza,\n    count(co.pizza_id) as count_of_pizza_delivered\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id\n    INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP by pn.pizza_name;\n\n\n2 records\n\n\npizza\ncount_of_pizza_delivered\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\n\n5. How many Vegetarian and Meatlovers were ordered by each customer?\n\n\nselect co.customer_id as customers, count(co.pizza_id) as count_of_pizza_ordered from customer_orders co INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id group by 1;\n\n\n5 records\n\n\ncustomers\ncount_of_pizza_ordered\n\n\n\n\n101\n3\n\n\n103\n4\n\n\n104\n3\n\n\n105\n1\n\n\n102\n3\n\n\n\n\n\n\n\n6. What was the maximum number of pizzas delivered in a single order?\n\n\nselect ro.order_id as order,\n    count(co.pizza_id) as number_of_pizzas\nfrom runner_orders ro\n    INNER JOIN customer_orders co ON ro.order_id = co.order_id\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 1;\n\n\n1 records\n\n\norder\nnumber_of_pizzas\n\n\n\n\n4\n3\n\n\n\n\n\n\n\n7. For each customer, how many delivered pizzas had at least 1 change and how many? and no changes?\n\n-- PART 1\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with at least 1 changes\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n        or (\n            exclusions not in ('', 'null')\n            or extras not in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n4 records\n\n\ncustomer_id\ndelivered_pizzas_with_changes\n\n\n\n\n102\n1\n\n\n105\n1\n\n\n104\n3\n\n\n103\n3\n\n\n\n\n\n\n-- PART 2: orders with no changes\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions = ''\n        and extras = ''\n        or (\n            exclusions in ('', 'null')\n            or extras in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_no_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n5 records\n\n\ncustomer_id\ndelivered_pizzas_with_no_changes\n\n\n\n\n101\n2\n\n\n102\n3\n\n\n103\n3\n\n\n104\n2\n\n\n105\n1\n\n\n\n\n\n\n\n8. How many pizzas were delivered that had both exclusions and extras?\n\n\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with both exclusions and extras\ncte2 as (\n    select *\n    from customer_orders\n    where (\n            exclusions &lt;&gt; 'null'\n            and extras &lt;&gt; 'null'\n        )\n        and exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n)\nselect *\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id;\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\norder_id..7\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10minutes\nnull\n10\n104\n1\n2, 6\n1, 4\n2020-01-11 18:34:49\n\n\n\n\n\n\n\n9. What was the total volume of pizzas ordered for each hour of the day?\n\n\nselect EXTRACT (\n        hour\n        from order_time\n    ) as hour_of_day,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n6 records\n\n\nhour_of_day\npizza_volume\n\n\n\n\n18\n3\n\n\n21\n3\n\n\n23\n3\n\n\n13\n3\n\n\n19\n1\n\n\n11\n1\n\n\n\n\n\n\n\n10. What was the volume of orders for each day of the week?\n\n-- select EXTRACT (\n--        dow\n--        from order_time\n--    ) as day_of_week,\n--    count(pizza_id) as pizza_volume\n-- from customer_orders\n-- GROUP BY 1;\n-- or\nselect to_char(order_time, 'Day') as day_of_week,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n4 records\n\n\nday_of_week\npizza_volume\n\n\n\n\nSaturday\n5\n\n\nThursday\n3\n\n\nFriday\n1\n\n\nWednesday\n5\n\n\n\n\n\n\n\n\nB. Runner and Customer Experience\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\n\nselect EXTRACT(\n        week\n        from registration_date\n    ),\n    count(runner_id)\nfrom runners\nGROUP BY 1;\n\n\n3 records\n\n\nextract\ncount\n\n\n\n\n1\n1\n\n\n53\n2\n\n\n2\n1\n\n\n\n\n\n\n\n2. What was the average time in minutes it took for each runner to arrive at the Pizza – Runner HQ to pickup the order?\n\nselect case\n        when ro.pickup_time = 'null' then null\n        else round(\n            EXTRACT(\n                epoch\n                from (\n                        ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                    )\n            ) / 60,\n            2\n        )\n    end as duration_till_pickup,\n    ro.pickup_time,\n    co.order_time\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id; \n\n\nDisplaying records 1 - 10\n\n\nduration_till_pickup\npickup_time\norder_time\n\n\n\n\n10.53\n2020-01-01 18:15:34\n2020-01-01 18:05:02\n\n\n10.03\n2020-01-01 19:10:54\n2020-01-01 19:00:52\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n10.47\n2020-01-08 21:10:57\n2020-01-08 21:00:29\n\n\nNA\nnull\n2020-01-08 21:03:13\n\n\n10.27\n2020-01-08 21:30:45\n2020-01-08 21:20:29\n\n\n\n\n\nThis query calculates the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pick up the order. It first joins the customer_orders and runner_orders tables to get the order information and the runner’s pickup time. Then, it checks if the pickup_time is null, and if so, it sets the corresponding duration_till_pickup to null. Otherwise, it calculates the time difference between the pickup_time and the order_time in seconds using the EXTRACT() and TIMESTAMP()functions. It then converts the time difference to minutes and rounds it to two decimal places. Finally, it selects the duration_till_pickup, pickup_time, and order_time for each order.\n\n\n3. What was the average time in minutes it took for each runner to deliver pizzas?\n\n\nselect runner_id,\n    round(\n        avg(\n            case\n                when left(duration, 2) ~ '^\\d+$' THEN cast(left(duration, 2) as integer)\n                else null\n            end\n        ),\n        2\n    ) as extracted_minutes\nfrom runner_orders\ngroup by runner_id;\n\n\n3 records\n\n\nrunner_id\nextracted_minutes\n\n\n\n\n3\n15.00\n\n\n2\n26.67\n\n\n1\n22.25\n\n\n\n\n\n\n\n5. What was the difference between the longest and shortest delivery times for all?\n\n\n-- If we define delivery times as the duration between ro.pickup_time - co.order_time + ro.duration\n-- then:\nwith cte1 as (\n    select case\n            when ro.pickup_time = 'null' then null\n            else round(\n                EXTRACT(\n                    epoch\n                    from (\n                            ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                        )\n                ) / 60,\n                2\n            )\n        end as duration_till_pickup,\n        ro.pickup_time,\n        co.order_time,\n        round(\n            case\n                when left(ro.duration, 2) ~ '^\\d+$' THEN cast(left(ro.duration, 2) as integer)\n                else null\n            end,\n            2\n        ) as cleaned_duration_minutes\n    from customer_orders co\n        INNER JOIN runner_orders ro on co.order_id = ro.order_id\n)\nselect max(duration_till_pickup + cleaned_duration_minutes) as longest_delivery_time,\n    min(duration_till_pickup + cleaned_duration_minutes) as shortest_delivery_time,\n    max(duration_till_pickup + cleaned_duration_minutes) - min(duration_till_pickup + cleaned_duration_minutes) as difference\nfrom cte1;\n\n\n1 records\n\n\nlongest_delivery_time\nshortest_delivery_time\ndifference\n\n\n\n\n69.28\n25.47\n43.81\n\n\n\n\n\nNow, this is a monstrous looking one 😅 .\nA (CTE) named cte1 is created, extracting relevant information such as duration_till_pickup, pickup_time, order_time, and cleaned_duration_minutes from the customer_orders (co) and runner_orders (ro) tables. The duration_till_pickup is calculated as the time between pickup and order in minutes. The cleaned_duration_minutes extracts the duration in minutes from the ro.duration field.\nThe main query then computes the maximum, minimum, and the difference between the sum of duration_till_pickup and cleaned_duration_minutes. These values represent the longest, shortest, and the time difference between delivery times for all orders.\n\n\n6. What was the average speed for each runner for each delivery and do you notice – any trend for these values?\n\n\nwith cte as (\n    select runner_id,\n        case\n            when distance ~ '.*' THEN cast(substring(distance, '[0-9\\-+\\.]+') as float)\n            else null\n        end as cleaned_distance_km,\n        case\n            when duration ~ '.*' THEN cast(substring(duration, '[0-9\\-+\\.]+') as float) / 60\n            else null\n        end as cleaned_duration_hr\n    from runner_orders\n)\nselect runner_id,\n    avg(cleaned_distance_km / cleaned_duration_hr) as speed_km_hr\nfrom cte\ngroup by 1;\n\n\n3 records\n\n\nrunner_id\nspeed_km_hr\n\n\n\n\n3\n40.00000\n\n\n2\n62.90000\n\n\n1\n45.53611\n\n\n\n\n\nThis psql query calculates the average speed for each runner for each delivery. It uses a CTE named cte to clean and extract relevant information such as cleaned_distance_km and cleaned_duration_hr from the runner_orders table. The main query then computes the average speed (cleaned_distance_km / cleaned_duration_hr) for each runner and presents the results grouped by runner_id. This allows you to observe trends in the average speed of each runner across their deliveries.\n\n\n7. What is the successful delivery percentage for each runner?\n\nwith part as (\n    select cte.runner_id,\n        count(*) as part_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    where cancel is null\n    group by runner_id\n),\nwhole as (\n    select runner_id,\n        count(*) as whole_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    group by runner_id\n)\nselect p.runner_id,\n    case\n        when w.whole_cancel = 0 then null\n        else round(\n            (p.part_cancel::numeric / w.whole_cancel) * 100,\n            2\n        )\n    end as percent\nfrom part p\n    inner join whole w on p.runner_id = w.runner_id;\n\n\n3 records\n\n\nrunner_id\npercent\n\n\n\n\n3\n50\n\n\n2\n75\n\n\n1\n100\n\n\n\n\n\n\n\n\nC. Ingredient Optimisation\n\n1. What are the standard ingredients for each pizza?\n\n\nselect pn.pizza_name, pt.topping_name\nfrom pizza_names pn inner join new_pizza_recipes np\non pn.pizza_id = np.pizza_id\ninner join pizza_toppings pt on pt.topping_id::text = ANY (np.toppings)\n\n\nDisplaying records 1 - 10\n\n\npizza_name\ntopping_name\n\n\n\n\nMeatlovers\nBacon\n\n\nMeatlovers\nBBQ Sauce\n\n\nMeatlovers\nBeef\n\n\nMeatlovers\nCheese\n\n\nMeatlovers\nChicken\n\n\nMeatlovers\nMushrooms\n\n\nMeatlovers\nPepperoni\n\n\nMeatlovers\nSalami\n\n\nVegetarian\nCheese\n\n\nVegetarian\nMushrooms\n\n\n\n\n\n\n\n\nD. Pricing and Ratings\n\n1. If a Meat Lovers pizza costs $12 and Vegetarian costs $10 and there were no charges for changes, how much money has Pizza Runner made so far if there are no delivery fees?\n\n-- Creating a view of cleaned data\nCREATE OR REPLACE VIEW clean_runner_orders AS\nselect order_id,\n    runner_id,\n    CASE\n        WHEN pickup_time = 'null' THEN NULL\n        ELSE pickup_time::TIMESTAMP\n    END,\n    cast(substring(distance, '[0-9\\-+\\.]+') as float) as distance,\n    cast(substring(duration, '[0-9\\-+\\.]+') as float) as duration,\n    nullif(cancellation, '') || nullif(cancellation, 'null') as cancellation\nfrom runner_orders;\n--\n\n\nwith cte1 as (\n    select co.pizza_id,\n        count(co.pizza_id) as quantity_sold\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n    GROUP BY 1\n)\nselect pizza_id,\n    quantity_sold * price as revenue\nfrom (\n        select *,\n            CASE\n                WHEN cte1.pizza_id = 1 THEN 12\n                WHEN cte1.pizza_id = 2 THEN 10\n            END AS price\n        from cte1\n    ) sq\n\n\n2 records\n\n\npizza_id\nrevenue\n\n\n\n\n1\n108\n\n\n2\n30\n\n\n\n\n\n\n-- Creating a random integer generator\ncreate or replace function random_between(low int, high int) returns int as $$ begin return floor(random() * (high - low + 1) + low);\nend;\n$$ language 'plpgsql' STRICT;\n\n\n\n2. If a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre travelled, how much money does Pizza Runner have left over after these deliveries?\n\nwith cte as (\n    select co.pizza_id,\n        ro.distance * 0.3 as runner_cost,\n        CASE\n            WHEN co.pizza_id = 1 THEN 12\n            WHEN co.pizza_id = 2 THEN 10\n        END AS price\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n)\nselect sum(price) revenue,\n    round(sum(runner_cost)::numeric, 2) cost,\n    round(sum(price) - sum(runner_cost)::numeric, 2) profit\nfrom cte\n\n\n1 records\n\n\nrevenue\ncost\nprofit\n\n\n\n\n138\n64.62\n73.38"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#closing-the-connection",
    "href": "posts/2023-11-21-BDA-1/index.html#closing-the-connection",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html",
    "title": "Data Bank",
    "section": "",
    "text": "Data Bank runs just like any other digital bank - but with a distributed data storage platform. Customers are allocated cloud data storage limits which are directly linked to how much money they have in their accounts. The management team at Data Bank want to increase their total customer base - but also need some help tracking just how much data storage their customers will need. This case study is all about calculating metrics, growth and helping the business analyse their data in a smart way to better forecast and plan for their future developments!\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"data_bank\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "title": "Data Bank",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#database-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#database-connection",
    "title": "Data Bank",
    "section": "Database Connection",
    "text": "Database Connection"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#queries",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#queries",
    "title": "Data Bank",
    "section": "Queries",
    "text": "Queries"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#introduction",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#introduction",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "title": "Pizza Runner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#database-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#database-connection",
    "title": "Pizza Runner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"pizza_runner\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#queries",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#queries",
    "title": "Pizza Runner",
    "section": "Queries",
    "text": "Queries\n\nA. Pizza Metrics\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\nselect count(pizza_id)\nfrom customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n14\n\n\n\n\n\n\n\n2. How many unique customer orders were made?\n\n\nSELECT count(DISTINCT order_id) from customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n10\n\n\n\n\n\n\n\n3. How many successful orders were delivered by each runner?\n\n\nselect runner_id,\n    count(order_id) as count_of_successful_orders\nfrom runner_orders\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP BY runner_id;\n\n\n3 records\n\n\nrunner_id\ncount_of_successful_orders\n\n\n\n\n3\n1\n\n\n2\n3\n\n\n1\n4\n\n\n\n\n\n\n\n4. How many of each type of pizza was delivered?\n\n\nselect pn.pizza_name as pizza,\n    count(co.pizza_id) as count_of_pizza_delivered\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id\n    INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP by pn.pizza_name;\n\n\n2 records\n\n\npizza\ncount_of_pizza_delivered\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\n\n5. How many Vegetarian and Meatlovers were ordered by each customer?\n\n\nselect co.customer_id as customers, count(co.pizza_id) as count_of_pizza_ordered from customer_orders co INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id group by 1;\n\n\n5 records\n\n\ncustomers\ncount_of_pizza_ordered\n\n\n\n\n101\n3\n\n\n103\n4\n\n\n104\n3\n\n\n105\n1\n\n\n102\n3\n\n\n\n\n\n\n\n6. What was the maximum number of pizzas delivered in a single order?\n\n\nselect ro.order_id as order,\n    count(co.pizza_id) as number_of_pizzas\nfrom runner_orders ro\n    INNER JOIN customer_orders co ON ro.order_id = co.order_id\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 1;\n\n\n1 records\n\n\norder\nnumber_of_pizzas\n\n\n\n\n4\n3\n\n\n\n\n\n\n\n7. For each customer, how many delivered pizzas had at least 1 change and how many? and no changes?\n\n-- PART 1\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with at least 1 changes\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n        or (\n            exclusions not in ('', 'null')\n            or extras not in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n4 records\n\n\ncustomer_id\ndelivered_pizzas_with_changes\n\n\n\n\n102\n1\n\n\n105\n1\n\n\n104\n3\n\n\n103\n3\n\n\n\n\n\n\n-- PART 2: orders with no changes\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions = ''\n        and extras = ''\n        or (\n            exclusions in ('', 'null')\n            or extras in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_no_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n5 records\n\n\ncustomer_id\ndelivered_pizzas_with_no_changes\n\n\n\n\n101\n2\n\n\n102\n3\n\n\n103\n3\n\n\n104\n2\n\n\n105\n1\n\n\n\n\n\n\n\n8. How many pizzas were delivered that had both exclusions and extras?\n\n\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with both exclusions and extras\ncte2 as (\n    select *\n    from customer_orders\n    where (\n            exclusions &lt;&gt; 'null'\n            and extras &lt;&gt; 'null'\n        )\n        and exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n)\nselect *\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id;\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\norder_id..7\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10minutes\nnull\n10\n104\n1\n2, 6\n1, 4\n2020-01-11 18:34:49\n\n\n\n\n\n\n\n9. What was the total volume of pizzas ordered for each hour of the day?\n\n\nselect EXTRACT (\n        hour\n        from order_time\n    ) as hour_of_day,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n6 records\n\n\nhour_of_day\npizza_volume\n\n\n\n\n18\n3\n\n\n21\n3\n\n\n23\n3\n\n\n13\n3\n\n\n19\n1\n\n\n11\n1\n\n\n\n\n\n\n\n10. What was the volume of orders for each day of the week?\n\n-- select EXTRACT (\n--        dow\n--        from order_time\n--    ) as day_of_week,\n--    count(pizza_id) as pizza_volume\n-- from customer_orders\n-- GROUP BY 1;\n-- or\nselect to_char(order_time, 'Day') as day_of_week,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n4 records\n\n\nday_of_week\npizza_volume\n\n\n\n\nSaturday\n5\n\n\nThursday\n3\n\n\nFriday\n1\n\n\nWednesday\n5\n\n\n\n\n\n\n\n\nB. Runner and Customer Experience\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\n\nselect EXTRACT(\n        week\n        from registration_date\n    ),\n    count(runner_id)\nfrom runners\nGROUP BY 1;\n\n\n3 records\n\n\nextract\ncount\n\n\n\n\n1\n1\n\n\n53\n2\n\n\n2\n1\n\n\n\n\n\n\n\n2. What was the average time in minutes it took for each runner to arrive at the Pizza – Runner HQ to pickup the order?\n\nselect case\n        when ro.pickup_time = 'null' then null\n        else round(\n            EXTRACT(\n                epoch\n                from (\n                        ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                    )\n            ) / 60,\n            2\n        )\n    end as duration_till_pickup,\n    ro.pickup_time,\n    co.order_time\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id; \n\n\nDisplaying records 1 - 10\n\n\nduration_till_pickup\npickup_time\norder_time\n\n\n\n\n10.53\n2020-01-01 18:15:34\n2020-01-01 18:05:02\n\n\n10.03\n2020-01-01 19:10:54\n2020-01-01 19:00:52\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n10.47\n2020-01-08 21:10:57\n2020-01-08 21:00:29\n\n\nNA\nnull\n2020-01-08 21:03:13\n\n\n10.27\n2020-01-08 21:30:45\n2020-01-08 21:20:29\n\n\n\n\n\nThis query calculates the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pick up the order. It first joins the customer_orders and runner_orders tables to get the order information and the runner’s pickup time. Then, it checks if the pickup_time is null, and if so, it sets the corresponding duration_till_pickup to null. Otherwise, it calculates the time difference between the pickup_time and the order_time in seconds using the EXTRACT() and TIMESTAMP()functions. It then converts the time difference to minutes and rounds it to two decimal places. Finally, it selects the duration_till_pickup, pickup_time, and order_time for each order.\n\n\n3. What was the average time in minutes it took for each runner to deliver pizzas?\n\n\nselect runner_id,\n    round(\n        avg(\n            case\n                when left(duration, 2) ~ '^\\d+$' THEN cast(left(duration, 2) as integer)\n                else null\n            end\n        ),\n        2\n    ) as extracted_minutes\nfrom runner_orders\ngroup by runner_id;\n\n\n3 records\n\n\nrunner_id\nextracted_minutes\n\n\n\n\n3\n15.00\n\n\n2\n26.67\n\n\n1\n22.25\n\n\n\n\n\n\n\n5. What was the difference between the longest and shortest delivery times for all?\n\n\n-- If we define delivery times as the duration between ro.pickup_time - co.order_time + ro.duration\n-- then:\nwith cte1 as (\n    select case\n            when ro.pickup_time = 'null' then null\n            else round(\n                EXTRACT(\n                    epoch\n                    from (\n                            ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                        )\n                ) / 60,\n                2\n            )\n        end as duration_till_pickup,\n        ro.pickup_time,\n        co.order_time,\n        round(\n            case\n                when left(ro.duration, 2) ~ '^\\d+$' THEN cast(left(ro.duration, 2) as integer)\n                else null\n            end,\n            2\n        ) as cleaned_duration_minutes\n    from customer_orders co\n        INNER JOIN runner_orders ro on co.order_id = ro.order_id\n)\nselect max(duration_till_pickup + cleaned_duration_minutes) as longest_delivery_time,\n    min(duration_till_pickup + cleaned_duration_minutes) as shortest_delivery_time,\n    max(duration_till_pickup + cleaned_duration_minutes) - min(duration_till_pickup + cleaned_duration_minutes) as difference\nfrom cte1;\n\n\n1 records\n\n\nlongest_delivery_time\nshortest_delivery_time\ndifference\n\n\n\n\n69.28\n25.47\n43.81\n\n\n\n\n\nNow, this is a monstrous looking one 😅 .\nA (CTE) named cte1 is created, extracting relevant information such as duration_till_pickup, pickup_time, order_time, and cleaned_duration_minutes from the customer_orders (co) and runner_orders (ro) tables. The duration_till_pickup is calculated as the time between pickup and order in minutes. The cleaned_duration_minutes extracts the duration in minutes from the ro.duration field.\nThe main query then computes the maximum, minimum, and the difference between the sum of duration_till_pickup and cleaned_duration_minutes. These values represent the longest, shortest, and the time difference between delivery times for all orders.\n\n\n6. What was the average speed for each runner for each delivery and do you notice – any trend for these values?\n\n\nwith cte as (\n    select runner_id,\n        case\n            when distance ~ '.*' THEN cast(substring(distance, '[0-9\\-+\\.]+') as float)\n            else null\n        end as cleaned_distance_km,\n        case\n            when duration ~ '.*' THEN cast(substring(duration, '[0-9\\-+\\.]+') as float) / 60\n            else null\n        end as cleaned_duration_hr\n    from runner_orders\n)\nselect runner_id,\n    avg(cleaned_distance_km / cleaned_duration_hr) as speed_km_hr\nfrom cte\ngroup by 1;\n\n\n3 records\n\n\nrunner_id\nspeed_km_hr\n\n\n\n\n3\n40.00000\n\n\n2\n62.90000\n\n\n1\n45.53611\n\n\n\n\n\nThis psql query calculates the average speed for each runner for each delivery. It uses a CTE named cte to clean and extract relevant information such as cleaned_distance_km and cleaned_duration_hr from the runner_orders table. The main query then computes the average speed (cleaned_distance_km / cleaned_duration_hr) for each runner and presents the results grouped by runner_id. This allows you to observe trends in the average speed of each runner across their deliveries.\n\n\n7. What is the successful delivery percentage for each runner?\n\nwith part as (\n    select cte.runner_id,\n        count(*) as part_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    where cancel is null\n    group by runner_id\n),\nwhole as (\n    select runner_id,\n        count(*) as whole_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    group by runner_id\n)\nselect p.runner_id,\n    case\n        when w.whole_cancel = 0 then null\n        else round(\n            (p.part_cancel::numeric / w.whole_cancel) * 100,\n            2\n        )\n    end as percent\nfrom part p\n    inner join whole w on p.runner_id = w.runner_id;\n\n\n3 records\n\n\nrunner_id\npercent\n\n\n\n\n3\n50\n\n\n2\n75\n\n\n1\n100\n\n\n\n\n\n\n\n\nC. Ingredient Optimisation\n\n1. What are the standard ingredients for each pizza?\n\n\nselect pn.pizza_name, pt.topping_name\nfrom pizza_names pn inner join new_pizza_recipes np\non pn.pizza_id = np.pizza_id\ninner join pizza_toppings pt on pt.topping_id::text = ANY (np.toppings)\n\n\nDisplaying records 1 - 10\n\n\npizza_name\ntopping_name\n\n\n\n\nMeatlovers\nBacon\n\n\nMeatlovers\nBBQ Sauce\n\n\nMeatlovers\nBeef\n\n\nMeatlovers\nCheese\n\n\nMeatlovers\nChicken\n\n\nMeatlovers\nMushrooms\n\n\nMeatlovers\nPepperoni\n\n\nMeatlovers\nSalami\n\n\nVegetarian\nCheese\n\n\nVegetarian\nMushrooms\n\n\n\n\n\n\n\n\nD. Pricing and Ratings\n\n1. If a Meat Lovers pizza costs $12 and Vegetarian costs $10 and there were no charges for changes, how much money has Pizza Runner made so far if there are no delivery fees?\n\n-- Creating a view of cleaned data\nCREATE OR REPLACE VIEW clean_runner_orders AS\nselect order_id,\n    runner_id,\n    CASE\n        WHEN pickup_time = 'null' THEN NULL\n        ELSE pickup_time::TIMESTAMP\n    END,\n    cast(substring(distance, '[0-9\\-+\\.]+') as float) as distance,\n    cast(substring(duration, '[0-9\\-+\\.]+') as float) as duration,\n    nullif(cancellation, '') || nullif(cancellation, 'null') as cancellation\nfrom runner_orders;\n--\n\n\nwith cte1 as (\n    select co.pizza_id,\n        count(co.pizza_id) as quantity_sold\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n    GROUP BY 1\n)\nselect pizza_id,\n    quantity_sold * price as revenue\nfrom (\n        select *,\n            CASE\n                WHEN cte1.pizza_id = 1 THEN 12\n                WHEN cte1.pizza_id = 2 THEN 10\n            END AS price\n        from cte1\n    ) sq\n\n\n2 records\n\n\npizza_id\nrevenue\n\n\n\n\n1\n108\n\n\n2\n30\n\n\n\n\n\n\n-- Creating a random integer generator\ncreate or replace function random_between(low int, high int) returns int as $$ begin return floor(random() * (high - low + 1) + low);\nend;\n$$ language 'plpgsql' STRICT;\n\n\n\n2. If a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre travelled, how much money does Pizza Runner have left over after these deliveries?\n\nwith cte as (\n    select co.pizza_id,\n        ro.distance * 0.3 as runner_cost,\n        CASE\n            WHEN co.pizza_id = 1 THEN 12\n            WHEN co.pizza_id = 2 THEN 10\n        END AS price\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n)\nselect sum(price) revenue,\n    round(sum(runner_cost)::numeric, 2) cost,\n    round(sum(price) - sum(runner_cost)::numeric, 2) profit\nfrom cte\n\n\n1 records\n\n\nrevenue\ncost\nprofit\n\n\n\n\n138\n64.62\n73.38"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#closing-the-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#closing-the-connection",
    "title": "Pizza Runner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#introduction",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#introduction",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "title": "Danny’s Diner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram\n\n\nSales\nThe sales table captures all customer_id level purchases with an corresponding order_date and product_id information for when and what menu items were ordered.\n\n\nMenu\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\n\n\nMembers\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\nkey concepts: CTEs, window functions"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#database-connection",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#database-connection",
    "title": "Danny’s Diner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"danny_diners\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#queries",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#queries",
    "title": "Danny’s Diner",
    "section": "Queries",
    "text": "Queries\nNow let’s convert business questions into SQL queries!\n\n1. What is the total amount each customer spent at the restaurant?\n\n\nselect s.customer_id as customer,\n   sum(m.price) as total_amount\nfrom sales s\n   inner join menu m on s.product_id = m.product_id\ngroup by customer;\n\n\n3 records\n\n\ncustomer\ntotal_amount\n\n\n\n\nB\n74\n\n\nC\n36\n\n\nA\n76\n\n\n\n\n\n\n\n2. How many days has each customer visited the restaurant?\n\n\nselect customer_id,\n   count(\n      distinct extract(\n         day\n         from order_date\n      )\n   ) as no_of_days_visited\nfrom sales\ngroup by customer_id\norder by customer_id;\n\n\n3 records\n\n\ncustomer_id\nno_of_days_visited\n\n\n\n\nA\n4\n\n\nB\n5\n\n\nC\n2\n\n\n\n\n\nThis SQL query calculates the number of days each customer has visited a restaurant by grouping the sales transactions by customer ID, extracting the day from the order date, counting the distinct days, and ordering the results by customer ID.\n\n\n3. What was the first item from the menu purchased by each customer?\n\n\nwith cte as (\n   select customer_id,\n      order_date,\n      row_number() over (\n         partition by customer_id\n         order by order_date\n      ) as order_rank,\n      product_id\n   from sales\n)\nselect c.customer_id,\n   c.order_date,\n   c.order_rank,\n   m.product_name\nfrom cte c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\norder_date\norder_rank\nproduct_name\n\n\n\n\nA\n2021-01-01\n1\nsushi\n\n\nB\n2021-01-01\n1\ncurry\n\n\nC\n2021-01-01\n1\nramen\n\n\n\n\n\nWithin the CTE, the row_number() function is employed to assign a ranking to each order for each customer based on the order_date. The ranking starts from 1, indicating the first order for each customer. The main query then selects the customer ID, order date, order rank, and product name from the CTE, joining the menu table to retrieve the product name corresponding to the product ID. Finally, it filters the results to include only records with an order rank of 1, effectively selecting the first item purchased for each customer.\n\n\n4. What is the most purchased item on the menu and how many times was it purchased by all customers?\n\n\nwith cte as(\n   select product_id,\n      count(product_id)\n   from sales\n   group by product_id\n   order by count(product_id) desc\n   limit 1\n) -- Most purchased item on the menu is the product with the id 3 which is ramen, according to cte\nselect customer_id,\n   count(product_id) as count_of_most_purchased_product\nfrom sales\nwhere product_id in (\n      select product_id\n      from cte\n   )\ngroup by customer_id;\n\n\n3 records\n\n\ncustomer_id\ncount_of_most_purchased_product\n\n\n\n\nA\n3\n\n\nB\n2\n\n\nC\n3\n\n\n\n\n\nThe query utilizes a common table expression (CTE) named cte to generate a temporary result set. Within the CTE, the count() function is employed to count the number of times each product ID appears in the sales table. The results are then sorted in descending order based on the product count. The limit 1 clause restricts the output to the top row, effectively identifying the product ID with the highest count.\nThe main query then selects the customer ID and the count of the most purchased product for each customer. It filters the sales table to include only records where the product_id matches the one identified in the CTE, effectively focusing on the most purchased item. Finally, it groups the results by customer_id to determine how many times each customer purchased the most popular item.\n\n\n5. Which item was the most popular for each customer?\n\n\nwith cte_1 as (\n   select customer_id,\n      product_id,\n      count(product_id) as count_of_item\n   from sales\n   group by customer_id,\n      product_id\n),\ncte_2 as (\n   select *,\n      row_number() over (\n         partition by customer_id\n         order by count_of_item desc\n      ) as order_rank\n   from cte_1\n)\nselect c.customer_id,\n   m.product_name,\n   c.count_of_item\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\nproduct_name\ncount_of_item\n\n\n\n\nC\nramen\n3\n\n\nB\nramen\n2\n\n\nA\nramen\n3\n\n\n\n\n\nThis query aims to identify the most popular item for each customer. The query utilizes two common table expressions to process the data and generate the desired output. The first CTE, named cte_1, calculates the count of each product purchased by each customer. It groups the rows in the sales table by customer_id and product_id, and then counts the occurrences of each product ID for each customer. This step determines the frequency of each product purchase for each customer.\nThe second CTE, named cte_2, assigns a ranking to each product for each customer based on the purchase frequency calculated in cte_1. It uses the row_number() function and partitions the data by customer_id, sorting within each partition by the count_of_item in descending order. This step effectively identifies the product with the highest purchase frequency (i.e., the most popular item) for each customer.\nThe main query then selects the customer ID, product name, and purchase count for each customer’s most popular item. It joins the menu table to obtain the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the most popular item for each customer is selected.\n\n\n6. Which item was purchased first by the customer after they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date\n      ) as order_rank\n   from cte_1\n   where order_date &gt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nramen\n2021-01-10\n1\n\n\nB\nsushi\n2021-01-11\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date, ensuring a chronological order of transactions. The second CTE, named cte_2, focuses on purchases made after each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is later than the join_date (membership start date).\nAdditionally, it assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date. This identifies the first purchase (order_rank = 1) made by each customer after becoming a member. The main query then selects the customer ID, product name, order date, and order rank for each customer’s first purchase after becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the first purchase is selected.\n\n\n7. Which item was purchased just before the customer became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      join_date,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date desc\n      ) as order_rank\n   from cte_1\n   where order_date &lt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nsushi\n2021-01-01\n1\n\n\nB\nsushi\n2021-01-04\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date in ascending order, ensuring a chronological sequence of transactions.\nThe second CTE, cte_2, focuses on purchases made before each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date).\nIt assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date in descending order. This identifies the last purchase (order_rank = 1) made by each customer before becoming a member.\nThe main query then selects the customer ID, product name, order date, and order rank for each customer’s last purchase before becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the last purchase is selected.\n\n\n8. What is the total items and amount spent for each member before they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n      natural join menu\n   order by order_date\n)\nselect customer_id,\n   count(distinct product_id) as count_of_products,\n   sum(price) as total_amount_spent\nfrom cte_1\nwhere order_date &lt; join_date\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ncount_of_products\ntotal_amount_spent\n\n\n\n\nA\n2\n25\n\n\nB\n2\n40\n\n\n\n\n\nThe query utilizes a CTE named cte_1 to prepare the data and simplifies the aggregation in the main query.\nThe main query then aggregates the data for each customer based on their membership status. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date). For each customer, it counts the distinct product_id values to determine the total number of unique items purchased and calculates the sum of price values to determine the total amount spent. The results are grouped by customer_id to provide individual summaries for each member.\n\n\n9. If each $1 spent equates to 10 points and sushi has a 2x points multiplier - how many points would each customer have?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points\n   from members\n      natural join sales\n      natural join menu\n)\nselect customer_id,\n   sum(points) as total_points\nfrom cte\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n860\n\n\nB\n940\n\n\n\n\n\nThe CTE combines the members, sales, and menu tables, providing a comprehensive view of customer memberships, their purchases, and the corresponding product names.\nThe main query then summarizes the points earned for each customer. It groups the data from the CTE by customer_id and calculates the sum of points values for each group, effectively determining the total points earned by each customer.\n\n\n10. In the first week after a customer joins the program (including their join date) they earn 2x points on all items, – not just sushi - how many points do customer A and B have at the end of January?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points,\n      case\n         when order_date - join_date &lt;= 7 then 2\n         else 1\n      end multiplier\n   from members\n      natural join sales\n      natural join menu\n),\ncte_2 as (\n   select *,\n      points * multiplier as total_points\n   from cte\n)\nselect customer_id,\n   sum(total_points) as total_points\nfrom cte_2\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n1720\n\n\nB\n1760\n\n\n\n\n\nThe first CTE, named cte, joins the members, sales, and menu tables, and calculates the points earned for each purchase using a conditional CASE expression, similar to the previous query.\nAdditionally, it assigns a multiplier to each purchase based on whether it falls within the first week after the customer’s join date. For purchases within the first week, the multiplier is 2 (double points); for purchases outside the first week, the multiplier is 1 (standard points).\nThe second CTE, named cte_2, simplifies the calculation by multiplying the points and multiplier columns for each purchase, effectively determining the total points earned per transaction. The main query then summarizes the points earned for each customer, including the double points accrued during the first week. It groups the data from cte_2 by customer_id and calculates the sum of total_points values for each group, providing the total points earned by customer A and customer B at the end of January\n\n\n11. Recreate the following table output using the available data:\n\n\nselect s.customer_id,\n    s.order_date,\n    men.product_name,\n    men.price,\n    CASE\n        WHEN s.order_date &gt;= m.join_date THEN 'Y'\n        ELSE 'N'\n    END\nfrom sales s\n    LEFT JOIN menu men ON s.product_id = men.product_id\n    LEFT JOIN members m on m.customer_id = s.customer_id\nORDER BY s.customer_id,\n    s.order_date;\n\n\nDisplaying records 1 - 10\n\n\ncustomer_id\norder_date\nproduct_name\nprice\ncase\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#closing-the-connection",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#closing-the-connection",
    "title": "Danny’s Diner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html",
    "href": "posts/2023-11-18_sleep-disorders/index.html",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "",
    "text": "The goal is to build a casual model using linear regression that explains quality of sleep, and logistic regression to explain sleep disorders\n\nlibrary(tidyverse)\n\n# descriptives\nlibrary(datawizard)\nlibrary(kableExtra)\nlibrary(skimr)\nlibrary(qqplotr)\nlibrary(gt)\n\n# os\nlibrary(here)"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#data-cleaning",
    "href": "posts/2023-11-18_sleep-disorders/index.html#data-cleaning",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Data cleaning",
    "text": "Data cleaning\nI observed the following issues in the data\n\nOccupation variable having levels with very few counts. Those with few counts can be consolidated with similar categories.\nThe BMI category variable has both “Normal” and “Normal Weight” as levels. This is most likely as data entry error.\nThe heart rate variable is not in a suitable format to work with. I’ll extract the systolic blood pressure from the variable as an integer. I would not need the diastolic blood pressure as I observed strong multicollinearity between both diastolic and systolic blood pressure.\n\n\n# observe occupations with low frequency counts\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# observe data entry error in bmi_category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# regrouping occupation variable so that occupations with lower counts are consolidated with other similar occupations. Simultaneously correcting data entry error in bmi_category variable\nsleep &lt;- sleep %&gt;% mutate( \n  occupation=ifelse(sleep$occupation %in% c(\"Software Engineer\", \"Scientist\", \"Engineer\"),\"Technical\", \nifelse(sleep$occupation %in% c(\"Salesperson\", \"Sales Representative\", \"Manager\"), \"Sales\", ifelse(sleep$occupation %in% c(\"Doctor\", \"Nurse\"), \"Medical\",\n                                                          as.character(occupation)))),\nbmi_category = ifelse(sleep$bmi_category %in% c(\"Normal\", \"Normal Weight\"), \"Normal\", as.character(sleep$bmi_category)))\n\n\n# extracting systolic blood pressure from the blood_pressure variable as it such a numeric variable is easier to work with. Subsequently dropping hear_rate variable\n\nsleep &lt;- sleep %&gt;% mutate(\nbp_sys= parse_number(str_sub(sleep$blood_pressure, 1, 3)),\nperson_id = as.character(person_id)\n) %&gt;% select(!heart_rate)\n\n\n# confirming changes\nhead(sleep)  %&gt;% gt() %&gt;% opt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      person_id\n      gender\n      age\n      occupation\n      sleep_duration\n      quality_of_sleep\n      physical_activity_level\n      stress_level\n      bmi_category\n      blood_pressure\n      daily_steps\n      sleep_disorder\n      bp_sys\n    \n  \n  \n    1\nMale\n27\nTechnical\n6.1\n6\n42\n6\nOverweight\n126/83\n4200\nNone\n126\n    2\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    3\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    4\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    5\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    6\nMale\n28\nTechnical\n5.9\n4\n30\n8\nObese\n140/90\n3000\nInsomnia\n140\n  \n  \n  \n\n\n\n\n\n# confirming changes to BMI category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# confirming changes to occupation variable\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#associations",
    "href": "posts/2023-11-18_sleep-disorders/index.html#associations",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Associations",
    "text": "Associations\n\nResearch Question: “What strong associations exists between quality_of_sleep and other variables in the dataset?\n\n# | warning: false\n# | message: false\n# packages required for associations\nlibrary(correlation)\nlibrary(GGally)\n\n\n# plotting the grid plot of scatterplots of numeric variables\nggscatmat(sleep %&gt;% select_if(is.numeric)) + theme_light()\n\n\n\n\n\n# showing relationships with strong correlations (greater than |+-0.4|)\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation() %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs)) %&gt;% filter(abs(r) &gt; 0.4) %&gt;% arrange(r) \ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.899\n0.000\n    sleep_duration\nstress_level\n−0.811\n0.000\n    age\nstress_level\n−0.422\n0.000\n    age\nquality_of_sleep\n0.474\n0.000\n    age\nbp_sys\n0.606\n0.000\n    physical_activity_level\ndaily_steps\n0.773\n0.000\n    sleep_duration\nquality_of_sleep\n0.883\n0.000\n  \n  \n  \n\n\n\n\n\n# robust correlation analysis since the data are skewed. Results is filtered to include only relationships involving quality_of_sleep. Sorted ascendingly by p-value\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation(method=\"percentage\") %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs))  %&gt;% arrange(p)  %&gt;% filter( Parameter1 == \"quality_of_sleep\" | Parameter2 == \"quality_of_sleep\") %&gt;% arrange(p) %&gt;% filter(p &lt; 0.05)\ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.910\n0.000\n    sleep_duration\nquality_of_sleep\n0.892\n0.000\n    age\nquality_of_sleep\n0.432\n0.000\n    quality_of_sleep\nphysical_activity_level\n0.178\n0.005\n  \n  \n  \n\n\n\n\n\nResults\nThe most significant relationships (at alpha = .05) are the relationships between:\n\nquality of sleep and stress level, r(372) =-0.91, p &lt; 0.0001\nquality of sleep and sleep duration, r(372) = 0.89, p &lt; 0.0001\nquality of sleep and age, r(372) =0.43, p &lt; 0.0001\nquality of sleep and physical activity level, r(372) =0.18, p = 0.0054\n\nThese are the variables we would fit to our linear regression model."
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#research-questions",
    "href": "posts/2023-11-18_sleep-disorders/index.html#research-questions",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research questions:",
    "text": "Research questions:\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\nIs the relationship between stress level and quality of sleep moderated by occupation?\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\n\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\n\n# Visualizing the relationship of the three variables\nggplot(sleep, aes(x = sleep_duration, y = quality_of_sleep, color = sleep_disorder)) +\ngeom_jitter(size=1, alpha=0.8) +\nlabs(x = \"Sleep duration\", y = \"Quality of sleep\", color = \"Sleep disorder\") +\ngeom_smooth(method = \"lm\", se = FALSE, linewidth=0.9) + theme_bw()\n\n\n\n\nThe plot shows that there might be some moderation effects.\n\n# making sleep_disorder a factor variable and setting the base level to \"None\"\nsleep &lt;- sleep %&gt;% mutate(\n  sleep_disorder = as_factor(sleep_disorder) %&gt;% fct_relevel(\"None\")\n)\n\n\nqs_lm_03 &lt;- lm(quality_of_sleep ~ sleep_duration * sleep_disorder, data=sleep)\nqs_lm_03 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-0.5750673\n0.36703899\n-1.566774\n1.180269e-01\n    sleep_duration\n1.1144522\n0.04963586\n22.452560\n6.124844e-71\n    sleep_disorderSleep Apnea\n-3.4341228\n0.57712441\n-5.950403\n6.233752e-09\n    sleep_disorderInsomnia\n-3.2040380\n1.11193745\n-2.881491\n4.190005e-03\n    sleep_duration:sleep_disorderSleep Apnea\n0.4802913\n0.08000178\n6.003508\n4.632039e-09\n    sleep_duration:sleep_disorderInsomnia\n0.4503706\n0.16657925\n2.703642\n7.176641e-03\n  \n  \n  \n\n\n\n\n\nInterpretation\n\n\n\n\n\n\nTip\n\n\n\nThe effect of increasing sleep duration in someone with no sleep disorder on sleep quality is 1.11\nThe effect of sleep apnea on sleep quality compared to someone with no disorders is -3.43\nThe effect of Insomnia on sleep quality compared to someone with no disorders is -3.20\nThe effect of increasing sleep duration in someone with sleep anea on sleep quality compared to someone with no disorders is 0.48\nThe effect of increasing sleep duration in someone with insomnia on sleep quality compared to someone with no disorders is 0.45\n\n\n\ninteractions::sim_slopes(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of sleep_duration when sleep_disorder = Insomnia: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.56   0.15   1.26    1.86    10.26   0.00\n\nSlope of sleep_duration when sleep_disorder = Sleep Apnea: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.59   0.06   1.48    1.71    27.35   0.00\n\nSlope of sleep_duration when sleep_disorder = None: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.11   0.03   1.05    1.17    36.50   0.00\n\n\nThe slopes for the three regressions are not significantly different\n\ninteractions::interact_plot(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Sleep disorder\"\n  )\n\n\n\n\nLooking at the slope, we can that the moderation effect is not significant.\n\n\nConclusion\nSleep disorder is not a moderator of the relationship between sleep duration and quality of sleep\n\n\n\nIs the relationship between stress level and quality of sleep moderated by occupation?\n\nqs_lm_04 &lt;- lm(quality_of_sleep ~ stress_level * occupation, data=sleep)\nqs_lm_04 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n9.3661417\n0.22464236\n41.6935685\n2.847324e-140\n    stress_level\n-0.3208661\n0.04725953\n-6.7894487\n4.624546e-11\n    occupationLawyer\n3.3611310\n1.08304354\n3.1034126\n2.063488e-03\n    occupationMedical\n1.3637168\n0.24305144\n5.6108156\n4.008125e-08\n    occupationSales\n3.5481440\n1.02746628\n3.4532948\n6.193572e-04\n    occupationTeacher\n-0.2842770\n0.33263086\n-0.8546322\n3.933200e-01\n    occupationTechnical\n2.0623438\n0.25972133\n7.9406026\n2.559029e-14\n    stress_level:occupationLawyer\n-0.6336793\n0.21426067\n-2.9575158\n3.304916e-03\n    stress_level:occupationMedical\n-0.2851357\n0.04939422\n-5.7726532\n1.679151e-08\n    stress_level:occupationSales\n-0.6791339\n0.15058782\n-4.5098857\n8.775327e-06\n    stress_level:occupationTeacher\n-0.1447393\n0.07087144\n-2.0422799\n4.184795e-02\n    stress_level:occupationTechnical\n-0.4718021\n0.05572917\n-8.4659819\n6.415721e-16\n  \n  \n  \n\n\n\n\n\ninteractions::sim_slopes(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of stress_level when occupation = Lawyer: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.95   0.03   -1.02   -0.89   -29.71   0.00\n\nSlope of stress_level when occupation = Accountant: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.32   0.04   -0.40   -0.24    -7.92   0.00\n\nSlope of stress_level when occupation = Teacher: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.47   0.09   -0.65   -0.28    -5.01   0.00\n\nSlope of stress_level when occupation = Sales: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -1.00   1.04   -3.04    1.04    -0.97   0.33\n\nSlope of stress_level when occupation = Medical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.61   0.01   -0.62   -0.59   -81.54   0.00\n\nSlope of stress_level when occupation = Technical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.79   0.07   -0.93   -0.65   -10.97   0.00\n\n\n\ninteractions::interact_plot(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Occupation\"\n  )\n\n\n\n\nAgain, we do not observe a significant moderation effect of occupation on the relationship between stress level and quality of sleep\n\n\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\nTo test this, we are going to run 3 regressions:\n\nThe total effect of physical_activity_level on quality_of_sleep.\nThe effect of physical_activity_level on quality_of_sleep that is mediated by daily_steps, a.k.a. the indirect effect\nThe effect of physical_activity_level on quality_of_sleep that is not mediated by daily_steps, a.k.a. the direct effect\n\n\nTotal effect\nWe first determine the total effect by running a regression of physical_activity_level on quality_of_sleep (without including daily_steps):\n\nlm(quality_of_sleep ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n6.657\n0.183\n36.307\n0.000\n    physical_activity_level\n0.011\n0.003\n3.792\n0.000\n  \n  \n  \n\n\n\n\nThe total effect is approximately 0.011, meaning that an increase of one unit in physical activity increases sleep quality by 0.001 units on average, not holding daily steps constant.\n\n\nMediated/Indirect Effect\nThe effect of physical_activity_level on quality_of_sleep mediated by daily_steps can be obtained by multiplying together the effect of physical_activity_level on daily_steps and the effect of daily_steps on quality_of_sleep.\n\n1. physical_activity_level on daily_steps (a)\n\nlm(daily_steps ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n3265.57619\n160.321889\n20.36887\n1.718735e-62\n    physical_activity_level\n60.01692\n2.556092\n23.47995\n1.968256e-75\n  \n  \n  \n\n\n\n\nOne unit increase in physical_activity_level increases daily_steps by an average of 60 steps\n\n\n2. daily_steps on quality_of_sleep\n\nlm(quality_of_sleep ~ daily_steps + physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n7.4500815511\n2.609352e-01\n28.551459\n1.151887e-95\n    daily_steps\n-0.0002428667\n5.802065e-05\n-4.185867\n3.551189e-05\n    physical_activity_level\n0.0256601109\n4.506428e-03\n5.694113\n2.524969e-08\n  \n  \n  \n\n\n\n\n(a * b) = 60.07 * -0.00024 = -0.0144\n\n\n\nDirect Effect\nThis is simply the coefficient of physical_activity_level in the above regression = 0.0257\nAccording to Zhao et al., what we have here isCompetitive Mediation (Regularly Partial Mediation)\nIn the competitive partial mediation hypothesis, it is assumed that the intermediate variable (daily_steps) could sometimes increase and at times decrease the relationship between the independent and dependent variables. i.e an “inconsistent” model.\n\n\n\nDecision tree for determining mediation"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "href": "posts/2023-11-18_sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research question: are significant differences in the mean sleep quality of both male and female participants",
    "text": "Research question: are significant differences in the mean sleep quality of both male and female participants\n\n# summary statistics of quality_of_sleep by gender\n\nby(cbind(data=sleep$quality_of_sleep), sleep$gender, psych::describe)\n\nINDICES: Female\n     vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 185 7.66 1.28      8    7.76 1.48   4   9     5 -0.49    -0.83 0.09\n------------------------------------------------------------ \nINDICES: Male\n     vars   n mean sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 189 6.97  1      7    7.01 1.48   4   9     5 -0.35    -0.59 0.07\n\n\nThe mean of female participants is one point higher than males. But is this difference significant?\n\n# plotting a violin-errorbar plot to visualise the relationship\nggplot(sleep, aes(gender,quality_of_sleep))+\ngeom_violin() +\nstat_summary(fun.data=\"mean_cl_normal\") + \ntheme_minimal()\n\n\n\n\nSince the errorbars do not overlap, we can be fairly confident that the difference is significant. Lets confirm this hunch with a Welsh’s t-test.\n\n t.test(quality_of_sleep ~ gender,\n                    data = sleep,\n                    paired = FALSE,\n                    var.equal = FALSE,\n                    conf.level = 0.95,\n                    na.action = na.exclude)\n\n\n    Welch Two Sample t-test\n\ndata:  quality_of_sleep by gender\nt = 5.8593, df = 347.96, p-value = 1.078e-08\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n 0.4627786 0.9304432\nsample estimates:\nmean in group Female   mean in group Male \n            7.664865             6.968254 \n\n\nBecause the p-value = 1.078e-08 is less than our alpha of .05, we can conclude that the difference between both genders is significant. Also, since the confidence interval expressing the true difference in means does not cross zero, we can be confident that there exists a true difference, with a chance of us being wrong 5% of the time.\n\neffectsize::cohens_d(quality_of_sleep ~ gender, data = sleep) %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Cohens_d\n      CI\n      CI_low\n      CI_high\n    \n  \n  \n    0.608\n0.950\n0.400\n0.815\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOn average, female participants have better sleep quality (M = 7.66, SE = 0.09), than those not given a cloak (M = 6.97, SE = 0.07). Sleep quality is significantly different for both genders: the mean difference, M = 0.69, 95% CI [0.46, 0.93], was significantly different from 0, t(347.96) = 5.85, p = 01.078e-08. This effect was quite large, d= 0.61[0.4, 0.82]"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#conclusion-1",
    "href": "posts/2023-11-18_sleep-disorders/index.html#conclusion-1",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Conclusion",
    "text": "Conclusion\nI created this post as a way of solidifying my understanding of these concepts. I find that documenting the material helps it to stick iwth me better. I hope that you find this helpful. My deepest gratitude goes to Andy Field for his exceptional book on statistical analysis with R."
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#introduction",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#introduction",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "title": "Foodie Fi",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#database-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#database-connection",
    "title": "Foodie Fi",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"foodie_fi\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#queries",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#queries",
    "title": "Foodie Fi",
    "section": "Queries",
    "text": "Queries\n\n1. How many customers has Foodie-Fi ever had?\n\n\nSELECT count(DISTINCT customer_id)\nFROM subscriptions;\n\n\n1 records\n\n\ncount\n\n\n\n\n1000\n\n\n\n\n\n\n\n3. What is the monthly distribution of trial plan start_date values for our dataset use the start of the month as the group by value?\n\nSELECT upper(to_char(start_date, 'month')) as start_month,\n    count(*) frequency\nfrom subscriptions\nwhere plan_id = 0\ngroup by 1\nORDER BY 2 desc;\n\n\nDisplaying records 1 - 10\n\n\nstart_month\nfrequency\n\n\n\n\nMARCH\n94\n\n\nJULY\n89\n\n\nAUGUST\n88\n\n\nMAY\n88\n\n\nJANUARY\n88\n\n\nSEPTEMBER\n87\n\n\nDECEMBER\n84\n\n\nAPRIL\n81\n\n\nJUNE\n79\n\n\nOCTOBER\n79\n\n\n\n\n\n\n\n3. What plan start_date values occur after the year 2020 for our dataset? Show the breakdown by count of events for each plan_name ?\n\n\nSELECT p.plan_name,\n    count(*) as count_of_events_after_2020\nfrom subscriptions s\n    natural join plans p\nwhere EXTRACT(\n        year\n        from start_date\n    ) &gt; 2020\ngroup by 1;\n\n\n4 records\n\n\nplan_name\ncount_of_events_after_2020\n\n\n\n\npro annual\n63\n\n\nchurn\n71\n\n\npro monthly\n60\n\n\nbasic monthly\n8\n\n\n\n\n\n\n\n4. What is the customer count and percentage of customers who have churned rounded to 1 decimal place?\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id = 4\n)\nSELECT cte1.whole as total_customers,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_churned\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ntotal_customers\npct_churned\n\n\n\n\n2650\n12\n\n\n\n\n\n\n\n5. How many customers have churned straight after their initial free trial? what percentage is this rounded to the nearest whole number?\n\nwith cte as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\ncte2 as (\n    -- getting rows whose values satisfy the condition in the question\n    select *\n    from cte\n    where plan_id = 0\n        and lead_plan_id = 4\n) -- solution\nselect count(*) as count_of_customers_who_churned_after_free_trial\nfrom cte2 \n\n\n1 records\n\n\ncount_of_customers_who_churned_after_free_trial\n\n\n\n\n92\n\n\n\n\n\n\n\n6. What is the number and percentage of customer plans after their initial free trial?\n\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id &lt;&gt; 0\n)\nSELECT cte2.part as customer_count_after_trial_plan,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_ccatp\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ncustomer_count_after_trial_plan\npct_ccatp\n\n\n\n\n1650\n62\n\n\n\n\n\n\n\n7. How many customers have upgraded to an annual plan in 2020?\n\n\nwith cte1 as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\n-- filtering to only annual plans\ncte2 as (\n    select *,\n        lead_plan_id - plan_id as diff\n    from cte1\n    where lead_plan_id = 3\n) -- excluding churned customers and unupgraded plans\nselect count(DISTINCT customer_id) as upgraded_customers_2020_count\nfrom cte2\nwhere (diff &gt; 0)\n    and (lead_plan_id &lt;&gt; 4)\n    and EXTRACT(\n        year\n        from start_date\n    ) = 2020;\n\n\n1 records\n\n\nupgraded_customers_2020_count\n\n\n\n\n253\n\n\n\n\n\n\n\n8. How many days on average does it take for a customer to upgrade to an annual plan from the – day they join Foodie-Fi?\n\nwith cte1 as (\n    select *,\n        max(plan_id) over (partition by customer_id) as highest_plan_suscribed,\n        max(start_date) over (partition by customer_id) as date_of_hps,\n        min(start_date) over (partition by customer_id) as date_of_lps,\n        row_number() over (partition by customer_id) as sn\n    from subscriptions\n    order by customer_id,\n        start_date,\n        plan_id\n),\ncte2 as(\n    select *,\n        date_of_hps - date_of_lps as diff_in_days\n    from cte1\n    where highest_plan_suscribed = 3\n        and sn = 1\n)\nselect round(avg(diff_in_days)::numeric, 2) as avg_days_to_upgrade_to_annual\nfrom cte2\n\n\n1 records\n\n\navg_days_to_upgrade_to_annual\n\n\n\n\n105.95\n\n\n\n\n\n\n\n9. How many customers downgraded from a pro monthly to a basic monthly plan in 2020?\n\nwith cte1 as(\n        select *,\n            lead(plan_id) over(partition by customer_id) as lead_plan_id\n        from subscriptions\n        order by customer_id,\n            start_date,\n            plan_id\n    ),\n    cte2 as (\n        select customer_id,\n            plan_id,\n            lead_plan_id,\n            start_date\n            from cte1\n        where plan_id = 2\n            and lead_plan_id = 1\n            and EXTRACT(\n                year\n                from start_date\n            ) = 2020\n    )\nselect count(*) as number_of_customers_downgrade_from_prom_basm\nfrom cte2\n\n\n1 records\n\n\nnumber_of_customers_downgrade_from_prom_basm\n\n\n\n\n0"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#closing-the-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#closing-the-connection",
    "title": "Foodie Fi",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts_index.html",
    "href": "posts_index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDeveloping a Modern Data Pipeline — Part 1\n\n\nA 3-part series featuring web-scraping, data pipelines, orchestration and containerisation\n\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a Modern Data Pipeline — Part 2\n\n\nA 3-part series featuring web-scraping, data pipelines, orchestration and containerisation\n\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a Modern Data Pipeline — Part 3\n\n\nA 3-part series featuring web-scraping, data pipelines, orchestration and containerisation\n\n\n\nMar 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPushing the Boundaries of Excel\n\n\nA Stunning Dashboard with Pivot Tables and Charts\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAqua Vista College Database\n\n\nA comprehensive case study in database design\n\n\n\nJan 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Causal-Behavioral Framework for Data Analysis\n\n\nIntroductory part to a series on Florent Buisson’s book, Behavioral Data Analysis with R & Python\n\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPizza Runner\n\n\nSecond part of a series of SQL case studies where I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoodie Fi\n\n\nThird part of a series of SQL case studies\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Bank\n\n\nFourth part of a series of SQL case studies… more CTEs!\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDanny’s Diner\n\n\nIn this first part of a series of SQL case studies, I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships of Variables in Sleep Data\n\n\nIn this post, I use regression, with an emphasis on understanding the casual relationships between variables\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeads Acquisition Analysis\n\n\nAn assessment for a Report Developer role at Toptal\n\n\n\nMar 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising call centre data trends\n\n\n… as part of PwC’s Virtual Internship program\n\n\n\nMar 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject: Analyzing Trends in Gun Sales Data across the U.S.\n\n\nAn project for Udacity’s Nanodegree\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving insights from bike traffic data\n\n\nSecond case study as part of my Udacity Nanodegree\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTweet Data Analysis\n\n\nHardcore python wrangling of Twitter data\n\n\n\nSep 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSales Monitoring Dashboard\n\n\nMy take on the AdventureWorks dataset; featuring database creation, data modeling in Power BI and python scripting\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising COVID-19’s Impact in Nigeria\n\n\nTracking a pandemic with data\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)\n\n\nFeaturing data modeling in Power BI, and Dax\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts_index.html#recent-posts-posts-recentes",
    "href": "posts_index.html#recent-posts-posts-recentes",
    "title": "Chamber of (data) secrets",
    "section": "",
    "text": "1 min\n\n\n\n\n\n\n  \n\n\n\n\n\nSecond part of a series of SQL case studies where I use Postgres SQL to answer a bunch of business questions\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 19, 2023\n\n\nEdun Joshua\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\n\nThird part of a series of SQL case studies\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 19, 2023\n\n\nEdun Joshua\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\nFourth part of a series of SQL case studies… more CTEs!\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 19, 2023\n\n\nEdun Joshua\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\nIn this first part of a series of SQL case studies, I use Postgres SQL to answer a bunch of business questions\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 18, 2023\n\n\nEdun Joshua\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\n\nIn this post, I use regression, with an emphasis on understanding the casual relationships between variables\n\n\n\n\nstats\n\n\ncausal regression modelling\n\n\n \n\n\n\n\nNov 18, 2023\n\n\nEdun Joshua\n\n\n16 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all/Ver todos"
  },
  {
    "objectID": "posts_index.html#recent-posts",
    "href": "posts_index.html#recent-posts",
    "title": "Chamber of (data) secrets",
    "section": "",
    "text": "See all"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html",
    "href": "posts/2023-11-30_BDA-1/index.html",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "",
    "text": "Please Note: The ideas presented in this article are not my own original thoughts. This material is merely a condensed version of my study notes based on Florent Buisson’s excellent book. Due to the need for brevity, certain nuances of these concepts have been omitted. For a more comprehensive understanding, please refer to the original source.\nPredictive analytics is like having a super cool fortune teller 🔮who can predict what’s going to happen next. But causal analytics is like being a detective 🕵️‍♀️🕵️‍♂️, trying to figure out why things happen in the first place 🤔. In predictive analytics, we just want to find a bunch of clues 🕵️‍♀️🕵️‍♂️ that can help us guess what’s going to happen next, even if those clues aren’t actually causing it to happen. But in causal analytics, we’re not just interested in guessing 🔮, we want to understand the real reasons behind things 🧠.\nImagine you want to know if eating more fruits and veggies 🍎🥦 will make you healthier 💪. Predictive analytics might tell you that people who eat more fruits and veggies tend to be healthier, but that doesn’t mean the fruits and veggies are actually causing the better health. It could be that healthier people just tend to make healthier choices overall 🏃‍♀️🚴🥗.\nSo, how do we really know if eating more fruits and veggies is making us healthier? That’s where causal analytics comes in 🕵️‍♀️🕵️‍♂️. We need to carefully choose the clues we use to solve the mystery, making sure they’re not just guessing 🔮but actually helping us understand the real reasons behind things 🧠.\ndf1 &lt;- read_csv(\"chap1-stand_data.csv\")\nhead(df1) %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      icecream_sales\n      iced_coffee_sales\n      summer_months\n      temps\n    \n  \n  \n    25649.78\n28592.18\n0\n28.59314\n    32694.93\n37152.84\n0\n37.13064\n    26467.80\n24074.19\n0\n24.11190\n    43438.79\n49169.47\n0\n49.21807\n    52452.63\n47249.40\n0\n47.25766\n    34130.66\n34228.50\n0\n34.25746\nImagine you’re an ice cream 🍦 seller and you want to know if hotter days ☀️mean more ice cream sales 🤑. You track the temperature and your sales for a while and find that for every degree the temperature rises, your sales go up by $1,171!\nsummary(lm(\"icecream_sales ~ temps\",data=df1))\n\n\nCall:\nlm(formula = \"icecream_sales ~ temps\", data = df1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-30272  -5197   -494   3787  37430 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6169.844    531.506  -11.61   &lt;2e-16 ***\ntemps        1171.335      9.027  129.76   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8730 on 2398 degrees of freedom\nMultiple R-squared:  0.8753,    Adjusted R-squared:  0.8753 \nF-statistic: 1.684e+04 on 1 and 2398 DF,  p-value: &lt; 2.2e-16\nYou’re excited and think you’ve found the perfect formula for success!🤑 But then something strange happens. October comes around and it’s a scorcher 🥵, but your sales actually drop! 😨 What gives?\nAfter some sleuthing, you realize that the culprit is school 🏫. In the summer, kids are out of school and have more time to enjoy ice cream 🍦. But in October, they’re back in class and have less time for frozen treats.\nSo, even though the temperature was high, your sales dropped because of the school year. This is called a “confounder” 😵. A confounder is something that can make it look like something else is causing a (third) something when it’s actually not.\nIn this case, the school year made it look like the temperature was causing sales to drop, when it was really the school year all along 🏫. So, next time you’re trying to figure out what’s causing something, be sure to watch out for confounders! 👀👀\nRemember that time when we added a bunch of random ingredients to our recipe and ended up with a weird-tasting soup? It’s the same with causal modelling.\nTo be fair, if your goal is only to predict a variable, you have a model that is carefully designed to generalize adequately beyond your testing data, and you don’t care about why the predicted variable is taking a certain value, then that’s a perfectly valid stance.\nLet’s demonstrate this with our example by adding a variable that we might be inclined to include but will bias our regression. The variable IcedCoffeeSales is correlated with Temperature but not with SummerMonth. Let’s look at what happens to our regression if we add this variable in addition to Temperature and SummerMonth (a binary 1/0 variable that indicates if the month was July or August (1) or any other month (0)):\nsummary(lm(icecream_sales ~ iced_coffee_sales + temps + summer_months, data=df1))\n\n\nCall:\nlm(formula = icecream_sales ~ iced_coffee_sales + temps + summer_months, \n    data = df1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25763.1  -3364.4     78.6   3317.5  27512.0 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -15.827    374.581  -0.042    0.966    \niced_coffee_sales    -1.701      2.083  -0.817    0.414    \ntemps              2702.788   2083.161   1.297    0.195    \nsummer_months     19548.168    361.572  54.064   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5859 on 2396 degrees of freedom\nMultiple R-squared:  0.9439,    Adjusted R-squared:  0.9438 \nF-statistic: 1.344e+04 on 3 and 2396 DF,  p-value: &lt; 2.2e-16\nOnly summer_months has a statistically significant influence on icecream_sales . The other variable are not good predictors of the target variable and only confound the model.\nImagine you’re running an ice cream stand 🍦🍨 and you want to know if people who like chocolate ice cream 🍫are more likely to buy from your shop than people who like vanilla ice. You ask a bunch of people how much they like chocolate and vanilla ice cream (on a scale of 1 to 10, of course), and whether they’ve ever bought from your shop. You find out that some people like one but not the other, some like both equally, and some like one more than the other.\nChecking the association between our variables we discover 2 things:\nAll good so far! 👍 If we want to understand how both vanilla and chocolate ice cream preferences affect whether someone is a customer or not, then it makes sense to include both variables in our logistic regression model. 📈 But what if we want to know something different? What if we want to know how much the people who like vanilla ice cream also like chocolate ice cream? 🍦🍫 In other words, we’re interested in the relationship between how much someone likes vanilla ice cream and how much they like chocolate ice cream, but only for people who have actually bought from the ice cream stand. 🛍️ To do this, we can plot a graph of how much customers who have bought ice cream like chocolate ice cream, based on how much they like vanilla ice cream.\nHmm, something’s not right! 🤨 When we looked at the relationship between how much vanilla ice cream customers like and how much chocolate ice cream they like, we found a weird pattern: people who like one flavor tend to dislike the other. 🤯 But don’t worry, vanilla lovers aren’t suddenly becoming chocolate haters, and vice versa! 👎 This strange correlation was actually caused by the way we looked at the data.\nWe only looked at people who had bought ice cream from the stand, and this left out a lot of people who might have liked both flavours but didn’t buy any ice cream. If someone has a weak taste for both vanilla and chocolate ice creams, they are most likely not shoppers. if someone has a strong taste for vanilla, they might shop at your stand even if they don’t have a strong taste for chocolate. However, since you excluded non-shoppers, you removed a bunch of data points that should exist in the lower left quadrant of the scatterplot. If they were included, a positive relationship would be observed.\nSo, next time you’re looking at data, be sure to consider all of the data points, not just the ones that fit your expectations.😉\nUp next, chapter 2!"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-30_BDA-1/index.html#entity-relationship-diagram",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#causal-analytics-unraveling-the-mystery-behind-why-things-happen",
    "href": "posts/2023-11-30_BDA-1/index.html#causal-analytics-unraveling-the-mystery-behind-why-things-happen",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Causal Analytics: 🕵️‍♂️Unraveling the Mystery Behind Why Things Happen",
    "text": "Causal Analytics: 🕵️‍♂️Unraveling the Mystery Behind Why Things Happen"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#data",
    "href": "posts/2023-11-30_BDA-1/index.html#data",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "href": "posts/2023-11-30_BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Why Correlation Is Not Causation: A Confounder in Action",
    "text": "Why Correlation Is Not Causation: A Confounder in Action"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#too-many-variables-can-spoil-the-broth",
    "href": "posts/2023-11-30_BDA-1/index.html#too-many-variables-can-spoil-the-broth",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Too Many Variables Can Spoil the Broth",
    "text": "Too Many Variables Can Spoil the Broth"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#another-example",
    "href": "posts/2023-11-30_BDA-1/index.html#another-example",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Another example",
    "text": "Another example"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html",
    "href": "posts/2022_05_28_UK-accidents/index.html",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "",
    "text": "&lt;/div&gt;\n&lt;/section&gt;\n&lt;section id=\"requirement-gathering\" class=\"level2\"&gt;\n&lt;h2&gt;Requirement Gathering&lt;/h2&gt;\n&lt;p&gt;Client wants to create a dashboard on road accidents for the year 2021 and 2022.&lt;/p&gt;\n&lt;section id=\"a.-primary-kpis\" class=\"level4\"&gt;\n&lt;h4&gt;a. Primary KPIs&lt;/h4&gt;\n&lt;ul&gt;\n&lt;li&gt;Total Casualties and Total Accident Values:\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;li&gt;Year-over-Year (YoY) Growth&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Total Casualties by Accident Severity:\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;li&gt;Year-over-Year (YoY) Growth&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"b.-secondary-kpis\" class=\"level4\"&gt;\n&lt;h4&gt;b. Secondary KPIs&lt;/h4&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Total Casualties with Respect to Vehicle Type:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Monthly Trend Comparison of Casualties:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year vs. Previous Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Casualties by Road Type:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Current Year Casualties by Area/Location &amp; Day/Night&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Total Casualties and Total Accidents by Location Total casualties and total accidents by location&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n&lt;section id=\"identifying-stakeholders\" class=\"level2\"&gt;\n&lt;h2&gt;Identifying Stakeholders 🧑🏽‍💼&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Emergency Services Departments&lt;/li&gt;\n&lt;li&gt;Road Safety Corps&lt;/li&gt;\n&lt;li&gt;Traffic Management Agencies&lt;/li&gt;\n&lt;li&gt;Police Force&lt;/li&gt;\n&lt;li&gt;General public&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"understanding-raw-data\" class=\"level2\"&gt;\n&lt;h2&gt;Understanding Raw Data 🥩&lt;/h2&gt;\n&lt;p&gt;Data Grain: A single instance of a reported accident event in the UK in 2021 and 2022&lt;/p&gt;\n&lt;table&gt;\n&lt;colgroup&gt;\n&lt;col style=\"width: 31%\" /&gt;\n&lt;col style=\"width: 68%\" /&gt;\n&lt;/colgroup&gt;\n&lt;thead&gt;\n&lt;tr class=\"header\"&gt;\n&lt;th&gt;Fields&lt;/th&gt;\n&lt;th&gt;description (datatype)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Accident_Index&lt;/td&gt;\n&lt;td&gt;unique row identifier (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Accident Date&lt;/td&gt;\n&lt;td&gt;date of accident event (date)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Day_of_Week&lt;/td&gt;\n&lt;td&gt;day of the week (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Junction_Control&lt;/td&gt;\n&lt;td&gt;mechanism of traffic control at junction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Junction_Detail&lt;/td&gt;\n&lt;td&gt;type of junction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Accident_Severity&lt;/td&gt;\n&lt;td&gt;severity of accident (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Latitude&lt;/td&gt;\n&lt;td&gt;latitude (float)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Light_Conditions&lt;/td&gt;\n&lt;td&gt;light conditions at accident scene (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Local_Authority_(District)&lt;/td&gt;\n&lt;td&gt;name of district (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Carriageway_Hazards&lt;/td&gt;\n&lt;td&gt;hazards on the road if any&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Number_of_Casualties&lt;/td&gt;\n&lt;td&gt;number of causalities (integer)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Number_of_Vehicles&lt;/td&gt;\n&lt;td&gt;number of vehicles involved in the accident (integer)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Police_Force&lt;/td&gt;\n&lt;td&gt;Police force jurisdiction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Road_Surface_Conditions&lt;/td&gt;\n&lt;td&gt;condition of road surface (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Road_Type&lt;/td&gt;\n&lt;td&gt;road type (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Speed_limit&lt;/td&gt;\n&lt;td&gt;road speed limit in mph (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Time&lt;/td&gt;\n&lt;td&gt;time of day accident occurred (time)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Urban_or_Rural_Area&lt;/td&gt;\n&lt;td&gt;Whether accident occurred in an urban or rural area (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Weather_Conditions&lt;/td&gt;\n&lt;td&gt;weather condition at time of accident (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Vehicle_Type&lt;/td&gt;\n&lt;td&gt;type of vehicle involved in the accident&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n&lt;/section&gt;\n&lt;section id=\"data-cleansing\" class=\"level2\"&gt;\n&lt;h2&gt;Data cleansing 🧹&lt;/h2&gt;\n&lt;p&gt;Issues - Junction_Control column has, among others, two values “Auto traffic sigl” and “Auto traffic signal” of which the former is a misspelling of the latter&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Instances where “Fatal” is misspelled as “Fetal” in Accident_Severity column&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;“Time” column was represented as a datetime data type in Power BI&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;Fixes - Used the Replace Values feature to correct the misspellings of Issues 1 &amp; 2 - Changed the data type to time accordingly - All fixes were done in Power Query.&lt;/p&gt;\n&lt;section id=\"data-processing\" class=\"level3\"&gt;\n&lt;h3&gt;Data Processing ⚙️&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;The data needs a calendar table so as to use Time Intelligence functions further down the line.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Using the CALENDAR function, a new table with calculated, dynamic columns of Date, Month and Year was generated.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"data-modeling\" class=\"level3\"&gt;\n&lt;h3&gt;Data Modeling 🏛&lt;/h3&gt;\n&lt;p&gt;a one-to-many active relationship was established between the calendar table and the data table &lt;img src=\"data-model.png\" class=\"img-fluid\" alt=\"Schema\" /&gt;&lt;/p&gt;\n&lt;/section&gt;\n&lt;section id=\"data-visualization\" class=\"level3\"&gt;\n&lt;h3&gt;Data Visualization 🎨&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;Power BI magic!✨ The report pbix file is available in this repo to explore design decisions The following DAX measures were used:&lt;/li&gt;\n&lt;/ul&gt;\n&lt;pre&gt;&lt;code&gt;Count of Accidents = DISTINCTCOUNT(Data[Accident_Index])\n\nCurrent Year Accident Count = TOTALYTD(COUNT(Data[Accident_Index]),&#39;Calendar&#39;[Date])\n\nCurrent Year Casualties = TOTALYTD(SUM(Data[Number_of_Casualties]),&#39;Calendar&#39;[Date])\n\nPrevious Year Accident Counts = CALCULATE(COUNT(Data[Accident_Index]), SAMEPERIODLASTYEAR(&#39;Calendar&#39;[Date]))\n\nPrevious Year Casualties = CALCULATE(SUM(Data[Number_of_Casualties]),\nSAMEPERIODLASTYEAR(&#39;Calendar&#39;[Date]))\n\nYoY Accident Count = DIVIDE([Current Year Accident Count] - [Previous Year Accident Counts], [Previous Year Accident Counts])\n\nYoY Casualties = (DIVIDE([Current Year Casualties] - [Previous Year Casualties],[Previous Year Casualties]))&lt;/code&gt;&lt;/pre&gt;\n&lt;/section&gt;\n&lt;section id=\"deriving-insights\" class=\"level3\"&gt;\n&lt;h3&gt;Deriving Insights&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;The vehicle type most involved in accidents was cars. This can be explained by the fact that most vehicles plying UK roads are cars&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There is a general decrease in casualty counts in 2022 compared to 2021&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n&lt;section id=\"contact\" class=\"level2\"&gt;\n&lt;h2&gt;Contact&lt;/h2&gt;\n&lt;p&gt;Edun Joshua Olubori - &lt;a href=\"https://www.linkedin.com/in/joshua-edun\"&gt;connect on linkedin&lt;/a&gt; - joshuaolubori@gmail.com&lt;/p&gt;\n&lt;p&gt;Live Demo: &lt;a href=\"https://app.powerbi.com/view?r=eyJrIjoiYjZiNDYwMTctY2EwOC00NGQ2LWJmOGEtMzVlN2I2YTFlZTEwIiwidCI6ImRmODY3OWNkLWE4MGUtNDVkOC05OWFjLWM4M2VkN2ZmOTVhMCJ9&amp;embedImagePlaceholder=true\"&gt;here&lt;/a&gt;&lt;/p&gt;\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Home\"&gt;Home&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.html\"&gt;/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Posts\"&gt;Posts&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/posts_index.html\"&gt;/posts_index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"footer-left\"&gt;Blog made with 💚 and &lt;a href=\"https://quarto.org/\"&gt;Quarto&lt;/a&gt;, by Edun Joshua. License: &lt;a href=\"https://creativecommons.org/licenses/by-sa/2.0/\"&gt;CC BY-SA 2.0&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;Featuring data modeling in Power BI, and Dax&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;Featuring data modeling in Power BI, and Dax&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;script src=\"https://giscus.app/client.js\"\n        data-repo=\"JoshuaOlubori/qblog\"\n        data-repo-id=\"R_kgDOKvhXew\"\n        data-category=\"General\"\n        data-category-id=\"DIC_kwDOKvhXe84CbFeH\"\n        data-mapping=\"title\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"top\"\n        data-theme=\"light\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async&gt;\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n&lt;footer class=\"footer\"&gt;\n  &lt;div class=\"nav-footer\"&gt;\n    &lt;div class=\"nav-footer-left\"&gt;\n      &lt;div class='footer-contents'&gt;Blog made with 💚 and [Quarto](https://quarto.org/),  by Edun Joshua.  License: [CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/).&lt;/div&gt;  \n    &lt;/div&gt;   \n    &lt;div class=\"nav-footer-center\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n    &lt;div class=\"nav-footer-right\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#about-the-project",
    "href": "posts/2022_05_28_UK-accidents/index.html#about-the-project",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "",
    "text": "&lt;/div&gt;\n&lt;/section&gt;\n&lt;section id=\"requirement-gathering\" class=\"level2\"&gt;\n&lt;h2&gt;Requirement Gathering&lt;/h2&gt;\n&lt;p&gt;Client wants to create a dashboard on road accidents for the year 2021 and 2022.&lt;/p&gt;\n&lt;section id=\"a.-primary-kpis\" class=\"level4\"&gt;\n&lt;h4&gt;a. Primary KPIs&lt;/h4&gt;\n&lt;ul&gt;\n&lt;li&gt;Total Casualties and Total Accident Values:\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;li&gt;Year-over-Year (YoY) Growth&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Total Casualties by Accident Severity:\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;li&gt;Year-over-Year (YoY) Growth&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"b.-secondary-kpis\" class=\"level4\"&gt;\n&lt;h4&gt;b. Secondary KPIs&lt;/h4&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Total Casualties with Respect to Vehicle Type:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Monthly Trend Comparison of Casualties:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year vs. Previous Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Casualties by Road Type:&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;Current Year&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Current Year Casualties by Area/Location &amp; Day/Night&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Total Casualties and Total Accidents by Location Total casualties and total accidents by location&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n&lt;section id=\"identifying-stakeholders\" class=\"level2\"&gt;\n&lt;h2&gt;Identifying Stakeholders 🧑🏽‍💼&lt;/h2&gt;\n&lt;ul&gt;\n&lt;li&gt;Emergency Services Departments&lt;/li&gt;\n&lt;li&gt;Road Safety Corps&lt;/li&gt;\n&lt;li&gt;Traffic Management Agencies&lt;/li&gt;\n&lt;li&gt;Police Force&lt;/li&gt;\n&lt;li&gt;General public&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"understanding-raw-data\" class=\"level2\"&gt;\n&lt;h2&gt;Understanding Raw Data 🥩&lt;/h2&gt;\n&lt;p&gt;Data Grain: A single instance of a reported accident event in the UK in 2021 and 2022&lt;/p&gt;\n&lt;table&gt;\n&lt;colgroup&gt;\n&lt;col style=\"width: 31%\" /&gt;\n&lt;col style=\"width: 68%\" /&gt;\n&lt;/colgroup&gt;\n&lt;thead&gt;\n&lt;tr class=\"header\"&gt;\n&lt;th&gt;Fields&lt;/th&gt;\n&lt;th&gt;description (datatype)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Accident_Index&lt;/td&gt;\n&lt;td&gt;unique row identifier (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Accident Date&lt;/td&gt;\n&lt;td&gt;date of accident event (date)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Day_of_Week&lt;/td&gt;\n&lt;td&gt;day of the week (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Junction_Control&lt;/td&gt;\n&lt;td&gt;mechanism of traffic control at junction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Junction_Detail&lt;/td&gt;\n&lt;td&gt;type of junction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Accident_Severity&lt;/td&gt;\n&lt;td&gt;severity of accident (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Latitude&lt;/td&gt;\n&lt;td&gt;latitude (float)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Light_Conditions&lt;/td&gt;\n&lt;td&gt;light conditions at accident scene (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Local_Authority_(District)&lt;/td&gt;\n&lt;td&gt;name of district (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Carriageway_Hazards&lt;/td&gt;\n&lt;td&gt;hazards on the road if any&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Number_of_Casualties&lt;/td&gt;\n&lt;td&gt;number of causalities (integer)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Number_of_Vehicles&lt;/td&gt;\n&lt;td&gt;number of vehicles involved in the accident (integer)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Police_Force&lt;/td&gt;\n&lt;td&gt;Police force jurisdiction (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Road_Surface_Conditions&lt;/td&gt;\n&lt;td&gt;condition of road surface (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Road_Type&lt;/td&gt;\n&lt;td&gt;road type (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Speed_limit&lt;/td&gt;\n&lt;td&gt;road speed limit in mph (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Time&lt;/td&gt;\n&lt;td&gt;time of day accident occurred (time)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Urban_or_Rural_Area&lt;/td&gt;\n&lt;td&gt;Whether accident occurred in an urban or rural area (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td&gt;Weather_Conditions&lt;/td&gt;\n&lt;td&gt;weather condition at time of accident (string)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td&gt;Vehicle_Type&lt;/td&gt;\n&lt;td&gt;type of vehicle involved in the accident&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n&lt;/section&gt;\n&lt;section id=\"data-cleansing\" class=\"level2\"&gt;\n&lt;h2&gt;Data cleansing 🧹&lt;/h2&gt;\n&lt;p&gt;Issues - Junction_Control column has, among others, two values “Auto traffic sigl” and “Auto traffic signal” of which the former is a misspelling of the latter&lt;/p&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Instances where “Fatal” is misspelled as “Fetal” in Accident_Severity column&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;“Time” column was represented as a datetime data type in Power BI&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;p&gt;Fixes - Used the Replace Values feature to correct the misspellings of Issues 1 &amp; 2 - Changed the data type to time accordingly - All fixes were done in Power Query.&lt;/p&gt;\n&lt;section id=\"data-processing\" class=\"level3\"&gt;\n&lt;h3&gt;Data Processing ⚙️&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;The data needs a calendar table so as to use Time Intelligence functions further down the line.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Using the CALENDAR function, a new table with calculated, dynamic columns of Date, Month and Year was generated.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;section id=\"data-modeling\" class=\"level3\"&gt;\n&lt;h3&gt;Data Modeling 🏛&lt;/h3&gt;\n&lt;p&gt;a one-to-many active relationship was established between the calendar table and the data table &lt;img src=\"data-model.png\" class=\"img-fluid\" alt=\"Schema\" /&gt;&lt;/p&gt;\n&lt;/section&gt;\n&lt;section id=\"data-visualization\" class=\"level3\"&gt;\n&lt;h3&gt;Data Visualization 🎨&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;Power BI magic!✨ The report pbix file is available in this repo to explore design decisions The following DAX measures were used:&lt;/li&gt;\n&lt;/ul&gt;\n&lt;pre&gt;&lt;code&gt;Count of Accidents = DISTINCTCOUNT(Data[Accident_Index])\n\nCurrent Year Accident Count = TOTALYTD(COUNT(Data[Accident_Index]),&#39;Calendar&#39;[Date])\n\nCurrent Year Casualties = TOTALYTD(SUM(Data[Number_of_Casualties]),&#39;Calendar&#39;[Date])\n\nPrevious Year Accident Counts = CALCULATE(COUNT(Data[Accident_Index]), SAMEPERIODLASTYEAR(&#39;Calendar&#39;[Date]))\n\nPrevious Year Casualties = CALCULATE(SUM(Data[Number_of_Casualties]),\nSAMEPERIODLASTYEAR(&#39;Calendar&#39;[Date]))\n\nYoY Accident Count = DIVIDE([Current Year Accident Count] - [Previous Year Accident Counts], [Previous Year Accident Counts])\n\nYoY Casualties = (DIVIDE([Current Year Casualties] - [Previous Year Casualties],[Previous Year Casualties]))&lt;/code&gt;&lt;/pre&gt;\n&lt;/section&gt;\n&lt;section id=\"deriving-insights\" class=\"level3\"&gt;\n&lt;h3&gt;Deriving Insights&lt;/h3&gt;\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;The vehicle type most involved in accidents was cars. This can be explained by the fact that most vehicles plying UK roads are cars&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;There is a general decrease in casualty counts in 2022 compared to 2021&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n&lt;section id=\"contact\" class=\"level2\"&gt;\n&lt;h2&gt;Contact&lt;/h2&gt;\n&lt;p&gt;Edun Joshua Olubori - &lt;a href=\"https://www.linkedin.com/in/joshua-edun\"&gt;connect on linkedin&lt;/a&gt; - joshuaolubori@gmail.com&lt;/p&gt;\n&lt;p&gt;Live Demo: &lt;a href=\"https://app.powerbi.com/view?r=eyJrIjoiYjZiNDYwMTctY2EwOC00NGQ2LWJmOGEtMzVlN2I2YTFlZTEwIiwidCI6ImRmODY3OWNkLWE4MGUtNDVkOC05OWFjLWM4M2VkN2ZmOTVhMCJ9&amp;embedImagePlaceholder=true\"&gt;here&lt;/a&gt;&lt;/p&gt;\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Home\"&gt;Home&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.html\"&gt;/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Posts\"&gt;Posts&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/posts_index.html\"&gt;/posts_index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"footer-left\"&gt;Blog made with 💚 and &lt;a href=\"https://quarto.org/\"&gt;Quarto&lt;/a&gt;, by Edun Joshua. License: &lt;a href=\"https://creativecommons.org/licenses/by-sa/2.0/\"&gt;CC BY-SA 2.0&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;Chamber of (data) secrets - UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Chamber of (data) secrets&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;Featuring data modeling in Power BI, and Dax&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;Featuring data modeling in Power BI, and Dax&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;script src=\"https://giscus.app/client.js\"\n        data-repo=\"JoshuaOlubori/qblog\"\n        data-repo-id=\"R_kgDOKvhXew\"\n        data-category=\"General\"\n        data-category-id=\"DIC_kwDOKvhXe84CbFeH\"\n        data-mapping=\"title\"\n        data-reactions-enabled=\"1\"\n        data-emit-metadata=\"0\"\n        data-input-position=\"top\"\n        data-theme=\"light\"\n        data-lang=\"en\"\n        crossorigin=\"anonymous\"\n        async&gt;\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n&lt;footer class=\"footer\"&gt;\n  &lt;div class=\"nav-footer\"&gt;\n    &lt;div class=\"nav-footer-left\"&gt;\n      &lt;div class='footer-contents'&gt;Blog made with 💚 and [Quarto](https://quarto.org/),  by Edun Joshua.  License: [CC BY-SA 2.0](https://creativecommons.org/licenses/by-sa/2.0/).&lt;/div&gt;  \n    &lt;/div&gt;   \n    &lt;div class=\"nav-footer-center\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n    &lt;div class=\"nav-footer-right\"&gt;\n      &nbsp;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#requirement-gathering",
    "href": "posts/2022_05_28_UK-accidents/index.html#requirement-gathering",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "Requirement Gathering",
    "text": "Requirement Gathering\nClient wants to create a dashboard on road accidents for the year 2021 and 2022.\n\na. Primary KPIs\n\nTotal Casualties and Total Accident Values:\n\nCurrent Year\nYear-over-Year (YoY) Growth\n\nTotal Casualties by Accident Severity:\n\nCurrent Year\nYear-over-Year (YoY) Growth\n\n\n\n\nb. Secondary KPIs\n\nTotal Casualties with Respect to Vehicle Type:\n\nCurrent Year\n\nMonthly Trend Comparison of Casualties:\n\nCurrent Year vs. Previous Year\n\nCasualties by Road Type:\n\nCurrent Year\n\nCurrent Year Casualties by Area/Location & Day/Night\nTotal Casualties and Total Accidents by Location Total casualties and total accidents by location"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#identifying-stakeholders",
    "href": "posts/2022_05_28_UK-accidents/index.html#identifying-stakeholders",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "Identifying Stakeholders 🧑🏽‍💼",
    "text": "Identifying Stakeholders 🧑🏽‍💼\n\nEmergency Services Departments\nRoad Safety Corps\nTraffic Management Agencies\nPolice Force\nGeneral public"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#understanding-raw-data",
    "href": "posts/2022_05_28_UK-accidents/index.html#understanding-raw-data",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "Understanding Raw Data 🥩",
    "text": "Understanding Raw Data 🥩\nData Grain: A single instance of a reported accident event in the UK in 2021 and 2022\n\n\n\n\n\n\n\nFields\ndescription (datatype)\n\n\n\n\nAccident_Index\nunique row identifier (string)\n\n\nAccident Date\ndate of accident event (date)\n\n\nDay_of_Week\nday of the week (string)\n\n\nJunction_Control\nmechanism of traffic control at junction (string)\n\n\nJunction_Detail\ntype of junction (string)\n\n\nAccident_Severity\nseverity of accident (string)\n\n\nLatitude\nlatitude (float)\n\n\nLight_Conditions\nlight conditions at accident scene (string)\n\n\nLocal_Authority_(District)\nname of district (string)\n\n\nCarriageway_Hazards\nhazards on the road if any\n\n\nNumber_of_Casualties\nnumber of causalities (integer)\n\n\nNumber_of_Vehicles\nnumber of vehicles involved in the accident (integer)\n\n\nPolice_Force\nPolice force jurisdiction (string)\n\n\nRoad_Surface_Conditions\ncondition of road surface (string)\n\n\nRoad_Type\nroad type (string)\n\n\nSpeed_limit\nroad speed limit in mph (string)\n\n\nTime\ntime of day accident occurred (time)\n\n\nUrban_or_Rural_Area\nWhether accident occurred in an urban or rural area (string)\n\n\nWeather_Conditions\nweather condition at time of accident (string)\n\n\nVehicle_Type\ntype of vehicle involved in the accident"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#data-cleansing",
    "href": "posts/2022_05_28_UK-accidents/index.html#data-cleansing",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "Data cleansing 🧹",
    "text": "Data cleansing 🧹\nIssues - Junction_Control column has, among others, two values “Auto traffic sigl” and “Auto traffic signal” of which the former is a misspelling of the latter\n\nInstances where “Fatal” is misspelled as “Fetal” in Accident_Severity column\n“Time” column was represented as a datetime data type in Power BI\n\nFixes - Used the Replace Values feature to correct the misspellings of Issues 1 & 2 - Changed the data type to time accordingly - All fixes were done in Power Query.\n\nData Processing ⚙️\n\nThe data needs a calendar table so as to use Time Intelligence functions further down the line.\nUsing the CALENDAR function, a new table with calculated, dynamic columns of Date, Month and Year was generated.\n\n\n\nData Modeling 🏛\na one-to-many active relationship was established between the calendar table and the data table \n\n\nData Visualization 🎨\n\nPower BI magic!✨ The report pbix file is available in this repo to explore design decisions The following DAX measures were used:\n\nCount of Accidents = DISTINCTCOUNT(Data[Accident_Index])\n\nCurrent Year Accident Count = TOTALYTD(COUNT(Data[Accident_Index]),'Calendar'[Date])\n\nCurrent Year Casualties = TOTALYTD(SUM(Data[Number_of_Casualties]),'Calendar'[Date])\n\nPrevious Year Accident Counts = CALCULATE(COUNT(Data[Accident_Index]), SAMEPERIODLASTYEAR('Calendar'[Date]))\n\nPrevious Year Casualties = CALCULATE(SUM(Data[Number_of_Casualties]),\nSAMEPERIODLASTYEAR('Calendar'[Date]))\n\nYoY Accident Count = DIVIDE([Current Year Accident Count] - [Previous Year Accident Counts], [Previous Year Accident Counts])\n\nYoY Casualties = (DIVIDE([Current Year Casualties] - [Previous Year Casualties],[Previous Year Casualties]))\n\n\nDeriving Insights\n\nThe vehicle type most involved in accidents was cars. This can be explained by the fact that most vehicles plying UK roads are cars\nThere is a general decrease in casualty counts in 2022 compared to 2021"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/index.html#contact",
    "href": "posts/2022_05_28_UK-accidents/index.html#contact",
    "title": "UK Road Accidents and Casualties Tracking Dashboard (2021 - 2022)",
    "section": "Contact",
    "text": "Contact\nEdun Joshua Olubori - connect on linkedin - joshuaolubori@gmail.com\nLive Demo: here"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/nga_covid19.html",
    "href": "posts/2022_05_28_COVID-NGA-analysis/nga_covid19.html",
    "title": "Chamber of (data) secrets",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot\nimport seaborn as sns\nimport matplotlib.dates as mdates\nfrom matplotlib.dates import DateFormatter\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nImporting the neccesary libraries..\nReading the files and inspecting the data…\n\nnga = pd.read_csv(\"nga_subnational_covid19_hera.csv\", sep=\";\")\n\n\n#checking the columns\nnga.columns\n\nIndex(['ID', 'DATE', 'ISO_3', 'PAYS', 'ID_PAYS', 'REGION', 'ID_REGION',\n       'CONTAMINES', 'DECES', 'GUERIS', 'CONTAMINES_FEMME', 'CONTAMINES_HOMME',\n       'CONTAMINES_GENRE_NON_SPECIFIE', 'SOURCE'],\n      dtype='object')\n\n\n\nnga.shape\n\n(26866, 14)\n\n\n\nprint(nga.head(10))\n\n   ID        DATE ISO_3     PAYS  ID_PAYS       REGION  ID_REGION  CONTAMINES  \\\n0   1  27/02/2020   NGA  Nigéria       15         Abia        170         0.0   \n1   2  27/02/2020   NGA  Nigéria       15      Adamawa        171         0.0   \n2   3  27/02/2020   NGA  Nigéria       15    Akwa Ibom        172         0.0   \n3   4  27/02/2020   NGA  Nigéria       15      Anambra        173         0.0   \n4   5  27/02/2020   NGA  Nigéria       15       Bauchi        174         0.0   \n5   6  27/02/2020   NGA  Nigéria       15      Bayelsa        175         0.0   \n6   7  27/02/2020   NGA  Nigéria       15        Benue        176         0.0   \n7   8  27/02/2020   NGA  Nigéria       15        Borno        177         0.0   \n8   9  27/02/2020   NGA  Nigéria       15  Cross River        178         0.0   \n9  10  27/02/2020   NGA  Nigéria       15        Delta        179         0.0   \n\n   DECES  GUERIS  CONTAMINES_FEMME  CONTAMINES_HOMME  \\\n0    0.0     0.0               0.0               0.0   \n1    0.0     0.0               0.0               0.0   \n2    0.0     0.0               0.0               0.0   \n3    0.0     0.0               0.0               0.0   \n4    0.0     0.0               0.0               0.0   \n5    0.0     0.0               0.0               0.0   \n6    0.0     0.0               0.0               0.0   \n7    0.0     0.0               0.0               0.0   \n8    0.0     0.0               0.0               0.0   \n9    0.0     0.0               0.0               0.0   \n\n   CONTAMINES_GENRE_NON_SPECIFIE                              SOURCE  \n0                            0.0  Nigeria Centre for Disease Control  \n1                            0.0  Nigeria Centre for Disease Control  \n2                            0.0  Nigeria Centre for Disease Control  \n3                            0.0  Nigeria Centre for Disease Control  \n4                            0.0  Nigeria Centre for Disease Control  \n5                            0.0  Nigeria Centre for Disease Control  \n6                            0.0  Nigeria Centre for Disease Control  \n7                            0.0  Nigeria Centre for Disease Control  \n8                            0.0  Nigeria Centre for Disease Control  \n9                            0.0  Nigeria Centre for Disease Control  \n\n\n\nnga.tail()\n\n\n\n\n\n\n\n\nID\nDATE\nISO_3\nPAYS\nID_PAYS\nREGION\nID_REGION\nCONTAMINES\nDECES\nGUERIS\nCONTAMINES_FEMME\nCONTAMINES_HOMME\nCONTAMINES_GENRE_NON_SPECIFIE\nSOURCE\n\n\n\n\n26861\n26862\n02/02/2022\nNGA\nNigéria\n15\nSokoto\n203\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n26862\n26863\n02/02/2022\nNGA\nNigéria\n15\nTaraba\n204\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n26863\n26864\n02/02/2022\nNGA\nNigéria\n15\nYobe\n205\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n26864\n26865\n02/02/2022\nNGA\nNigéria\n15\nZamfara\n206\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n26865\n26866\n02/02/2022\nNGA\nNigéria\n15\nNon spécifié\n207\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n\n\n\n\n\n\nnga.describe()\n#negative numbers present in contamines,gueris, homme, non_specifie columns\n#\n\n\n\n\n\n\n\n\nID\nID_PAYS\nID_REGION\nCONTAMINES\nDECES\nGUERIS\nCONTAMINES_FEMME\nCONTAMINES_HOMME\nCONTAMINES_GENRE_NON_SPECIFIE\n\n\n\n\ncount\n26866.000000\n26866.0\n26866.00000\n26802.000000\n26463.000000\n26280.000000\n17734.000000\n17734.000000\n23663.000000\n\n\nmean\n13433.500000\n15.0\n188.50000\n9.440452\n0.118467\n8.714574\n2.197023\n1.259276\n8.102481\n\n\nstd\n7755.690502\n0.0\n10.96606\n45.747271\n0.942725\n107.723565\n25.365332\n15.169063\n46.874095\n\n\nmin\n1.000000\n15.0\n170.00000\n-1.000000\n0.000000\n-46.000000\n0.000000\n-90.000000\n-1.000000\n\n\n25%\n6717.250000\n15.0\n179.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n13433.500000\n15.0\n188.50000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n20149.750000\n15.0\n198.00000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n26866.000000\n15.0\n207.00000\n3393.000000\n79.000000\n10946.000000\n696.000000\n493.000000\n3393.000000\n\n\n\n\n\n\n\nChecking for null values…\n\nnga.isna().sum()\n\nID                                  0\nDATE                                0\nISO_3                               0\nPAYS                                0\nID_PAYS                             0\nREGION                              0\nID_REGION                           0\nCONTAMINES                         64\nDECES                             403\nGUERIS                            586\nCONTAMINES_FEMME                 9132\nCONTAMINES_HOMME                 9132\nCONTAMINES_GENRE_NON_SPECIFIE    3203\nSOURCE                              0\ndtype: int64\n\n\nFilling in the null values with the median values per column\n\nnga['CONTAMINES']=nga.CONTAMINES.fillna(nga.CONTAMINES.median())\nnga['DECES']=nga.DECES.fillna(nga.DECES.median())\nnga['GUERIS']=nga.GUERIS.fillna(nga.GUERIS.median())\nnga['CONTAMINES_FEMME']=nga.CONTAMINES_FEMME.fillna(nga.CONTAMINES_FEMME.median())\nnga['CONTAMINES_HOMME']=nga.CONTAMINES_HOMME.fillna(nga.CONTAMINES_HOMME.median())\nnga['CONTAMINES_GENRE_NON_SPECIFIE']=nga.CONTAMINES_GENRE_NON_SPECIFIE.fillna(nga.CONTAMINES_GENRE_NON_SPECIFIE.median())\n\n\n#checking for null values\nnga.isna().sum()\n\nID                               0\nDATE                             0\nISO_3                            0\nPAYS                             0\nID_PAYS                          0\nREGION                           0\nID_REGION                        0\nCONTAMINES                       0\nDECES                            0\nGUERIS                           0\nCONTAMINES_FEMME                 0\nCONTAMINES_HOMME                 0\nCONTAMINES_GENRE_NON_SPECIFIE    0\nSOURCE                           0\ndtype: int64\n\n\n\nnga.describe()\n\n\n\n\n\n\n\n\nID\nID_PAYS\nID_REGION\nCONTAMINES\nDECES\nGUERIS\nCONTAMINES_FEMME\nCONTAMINES_HOMME\nCONTAMINES_GENRE_NON_SPECIFIE\n\n\n\n\ncount\n26866.000000\n26866.0\n26866.00000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n\n\nmean\n13433.500000\n15.0\n188.50000\n9.417963\n0.116690\n8.524492\n1.450234\n0.831237\n7.136492\n\n\nstd\n7755.690502\n0.0\n10.96606\n45.695065\n0.935739\n106.549817\n20.634374\n12.338562\n44.069426\n\n\nmin\n1.000000\n15.0\n170.00000\n-1.000000\n0.000000\n-46.000000\n0.000000\n-90.000000\n-1.000000\n\n\n25%\n6717.250000\n15.0\n179.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n13433.500000\n15.0\n188.50000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n20149.750000\n15.0\n198.00000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n26866.000000\n15.0\n207.00000\n3393.000000\n79.000000\n10946.000000\n696.000000\n493.000000\n3393.000000\n\n\n\n\n\n\n\nI observe from the code above that some columns contain negative values. These are probable input errors. I replace them with the median values per column.\n\nimport numpy as np\nnga.GUERIS = np.abs(nga.GUERIS)\nnga.CONTAMINES = np.abs(nga.CONTAMINES)\nnga.CONTAMINES_HOMME = np.abs(nga.CONTAMINES_HOMME)\nnga.CONTAMINES = np.abs(nga.CONTAMINES)\nnga.CONTAMINES_GENRE_NON_SPECIFIE = np.abs(nga.CONTAMINES_GENRE_NON_SPECIFIE)\n\n\nnga.describe()\n\n\n\n\n\n\n\n\nID\nID_PAYS\nID_REGION\nCONTAMINES\nDECES\nGUERIS\nCONTAMINES_FEMME\nCONTAMINES_HOMME\nCONTAMINES_GENRE_NON_SPECIFIE\n\n\n\n\ncount\n26866.000000\n26866.0\n26866.00000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n26866.000000\n\n\nmean\n13433.500000\n15.0\n188.50000\n9.418038\n0.116690\n8.528735\n1.450234\n0.837936\n7.136567\n\n\nstd\n7755.690502\n0.0\n10.96606\n45.695049\n0.935739\n106.549477\n20.634374\n12.338109\n44.069414\n\n\nmin\n1.000000\n15.0\n170.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n6717.250000\n15.0\n179.00000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n13433.500000\n15.0\n188.50000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n20149.750000\n15.0\n198.00000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n26866.000000\n15.0\n207.00000\n3393.000000\n79.000000\n10946.000000\n696.000000\n493.000000\n3393.000000\n\n\n\n\n\n\n\n\nnga.head()\n\n\n\n\n\n\n\n\nID\nDATE\nISO_3\nPAYS\nID_PAYS\nREGION\nID_REGION\nCONTAMINES\nDECES\nGUERIS\nCONTAMINES_FEMME\nCONTAMINES_HOMME\nCONTAMINES_GENRE_NON_SPECIFIE\nSOURCE\n\n\n\n\n0\n1\n27/02/2020\nNGA\nNigéria\n15\nAbia\n170\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n1\n2\n27/02/2020\nNGA\nNigéria\n15\nAdamawa\n171\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n2\n3\n27/02/2020\nNGA\nNigéria\n15\nAkwa Ibom\n172\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n3\n4\n27/02/2020\nNGA\nNigéria\n15\nAnambra\n173\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n4\n5\n27/02/2020\nNGA\nNigéria\n15\nBauchi\n174\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n\n\n\n\n\nNow the values are all integers\nThe columns are in French. I rename each accordingly below\n\nnga.rename(columns={\n    'ID': 'id',\n    'DATE': 'date',\n    'ISO_3': 'iso_3',\n    'PAYS': 'country',\n    'ID_PAYS': 'country_id',\n    'REGION': 'state',\n    'ID_REGION': 'state_id',\n    'CONTAMINES': 'confirmed',\n    'DECES': 'deaths',\n    'GUERIS': 'recovered',\n    'CONTAMINES_FEMME': 'confirmed_female',\n    'CONTAMINES_HOMME': 'confirmed_male',\n    'CONTAMINES_GENRE_NON_SPECIFIE': 'confirmed_gender_ns',\n    'SOURCE': 'source'\n}, inplace=True)\n\n\n\nnga.head()\n\n\n\n\n\n\n\n\nid\ndate\niso_3\ncountry\ncountry_id\nstate\nstate_id\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\nsource\n\n\n\n\n0\n1\n27/02/2020\nNGA\nNigéria\n15\nAbia\n170\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n1\n2\n27/02/2020\nNGA\nNigéria\n15\nAdamawa\n171\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n2\n3\n27/02/2020\nNGA\nNigéria\n15\nAkwa Ibom\n172\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n3\n4\n27/02/2020\nNGA\nNigéria\n15\nAnambra\n173\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n4\n5\n27/02/2020\nNGA\nNigéria\n15\nBauchi\n174\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n\n\n\n\n\n‘Nigeria’ is misspelt. Correcting the error.\n\nnga.country=nga.country.replace(['Nigéria'],'Nigeria')\nnga.head()\n\n\n\n\n\n\n\n\nid\ndate\niso_3\ncountry\ncountry_id\nstate\nstate_id\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\nsource\n\n\n\n\n0\n1\n27/02/2020\nNGA\nNigeria\n15\nAbia\n170\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n1\n2\n27/02/2020\nNGA\nNigeria\n15\nAdamawa\n171\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n2\n3\n27/02/2020\nNGA\nNigeria\n15\nAkwa Ibom\n172\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n3\n4\n27/02/2020\nNGA\nNigeria\n15\nAnambra\n173\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n4\n5\n27/02/2020\nNGA\nNigeria\n15\nBauchi\n174\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNigeria Centre for Disease Control\n\n\n\n\n\n\n\n\n# Check that there is only one instance of each country\nnga['country'].value_counts()\n\nNigeria    26866\nName: country, dtype: int64\n\n\n\n#checking that there are 36 states\nstates = nga['state']\nprint(states)\n\n0                Abia\n1             Adamawa\n2           Akwa Ibom\n3             Anambra\n4              Bauchi\n             ...     \n26861          Sokoto\n26862          Taraba\n26863            Yobe\n26864         Zamfara\n26865    Non spécifié\nName: state, Length: 26866, dtype: object\n\n\n\n\nprint(states)\n\n0                Abia\n1             Adamawa\n2           Akwa Ibom\n3             Anambra\n4              Bauchi\n             ...     \n26861          Sokoto\n26862          Taraba\n26863            Yobe\n26864         Zamfara\n26865    Non spécifié\nName: state, Length: 26866, dtype: object\n\n\n\n#checking for this unwanted value\nnga.state[26865]\n\n'Non spécifié'\n\n\n\n#converting date datattype to datetime\nnga.date = pd.to_datetime(nga.date)\n\n\n#checking if conversion is successfull\nnga.dtypes\n\nid                              int64\ndate                   datetime64[ns]\niso_3                          object\ncountry                        object\ncountry_id                      int64\nstate                          object\nstate_id                        int64\nconfirmed                     float64\ndeaths                        float64\nrecovered                     float64\nconfirmed_female              float64\nconfirmed_male                float64\nconfirmed_gender_ns           float64\nsource                         object\ndtype: object\n\n\n\n#dropping unwanted columns\nnga.drop(['iso_3', 'country_id', 'state_id', 'id'], axis=1, inplace=True)\n\n\n#removing an error value (row)\nnga.drop([26865], inplace=True)\n\n\n#removing an unwanted column\nnga.drop(['source'], axis=1, inplace=True)\n\n\nprint(nga.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 26865 entries, 0 to 26864\nData columns (total 9 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26865 non-null  datetime64[ns]\n 1   country              26865 non-null  object        \n 2   state                26865 non-null  object        \n 3   confirmed            26865 non-null  float64       \n 4   deaths               26865 non-null  float64       \n 5   recovered            26865 non-null  float64       \n 6   confirmed_female     26865 non-null  float64       \n 7   confirmed_male       26865 non-null  float64       \n 8   confirmed_gender_ns  26865 non-null  float64       \ndtypes: datetime64[ns](1), float64(6), object(2)\nmemory usage: 1.8+ MB\nNone\n\n\n\n#number of confirmed cases\nprint(nga.confirmed.sum())\nprint(nga.deaths.sum())\nprint(nga.recovered.sum())\n\n253025.0\n3135.0\n229133.0\n\n\n\n#checking most states with confirmed cases\nnga.sort_values(by=['confirmed'], ascending=False)\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\n\n\n\n\n25256\n2021-12-22\nNigeria\nLagos\n3393.0\n0.0\n0.0\n0.0\n0.0\n3393.0\n\n\n25218\n2021-12-21\nNigeria\nLagos\n1552.0\n0.0\n0.0\n0.0\n0.0\n1552.0\n\n\n12868\n2021-01-30\nNigeria\nLagos\n1040.0\n0.0\n461.0\n0.0\n0.0\n1040.0\n\n\n25066\n2021-12-17\nNigeria\nLagos\n906.0\n0.0\n0.0\n0.0\n0.0\n906.0\n\n\n12374\n2021-01-17\nNigeria\nLagos\n901.0\n2.0\n717.0\n0.0\n0.0\n901.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10456\n2020-11-28\nNigeria\nBenue\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n10455\n2020-11-28\nNigeria\nBayelsa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n10454\n2020-11-28\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n10453\n2020-11-28\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n26864\n2022-02-02\nNigeria\nZamfara\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n26865 rows × 9 columns\n\n\n\n\n#exporting the data\nout_nga = 'nga_covid19.csv'\nnga.to_csv(out_nga)\n\n\nnga.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 26865 entries, 0 to 26864\nData columns (total 9 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26865 non-null  datetime64[ns]\n 1   country              26865 non-null  object        \n 2   state                26865 non-null  object        \n 3   confirmed            26865 non-null  float64       \n 4   deaths               26865 non-null  float64       \n 5   recovered            26865 non-null  float64       \n 6   confirmed_female     26865 non-null  float64       \n 7   confirmed_male       26865 non-null  float64       \n 8   confirmed_gender_ns  26865 non-null  float64       \ndtypes: datetime64[ns](1), float64(6), object(2)\nmemory usage: 1.8+ MB\n\n\n\n#reading population table. The goal is to merge with nga\nimport pandas as pd\npop_nga = pd.read_csv(r\"C:\\Users\\eduna\\Documents\\state_pop.txt\", sep='\\s+')\npop_nga.head()\n\n\n\n\n\n\n\n\nstate\npopulation_(2022)\n\n\n\n\n0\nLagos\n21,000,000\n\n\n1\nKano\n12,550,598\n\n\n2\nKaduna\n8,252,366\n\n\n3\nOyo\n7,840,864\n\n\n4\nKatsina\n7,831,319\n\n\n\n\n\n\n\n\n#correcting some states misspellings\npop_nga.state=pop_nga.state.replace(['Akwa-Ibom'],'Akwa Ibom')\npop_nga.state=pop_nga.state.replace(['Cross-River'],'Cross River')\n\n\n#checking the correction\npop_nga.state[13]\n\n'Akwa Ibom'\n\n\n\n#checking that data is clean\npop_nga.isna().sum()\npop_nga.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 37 entries, 0 to 36\nData columns (total 2 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   state              37 non-null     object\n 1   population_(2022)  37 non-null     object\ndtypes: object(2)\nmemory usage: 720.0+ bytes\n\n\n\n#exporting the data\npop_nga.to_csv('pop_nga.csv')\n\n\nnga.head()\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\n\n\n\n\n0\n2020-02-27\nNigeria\nAbia\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n2020-02-27\nNigeria\nAdamawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n2020-02-27\nNigeria\nAkwa Ibom\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n2020-02-27\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n2020-02-27\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n#dropping rows with 'Non specifie'\nnga.drop(nga.index[nga['state'] == 'Non spécifié'], inplace=True)\nnga['state']\n\n0             Abia\n1          Adamawa\n2        Akwa Ibom\n3          Anambra\n4           Bauchi\n           ...    \n26860       Rivers\n26861       Sokoto\n26862       Taraba\n26863         Yobe\n26864      Zamfara\nName: state, Length: 26159, dtype: object\n\n\n\n#merging the data with population data\nnga_merged = pd.merge(nga, pop_nga, on='state', how='left')\nnga_merged.head(45)\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\n\n\n\n\n0\n2020-02-27\nNigeria\nAbia\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,727,347\n\n\n1\n2020-02-27\nNigeria\nAdamawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,248,436\n\n\n2\n2020-02-27\nNigeria\nAkwa Ibom\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,482,177\n\n\n3\n2020-02-27\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,527,809\n\n\n4\n2020-02-27\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6,537,314\n\n\n5\n2020-02-27\nNigeria\nBayelsa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2,277,961\n\n\n6\n2020-02-27\nNigeria\nBenue\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,741,815\n\n\n7\n2020-02-27\nNigeria\nBorno\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,860,183\n\n\n8\n2020-02-27\nNigeria\nCross River\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,866,269\n\n\n9\n2020-02-27\nNigeria\nDelta\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,663,362\n\n\n10\n2020-02-27\nNigeria\nEbonyi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2,880,383\n\n\n11\n2020-02-27\nNigeria\nEdo\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,235,595\n\n\n12\n2020-02-27\nNigeria\nEkiti\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,270,798\n\n\n13\n2020-02-27\nNigeria\nEnugu\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,411,119\n\n\n14\n2020-02-27\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n15\n2020-02-27\nNigeria\nGombe\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,256,962\n\n\n16\n2020-02-27\nNigeria\nImo\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,408,756\n\n\n17\n2020-02-27\nNigeria\nJigawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,828,163\n\n\n18\n2020-02-27\nNigeria\nKaduna\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n8,252,366\n\n\n19\n2020-02-27\nNigeria\nKano\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n12,550,598\n\n\n20\n2020-02-27\nNigeria\nKatsina\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n7,831,319\n\n\n21\n2020-02-27\nNigeria\nKebbi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,440,050\n\n\n22\n2020-02-27\nNigeria\nKogi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,473,490\n\n\n23\n2020-02-27\nNigeria\nKwara\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,192,893\n\n\n24\n2020-02-27\nNigeria\nLagos\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n21,000,000\n\n\n25\n2020-02-27\nNigeria\nNasarawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2,523,395\n\n\n26\n2020-02-27\nNigeria\nNiger\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,556,247\n\n\n27\n2020-02-27\nNigeria\nOgun\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n5,217,716\n\n\n28\n2020-02-27\nNigeria\nOsun\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,705,589\n\n\n29\n2020-02-27\nNigeria\nOndo\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,671,695\n\n\n30\n2020-02-27\nNigeria\nOyo\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n7,840,864\n\n\n31\n2020-02-27\nNigeria\nPlateau\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,200,442\n\n\n32\n2020-02-27\nNigeria\nRivers\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n7,303,924\n\n\n33\n2020-02-27\nNigeria\nSokoto\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,998,090\n\n\n34\n2020-02-27\nNigeria\nTaraba\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,066,834\n\n\n35\n2020-02-27\nNigeria\nYobe\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,294,137\n\n\n36\n2020-02-27\nNigeria\nZamfara\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,515,427\n\n\n37\n2020-02-28\nNigeria\nAbia\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,727,347\n\n\n38\n2020-02-28\nNigeria\nAdamawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,248,436\n\n\n39\n2020-02-28\nNigeria\nAkwa Ibom\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,482,177\n\n\n40\n2020-02-28\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,527,809\n\n\n41\n2020-02-28\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6,537,314\n\n\n42\n2020-02-28\nNigeria\nBayelsa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2,277,961\n\n\n43\n2020-02-28\nNigeria\nBenue\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,741,815\n\n\n44\n2020-02-28\nNigeria\nBorno\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,860,183\n\n\n\n\n\n\n\n\n#exporting the data\nnga_merged.to_csv('nga_merged.csv')\n\nPermissionError: [Errno 13] Permission denied: 'nga_merged.csv'\n\n\n\n#checking the newly merged column\nnga_merged.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 26159 entries, 0 to 26158\nData columns (total 10 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26159 non-null  datetime64[ns]\n 1   country              26159 non-null  object        \n 2   state                26159 non-null  object        \n 3   confirmed            26159 non-null  float64       \n 4   deaths               26159 non-null  float64       \n 5   recovered            26159 non-null  float64       \n 6   confirmed_female     26159 non-null  float64       \n 7   confirmed_male       26159 non-null  float64       \n 8   confirmed_gender_ns  26159 non-null  float64       \n 9   population_(2022)    25452 non-null  object        \ndtypes: datetime64[ns](1), float64(6), object(3)\nmemory usage: 2.2+ MB\n\n\n\n#checking for the columns with null values\nnga_merged.isnull().sum()\n\ndate                     0\ncountry                  0\nstate                    0\nconfirmed                0\ndeaths                   0\nrecovered                0\nconfirmed_female         0\nconfirmed_male           0\nconfirmed_gender_ns      0\npopulation_(2022)      707\ndtype: int64\n\n\nThe population column has some missing values. Let’s check that out…\n\nnga_merged[nga_merged['population_(2022)'].isnull()]\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\n\n\n\n\n14\n2020-02-27\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n51\n2020-02-28\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n88\n2020-02-29\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n125\n2020-01-03\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n162\n2020-02-03\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n25988\n2022-01-29\nNigeria\nFederal Capital Territory\n3.0\n0.0\n0.0\n0.0\n0.0\n3.0\nNaN\n\n\n26025\n2022-01-30\nNigeria\nFederal Capital Territory\n6.0\n0.0\n245.0\n0.0\n0.0\n6.0\nNaN\n\n\n26062\n2022-01-31\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n26099\n2022-01-02\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n26136\n2022-02-02\nNigeria\nFederal Capital Territory\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNaN\n\n\n\n\n707 rows × 10 columns\n\n\n\nThe problem seems to be that values for the Federal Capital Territory were not inputed during the merging\n\n#fixing the nans in the population column by inserting with the value of the population of the FCT\nnga_merged['population_(2022)'] = nga_merged['population_(2022)'].fillna(3564126)\n#confirming that no nulls are present\nnga_merged[nga_merged['population_(2022)'].isnull()]\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\n\n\n\n\n\n\n\n\n\n\nnga_merged.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 26159 entries, 0 to 26158\nData columns (total 10 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26159 non-null  datetime64[ns]\n 1   country              26159 non-null  object        \n 2   state                26159 non-null  object        \n 3   confirmed            26159 non-null  float64       \n 4   deaths               26159 non-null  float64       \n 5   recovered            26159 non-null  float64       \n 6   confirmed_female     26159 non-null  float64       \n 7   confirmed_male       26159 non-null  float64       \n 8   confirmed_gender_ns  26159 non-null  float64       \n 9   population_(2022)    26159 non-null  object        \ndtypes: datetime64[ns](1), float64(6), object(3)\nmemory usage: 2.2+ MB\n\n\nCreating separate columns for year, month and date in preparation for aggregation\n\n#Extracting the year from the date column\nnga_merged['year'] = pd.DatetimeIndex(nga_merged['date']).year\n#confirming\nnga_merged.head()\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\nyear\n\n\n\n\n0\n2020-02-27\nNigeria\nAbia\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,727,347\n2020\n\n\n1\n2020-02-27\nNigeria\nAdamawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,248,436\n2020\n\n\n2\n2020-02-27\nNigeria\nAkwa Ibom\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,482,177\n2020\n\n\n3\n2020-02-27\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,527,809\n2020\n\n\n4\n2020-02-27\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6,537,314\n2020\n\n\n\n\n\n\n\n\n#Extracting month\nnga_merged['month'] = pd.DatetimeIndex(nga_merged['date']).month\n#confirming\nnga_merged.head()\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\nyear\nmonth\n\n\n\n\n0\n2020-02-27\nNigeria\nAbia\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n3,727,347\n2020\n2\n\n\n1\n2020-02-27\nNigeria\nAdamawa\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n4,248,436\n2020\n2\n\n\n2\n2020-02-27\nNigeria\nAkwa Ibom\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,482,177\n2020\n2\n\n\n3\n2020-02-27\nNigeria\nAnambra\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5,527,809\n2020\n2\n\n\n4\n2020-02-27\nNigeria\nBauchi\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6,537,314\n2020\n2\n\n\n\n\n\n\n\n\nnga_merged.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 26159 entries, 0 to 26158\nData columns (total 12 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26159 non-null  datetime64[ns]\n 1   country              26159 non-null  object        \n 2   state                26159 non-null  object        \n 3   confirmed            26159 non-null  float64       \n 4   deaths               26159 non-null  float64       \n 5   recovered            26159 non-null  float64       \n 6   confirmed_female     26159 non-null  float64       \n 7   confirmed_male       26159 non-null  float64       \n 8   confirmed_gender_ns  26159 non-null  float64       \n 9   population_(2022)    26159 non-null  object        \n 10  year                 26159 non-null  int64         \n 11  month                26159 non-null  int64         \ndtypes: datetime64[ns](1), float64(6), int64(2), object(3)\nmemory usage: 2.6+ MB\n\n\n\nnga_merged.to_csv(\"nga_merged_month_year.csv\")\n\n\nnga_merged['active_cases'] = nga_merged['confirmed']-(nga_merged['recovered']+nga_merged['deaths'])\nnga_merged.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 26159 entries, 0 to 26158\nData columns (total 13 columns):\n #   Column               Non-Null Count  Dtype         \n---  ------               --------------  -----         \n 0   date                 26159 non-null  datetime64[ns]\n 1   country              26159 non-null  object        \n 2   state                26159 non-null  object        \n 3   confirmed            26159 non-null  float64       \n 4   deaths               26159 non-null  float64       \n 5   recovered            26159 non-null  float64       \n 6   confirmed_female     26159 non-null  float64       \n 7   confirmed_male       26159 non-null  float64       \n 8   confirmed_gender_ns  26159 non-null  float64       \n 9   population_(2022)    26159 non-null  object        \n 10  year                 26159 non-null  int64         \n 11  month                26159 non-null  int64         \n 12  active_cases         26159 non-null  float64       \ndtypes: datetime64[ns](1), float64(7), int64(2), object(3)\nmemory usage: 2.8+ MB\n\n\n\nnga_merged[nga_merged['active_cases'].values &lt; 0]\n\n\n\n\n\n\n\n\ndate\ncountry\nstate\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\npopulation_(2022)\nyear\nmonth\nactive_cases\n\n\n\n\n767\n2020-03-18\nNigeria\nOgun\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n5,217,716\n2020\n3\n-1.0\n\n\n878\n2020-03-21\nNigeria\nOgun\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n5,217,716\n2020\n3\n-1.0\n\n\n1233\n2020-03-31\nNigeria\nEkiti\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n3,270,798\n2020\n3\n-1.0\n\n\n1356\n2020-03-04\nNigeria\nLagos\n7.0\n0.0\n11.0\n0.0\n0.0\n0.0\n21,000,000\n2020\n3\n-4.0\n\n\n1467\n2020-06-04\nNigeria\nLagos\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n21,000,000\n2020\n6\n-1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26029\n2022-01-30\nNigeria\nKaduna\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n8,252,366\n2022\n1\n-7.0\n\n\n26030\n2022-01-30\nNigeria\nKano\n1.0\n0.0\n6.0\n0.0\n0.0\n1.0\n12,550,598\n2022\n1\n-5.0\n\n\n26038\n2022-01-30\nNigeria\nOgun\n0.0\n0.0\n3.0\n0.0\n0.0\n0.0\n5,217,716\n2022\n1\n-3.0\n\n\n26039\n2022-01-30\nNigeria\nOsun\n9.0\n0.0\n15.0\n0.0\n0.0\n9.0\n4,705,589\n2022\n1\n-6.0\n\n\n26043\n2022-01-30\nNigeria\nRivers\n2.0\n0.0\n11.0\n0.0\n0.0\n2.0\n7,303,924\n2022\n1\n-9.0\n\n\n\n\n3372 rows × 13 columns\n\n\n\nThe issue seems to be from the formula. And the negative values occur at dates when ‘recovered’ is greater than ‘confirmed’. At such cases, active cases should be zero\n\n#using clip function to correct this\nnga_merged['active_cases'] = nga_merged['active_cases'].clip(lower=0)\nnga_merged.describe()\n\n\n\n\n\n\n\n\nconfirmed\ndeaths\nrecovered\nconfirmed_female\nconfirmed_male\nconfirmed_gender_ns\nyear\nmonth\nactive_cases\n\n\n\n\ncount\n26159.000000\n26159.000000\n26159.000000\n26159.0\n26159.0\n26159.000000\n26159.000000\n26159.000000\n26159.000000\n\n\nmean\n9.672579\n0.119844\n8.759242\n0.0\n0.0\n7.329447\n2020.609618\n6.595474\n6.319584\n\n\nstd\n46.281864\n0.948101\n107.970440\n0.0\n0.0\n44.645168\n0.575629\n3.429761\n38.039798\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n2020.000000\n1.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n2020.000000\n4.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n2021.000000\n7.000000\n0.000000\n\n\n75%\n3.000000\n0.000000\n0.000000\n0.0\n0.0\n0.000000\n2021.000000\n10.000000\n0.000000\n\n\nmax\n3393.000000\n79.000000\n10946.000000\n0.0\n0.0\n3393.000000\n2022.000000\n12.000000\n3393.000000\n\n\n\n\n\n\n\n\n#total deaths\nnga_merged['deaths'].sum()\n\n3135.0\n\n\n\n#total cases\nnga_merged['confirmed'].sum()\n\n253025.0\n\n\n\n#total recovered\nnga_merged['recovered'].sum()\n\n229133.0\n\n\nVisualizing the data\n\nsns.set(rc={'figure.figsize':(10,8)})\nsns.lineplot(x='date', y='confirmed', data=nga_merged)\n#number of confirmed cases peaked early 2022\n\n&lt;AxesSubplot:xlabel='date', ylabel='confirmed'&gt;\n\n\n\n\n\n\nsns.set(rc={'figure.figsize':(10,8)})\nsns.lineplot(x='date', y='deaths', data=nga_merged)\n#number of deaths peaked mid 2021\n\n&lt;AxesSubplot:xlabel='date', ylabel='deaths'&gt;\n\n\n\n\n\n\nsns.set(rc={'figure.figsize':(30,16)})\nsns.barplot(x='state', y='confirmed', data=nga_merged)\n\n&lt;AxesSubplot:xlabel='state', ylabel='confirmed'&gt;\n\n\n\n\n\n\nsns.set(rc={'figure.figsize':(30,16)})\nsns.barplot(x='state', y='deaths', data=nga_merged)\n#Lagos by far suffered the most fatalities. Not surprising given its massive population.\n\n&lt;AxesSubplot:xlabel='state', ylabel='deaths'&gt;\n\n\n\n\n\nGrouping the data\n\nnga_20 = nga_merged2[nga_merged2['year']==2020] # Considering data for the year 2020 only\nnga_21 = nga_merged2[nga_merged2['year']==2021] # Considering data for the year 2021 only\n#nga_22 = nga_merged2[nga_merged2['year']==2022] # Considering data for the year 2022 only\n\nGrouping by confirmed cases, recovered and deaths by month for each year since the pandemic\nFor 2020…\n\nnga_confirmed_20 = nga_20['confirmed'].groupby(nga_20['month']).sum()\nnga_confirmed_20\n\n\nplt.plot(nga_confirmed_20)\n\n\n\n\n\nnga_rec_20 = nga_20['recovered'].groupby(nga_20['month']).sum()\nnga_rec_20.head()\n\n\nplt.plot(nga_rec_20)\n\n\n\n\n\nnga_dea_20 = nga_20['deaths'].groupby(nga_20['month']).sum()\nnga_dea_20.head()\n\n\nplt.plot(nga_dea_20)\n\n\n\n\nFor 2021…\n\nnga_confirmed_21 = nga_21['confirmed'].groupby(nga_21['month']).sum()\nnga_confirmed_21.head()\n\n\nplt.plot(nga_confirmed_21)\n\n\n\n\n\nnga_rec_21 = nga_21['recovered'].groupby(nga_21['month']).sum()\nnga_rec_21.head()\n\n\nplt.plot(nga_rec_21)\n\n\n\n\n\nnga_dea_21 = nga_21['deaths'].groupby(nga_21['month']).sum()\nnga_dea_21.head\n\n\nplt.plot(nga_dea_21)\n\n\n\n\nFindings: 2020 &gt; Cases surged in the middle of year, spiking at the end of the year\n\nMosts of the deaths occurred during the middle of the year\n\nFindings: 2021 &gt; The beginning saw high number of cases, sharply falling and surging for a while in the middle of the year before rising sharply again towards the end of the year\n\nMosts of the deaths occurred during the in the beginning of the year, paralleling the rise in cases. Deaths lowered throughout the first quarter until August, its peak."
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/index.html",
    "href": "posts/2022_05_28_Adventure-works/index.html",
    "title": "Sales Monitoring Dashboard",
    "section": "",
    "text": "Client wants a dashboard monitoring sales of company’s product\n\n\n\nTop ten customers, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\nTop ten products by sales, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\n\n\n\n\n\nTotal sales by category\nDaily sales trend"
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/index.html#about-the-project",
    "href": "posts/2022_05_28_Adventure-works/index.html#about-the-project",
    "title": "Sales Monitoring Dashboard",
    "section": "",
    "text": "Client wants a dashboard monitoring sales of company’s product\n\n\n\nTop ten customers, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\nTop ten products by sales, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\n\n\n\n\n\nTotal sales by category\nDaily sales trend"
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/index.html#identifying-stakeholders",
    "href": "posts/2022_05_28_Adventure-works/index.html#identifying-stakeholders",
    "title": "Sales Monitoring Dashboard",
    "section": "Identifying Stakeholders 🧑🏽‍💼",
    "text": "Identifying Stakeholders 🧑🏽‍💼\n\nSales team\nMarketing Department\n\n\nUnderstanding Raw Data 🥩\nThe raw data exists in 9 CSV files representing 1 fact table and 8 dimension tables\n\n\n\n\n\n\n\nTable\ndescription (datatype)\n\n\n\n\nBudget\ndetails budget of each product and product categories per period\n\n\nBudgetPeriod\ndate table for the budget table\n\n\ncustomer\neach row records a customer information\n\n\ndimdates\ndate table for the fact table\n\n\ndimProductCategory\nrecords the various product categories available\n\n\ndimProductSubCategory\nlists the product subcategories available\n\n\nproduct\nlists individual products\n\n\nSales\nThe fact table: records individual sales transaction\n\n\nTerritory\nLists regional locations\n\n\n\n\n\nConverting CSV to SQL INSERT statements with Python 🐍\nI wrote a Python script using the csv and os modules, that takes in a csv file and outputs an SQL file with INSERT statements for each row in the CSV.\n\nimport csv\nimport os\n\ndef escape_quotes(value):\n    return value.replace(\"'\", \"''\")\n\ndef convert_csv_to_sql(csv_file):\n    try:\n        table_name = os.path.splitext(os.path.basename(csv_file))[0].lower()\n        \n        with open(csv_file, 'r') as csvfile:\n            reader = csv.reader(csvfile)\n            \n            headers = next(reader)\n            columns = ', '.join(headers)\n            \n            sql_file = f\"{table_name}.sql\"\n            \n            os.makedirs('results', exist_ok=True)\n            \n            sql_file = os.path.join('results', sql_file)\n            \n            with open(sql_file, 'w') as sqlfile:\n                all_values = []\n                \n                for row in reader:\n                    values = []\n                    for header, value in zip(headers, row):\n                        if header.lower() == 'id' or header.lower().endswith('_id') or header.lower() == 'quantity' or header.lower() == 'unit_price':\n                            values.append(value)\n                        elif value == '':\n                            values.append('NULL')\n                        else:\n                            values.append(f\"'{escape_quotes(value)}'\")\n                    \n                    all_values.append(f\"({', '.join(values)})\")\n\n                sql_insert = f\"INSERT INTO {table_name} ({columns}) VALUES {', '.join(all_values)};\\n\"\n                \n                sqlfile.write(sql_insert)\n                \n                print(f\"Conversion completed successfully. SQL file '{sql_file}' generated.\")\n    \n    except FileNotFoundError:\n        print(\"The specified CSV file does not exist.\")\n    \n    except Exception as e:\n        print(f\"An error occurred during the conversion: {str(e)}\")\n\n\nfiles = [\"customer.csv\",\"department.csv\",\"employee.csv\",\"employee_product.csv\",\"orders.csv\",\"product.csv\"]\n\nfor file in files:\n    convert_csv_to_sql(file)\nprint(\"\\nConversion Completed\")\n\n\n\nLoading into database 🧺\na MySQL database was created to host the data. The ERD is shown below and using MySQL’s built-in GUI tool, it was converted into a database schema\n\n\n\n\n\nTo lessen the load on the BI tool further down the pipeline, and to make modelling easier, I created 4 views on top of the data (dim_customer, dim_calendar, dim_product and fact_sales)\n\n\nData cleansing 🧹\nIssues - No major cleaning issues; data was loaded directly into the BI tool\n\n\nData Modeling 🏛\nThe following star schema was created for the data model in Power BI\n\n\n\n\n\n\n\nData Visualization 🎨\n\nPower BI magic!✨\nThe following DAX measures were used:\n\nNon-Null Days = \nCALCULATE(\n    COUNTROWS('awdb dim_calendar'),\n    NOT(ISBLANK('awdb dim_calendar'[Day]))\n)\n\nTop 10 Customers by Sales = \nIF(RANKX(ALL('awdb dim_customer'[fullname]), [Total Sales],,DESC)&lt;=10,[Total Sales], BLANK())\n\nTop 10 Products by Sales = \nIF(RANKX(ALL('awdb dim_product'[product_name]), [Total Sales],,DESC)&lt;=10,[Total Sales], BLANK())\n\nTotal Sales = \nSUM('awdb fact_sales'[SalesAmount])"
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/Adw README 3.html#about-the-project",
    "href": "posts/2022_05_28_Adventure-works/Adw README 3.html#about-the-project",
    "title": "Chamber of (data) secrets",
    "section": "About The Project 🍪",
    "text": "About The Project 🍪\n\n\n\nDashboard\n\n\nLive Demo: Here\nTracking Accidents and Casualties across UK Roads in 2021 - 2022  ### Requirement Gathering\nClient wants a dashboard monitoring sales of company’s product\n\na. Primary KPIs\n\nTop ten customers, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\nTop ten products by sales, filtered by:\n\nproduct name\nproduct category\nproduct status\nproduct line\n\n\n\n\nb. Secondary KPIs\n\nTotal sales by category\nDaily sales trend"
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/Adw README 3.html#identifying-stakeholders",
    "href": "posts/2022_05_28_Adventure-works/Adw README 3.html#identifying-stakeholders",
    "title": "Chamber of (data) secrets",
    "section": "Identifying Stakeholders 🧑🏽‍💼",
    "text": "Identifying Stakeholders 🧑🏽‍💼\n\nSales team\nMarketing Department\n\n\n\nUnderstanding Raw Data 🥩\nThe raw data exists in 9 CSV files representing 1 fact table and 8 dimension tables\n\n\n\n\n\n\n\nTable\ndescription (datatype)\n\n\n\n\nBudget\ndetails budget of each product and product categories per period\n\n\nBudgetPeriod\ndate table for the budget table\n\n\ncustomer\neach row records a customer information\n\n\ndimdates\ndate table for the fact table\n\n\ndimProductCategory\nrecords the various product categories available\n\n\ndimProductSubCategory\nlists the product subcategories available\n\n\nproduct\nlists individual products\n\n\nSales\nThe fact table: records individual sales transaction\n\n\nTerritory\nLists regional locations\n\n\n\n\n\n\nConverting CSV to SQL INSERT statements with Python 🐍\nI wrote a Python  script using the csv and os modules, that takes in a csv file and outputs an SQL file with INSERT statements for each row in the CSV.\n\n\n\nLoading into database 🧺\na MySQL database was created to host the data. The ERD is shown below and using MySQL’s built-in GUI tool, it was converted into a database schema\n\n\n\n\nCreating views 🌁\nTo lessen the load on the BI tool further down the pipeline, and to make modeling easier, I created 4 views on top of the data  (dim_customer, dim_calendar, dim_product and fact_sales)\n\n\n\nData cleansing 🧹\nIssues - No major cleaning issues; data was loaded directly into the BI tool\n ### Data Modeling 🏛\nThe following star schema was created for the data model in Power BI\n\n\n\nSchema\n\n\n\n\n\nData Visualization 🎨\n\nPower BI magic!✨ The report pbix file is available in this repo to explore design decisions\nThe following DAX measures were used:\n\n``` Non-Null Days = CALCULATE( COUNTROWS(‘awdb dim_calendar’), NOT(ISBLANK(‘awdb dim_calendar’[Day])) )\nTop 10 Customers by Sales = IF(RANKX(ALL(‘awdb dim_customer’[fullname]), [Total Sales],,DESC)&lt;=10,[Total Sales], BLANK())\nTop 10 Products by Sales = IF(RANKX(ALL(‘awdb dim_product’[product_name]), [Total Sales],,DESC)&lt;=10,[Total Sales], BLANK())\nTotal Sales = SUM(‘awdb fact_sales’[SalesAmount]) ```\n\n\n\nDeriving Insights\n\nThe primary source of revenue for the company is from the sales of bikes with 96% of sales coming from that category alone\nSales peaked on the 22nd of the month"
  },
  {
    "objectID": "posts/2022_05_28_Adventure-works/Adw README 3.html#contact",
    "href": "posts/2022_05_28_Adventure-works/Adw README 3.html#contact",
    "title": "Chamber of (data) secrets",
    "section": "Contact",
    "text": "Contact\nEdun Joshua Olubori - connect on linkedin - joshuaolubori@gmail.com\nLive Demo: Here\n\n(back to top)"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#footnotes",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#footnotes",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKatherine Unger Baillie, Credits, Baillie, K. U., Berger, M. W., Brockmeier, E. K., Writers, Date October 28, Date, 28, O., Subtopics Data Science, Subtopics, Science, D., Schools School of Engineering & Applied Science, Schools, School of Engineering & Applied Science, Share Twitter Facebook, Share, & Twitter Facebook. (1970, May 26). The role of data in a world reshaped by covid-19. Penn Today. Retrieved May 30, 2022, from https://penntoday.upenn.edu/news/role-data-world-reshaped-covid-19↩︎\nNCDC coronavirus COVID-19 microsite. Retrieved May 30, 2022, from https://covid19.ncdc.gov.ng/advisory/↩︎"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#task",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#task",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Task 📌",
    "text": "Task 📌\nTo design a Tableau dashboard to clearly visualize the COVID-19 pandemic in Nigeria since the start of the pandemic in 2020. Key metrics to show include total number of cases, infection rate, fatality rate e.t.c. My audience is the general public and therefore the viz is to be hosted on Tableau Public."
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#prepare",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#prepare",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Prepare 🧹",
    "text": "Prepare 🧹\nWhere was the data located and organized?\nThe data was sourced from the NCDC (Nigerian Centre for Disease Control) and accessed via link on the 15th of May, 2022. The dataset was contained in a CSV file, arranged in 14 columns with the headers written in French. That had to be taken care of in the data cleaning process.\nAre there issues with bias or credibility in this data? There are concerns that the data from Nigeria may not completely reflective of the true situation on ground due to low testing rates compared to Western countries. #Are there any problems with the data? Problems of note include the columns headers being in French and a few redundant columns."
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#process",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#process",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Process ⚙️",
    "text": "Process ⚙️\nWhat tools are you choosing and why?\nFor data cleaning, exploratory data analysis (EDA) and visualization, I choose Python’s pandas with Jupyter Notebooks. I like pandas versatility with handling data and together with Jupyter Notebook’s flexible running of code in cells, it makes for an efficient workflow.\nWhat steps have you taken to ensure that your data is clean?\nI opened a folder for the project on my PC and named the file with an appropriate naming convention. Then I imported the dataset using pandas and inspected the first few rows, the last few rows, the columns, the shape and some summary statistics of the data to confirm that it was imported correctly and to have a feel of the data. Using a mapping function, I renamed the column headers appropriately. Where null values where present, I filled in with zero. The Jupyter Notebook file delineates the whole data preparation process\nOn preliminary exploration on Excel, I discovered that date values on some of the tables were inconsistently formatted. Therefore all date fields have to be homogenized going forward\nI did all data cleaning and wrangling with Python’s pandas on Jupyter Notebooks. The codes can be accessed checked here"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#insights",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#insights",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Insights 🕵🏽‍♂️",
    "text": "Insights 🕵🏽‍♂️\n\nThe states of Lagos, Federal Capital Territory (Abuja), and Rivers have the highest confirmed cases most likely due to their being port cities and Lagos’ high population density.\nThere are 246,889 confirmed cases, 223,634 recoveries and 3079 deaths.\nDecember 22nd, 2021 saw the highest number of confirmed cases at 4,006 while August 29th, 2021 was the peak of total deaths at 93.\n\nThe visual can be viewed and interacted with at link"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#contact",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#contact",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "Contact",
    "text": "Contact\nEdun Joshua Olubori - connect on linkedin - joshuaolubori@gmail.com\nLive Demo: here"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#about-the-project",
    "href": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#about-the-project",
    "title": "Chamber of (data) secrets",
    "section": "About The Project 🍪",
    "text": "About The Project 🍪\n\n\n\nDashboard\n\n\nLive Demo: here\nTracking Accidents and Casualties across UK Roads in 2021 - 2022  ### Requirement Gathering\nClient wants to create a dashboard on road accidents for the year 2021 and 2022.\n\na. Primary KPIs\n\nTotal Casualties and Total Accident Values:\n\nCurrent Year\nYear-over-Year (YoY) Growth\n\nTotal Casualties by Accident Severity:\n\nCurrent Year\nYear-over-Year (YoY) Growth\n\n\n\n\nb. Secondary KPIs\n\nTotal Casualties with Respect to Vehicle Type:\n\nCurrent Year\n\nMonthly Trend Comparison of Casualties:\n\nCurrent Year vs. Previous Year\n\nCasualties by Road Type:\n\nCurrent Year\n\nCurrent Year Casualties by Area/Location & Day/Night\nTotal Casualties and Total Accidents by Location Total casualties and total accidents by location"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#identifying-stakeholders",
    "href": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#identifying-stakeholders",
    "title": "Chamber of (data) secrets",
    "section": "Identifying Stakeholders 🧑🏽‍💼",
    "text": "Identifying Stakeholders 🧑🏽‍💼\n\nEmergency Services Departments\nRoad Safety Corps\nTraffic Management Agencies\nPolice Force\nGeneral public\n\n\n\nUnderstanding Raw Data 🥩\nData Grain: A single instance of a reported accident event in the UK in 2021 amd 2022\n\n\n\n\n\n\n\nFields\ndescription (datatype)\n\n\n\n\nAccident_Index\nunique row indentifier (string)\n\n\nAccident Date\ndate of accident event (date)\n\n\nDay_of_Week\nday of the week (string)\n\n\nJunction_Control\nmechanism of traffic control at junction (string)\n\n\nJunction_Detail\ntype of junction (string)\n\n\nAccident_Severity\nseverity of accident (string)\n\n\nLatitude\nlatitude (float)\n\n\nLight_Conditions\nlight conditions at accident scene (string)\n\n\nLocal_Authority_(District)\nname of district (string)\n\n\nCarriageway_Hazards\nhazards on the road if any\n\n\nNumber_of_Casualties\nnumber of causaulties (integer)\n\n\nNumber_of_Vehicles\nnumber of vehicles involved in the accident (integer)\n\n\nPolice_Force\nPolice force jurisdiction (string)\n\n\nRoad_Surface_Conditions\ncondition of road surface (string)\n\n\nRoad_Type\nroad type (string)\n\n\nSpeed_limit\nroad speed limit in mph (string)\n\n\nTime\ntime of day accident occurred (time)\n\n\nUrban_or_Rural_Area\nWhether accident occurred in an urban or rural area (string)\n\n\nWeather_Conditions\nweather condition at time of accident (string)\n\n\nVehicle_Type\ntype of vehicle involved in the accident\n\n\n\n\n\n\nData cleansing 🧹\nIssues - Junction_Control column has, among others, two values “Auto traffic sigl” and “Auto traffic signal” of which the former is a misspelling of the latter\n\nInstances where “Fatal” is misspelled as “Fetal” in Accident_Severity column\n“Time” column was represented as a datetime data type in Power BI\n\nFixes - Used the Replace Values feature to correct the misspellings of Issues 1 & 2 - Changed the data type to time accordingly - All fixes were done in Power Query\n ### Data Processing ⚙️\n\nThe data needs a calendar table so as to use Time Intelligence functions further down the line.\nUsing the CALENDAR function, a new table with calculated, dynamic columns of Date, Month and Year was generated.\n ### Data Modeling 🏛\na one-to-many active relationship was established between the calendar table and the data table \n ### Data Visualization 🎨\nPower BI magic!✨ The report pbix file is available in this repo to explore design decisions The following DAX measures were used:\n\nCount of Accidents = DISTINCTCOUNT(Data[Accident_Index])\n\nCurrent Year Accident Count = TOTALYTD(COUNT(Data[Accident_Index]),'Calendar'[Date])\n\nCurrent Year Casualties = TOTALYTD(SUM(Data[Number_of_Casualties]),'Calendar'[Date])\n\nPrevious Year Accident Counts = CALCULATE(COUNT(Data[Accident_Index]), SAMEPERIODLASTYEAR('Calendar'[Date]))\n\nPrevious Year Casualties = CALCULATE(SUM(Data[Number_of_Casualties]),\nSAMEPERIODLASTYEAR('Calendar'[Date]))\n\nYoY Accident Count = DIVIDE([Current Year Accident Count] - [Previous Year Accident Counts], [Previous Year Accident Counts])\n\nYoY Casualties = (DIVIDE([Current Year Casualties] - [Previous Year Casualties],[Previous Year Casualties]))\n\n\n\nDeriving Insights\n\nThe vehicle type most involved in accidents was cars. This can be explained by the fact that most vehicles plying UK roads are cars\nThere is a general decrease in casualty counts in 2022 compared to 2021"
  },
  {
    "objectID": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#contact",
    "href": "posts/2022_05_28_UK-accidents/UK Road Accidents.html#contact",
    "title": "Chamber of (data) secrets",
    "section": "Contact",
    "text": "Contact\nEdun Joshua Olubori - connect on linkedin - joshuaolubori@gmail.com\nLive Demo: here\n\n(back to top)"
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "",
    "text": "Live Demo: here\n\n\n\n\n\n\n\n   \n\n\n\n\n\nThe year 2020 will be remembered for the global impact of a virus that had infected over 40 million people by late October. While drawing comparisons to the 1918 flu pandemic, one notable difference is the role of technology in providing real-time insights into the spread, mutations, and economic effects of the virus.\nThis enhanced understanding of COVID-19 is made possible by vast amounts of data, including hundreds of genomes and millions of tweets, coupled with advancements in computing power and analytical methods. (Credits Katherine Unger Baillie et al., 2020)1\nThe Nigeria Centre for Disease Control (NCDC) and the National Emergency Operations Centre (EOC) have been at the forefront of Nigeria’s public health response, working in coordination with the Presidential Task Force on COVID-19 (PTF-COVID-19). The NCDC also collaborates closely with all states of the Federation to support their pandemic response efforts. (NCDC coronavirus COVID-19 advisory, 2022)2\nGiven these circumstances, data collection and analysis are vital for informing appropriate responses, planning, and resource allocation.\nThis interactive dashboard provides information on the COVID-19 pandemic in Nigeria, offering insights into the disease and its impact on the Nigerian population.This is an interactive dashboard on the COVID-19 pandemic in Nigeria, reflecting information emerging on the disease and its impact on the Nigerian population."
  },
  {
    "objectID": "posts/2022_05_28_COVID-NGA-analysis/index.html#about-the-project",
    "href": "posts/2022_05_28_COVID-NGA-analysis/index.html#about-the-project",
    "title": "Visualising COVID-19’s Impact in Nigeria",
    "section": "",
    "text": "Live Demo: here\n\n\n\n\n\n\n\n   \n\n\n\n\n\nThe year 2020 will be remembered for the global impact of a virus that had infected over 40 million people by late October. While drawing comparisons to the 1918 flu pandemic, one notable difference is the role of technology in providing real-time insights into the spread, mutations, and economic effects of the virus.\nThis enhanced understanding of COVID-19 is made possible by vast amounts of data, including hundreds of genomes and millions of tweets, coupled with advancements in computing power and analytical methods. (Credits Katherine Unger Baillie et al., 2020)1\nThe Nigeria Centre for Disease Control (NCDC) and the National Emergency Operations Centre (EOC) have been at the forefront of Nigeria’s public health response, working in coordination with the Presidential Task Force on COVID-19 (PTF-COVID-19). The NCDC also collaborates closely with all states of the Federation to support their pandemic response efforts. (NCDC coronavirus COVID-19 advisory, 2022)2\nGiven these circumstances, data collection and analysis are vital for informing appropriate responses, planning, and resource allocation.\nThis interactive dashboard provides information on the COVID-19 pandemic in Nigeria, offering insights into the disease and its impact on the Nigerian population.This is an interactive dashboard on the COVID-19 pandemic in Nigeria, reflecting information emerging on the disease and its impact on the Nigerian population."
  },
  {
    "objectID": "posts/2023_03_23_toptal-analytics/index.html",
    "href": "posts/2023_03_23_toptal-analytics/index.html",
    "title": "Leads Acquisition Analysis",
    "section": "",
    "text": "Live Demo: here\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\nAssume that you are partnering with Toptal’s Growth team; a highly analytical, experimental team that is responsible for growing Toptal’s revenue by scaling and optimizing client acquisition channels. The growth team wants your help in measuring and reporting on their business performance.\nFor this exercise, 2 datasets are provided. GrowthOperations.csv consists of all the leads that Toptal’s Growth Team has obtained over the past 48 weeks. GrowthSpend.csv shows how much the Growth Team has spent each week on each client acquisition channel over the past 48 weeks.\n\n\n\n\nChoose appropriate KPIs you recommend the Growth team use to measure their performance.\nDesign a dashboard that the Growth team can use to regularly monitor performance You can choose to use SQL, Microsoft Excel, Google Sheets, or any data manipulation tool to analyze the data. But you need to use Tableau to build the dashboard. ○ The purpose of the exercise is to choose and define meaningful KPIs and insights, to be presented in an effective design, using Tableau as a tool. ○ Feel free to include any additional documents or sheets you create to analyze the data, in addition to the Tableau file.\nPrepare slide for a presentation on the following: ● An Executive Summary on the Growth team performance ○ Include specific recommendations for improving performance. Why did you make these recommendations?\n\n● A more detailed dive into Growth’s team performance over the past 48 weeks: ○ Identify and discuss the most problematic channel. Why is it the most problematic? ○ Are there major shifts in performance over time? ○ In what ways does lead priority predict the way in which leads will move through the funnel? ● Definitions of the KPIs you have chosen and explain why you chose them ● Screenshots of your dashboard design showing overall layout and functionality ● A list of any assumptions you made and the justification of why you think they are reasonable\n\n\n\n● Lead: A company that has signed up on Toptal.com and is interested in learning more about Toptal. ● Verified: A company that has provided us their billing information after having a call with a sales representative. Treat this as a strong indicator that they are going to convert into a paying client. This is the primary metric that the Growth Team is responsible for on a weekly basis. ● Lead Priority: A priority assigned to each lead according to a machine learning algorithm that indicates how likely the lead is to become a Verified, based on Toptal’s historical data. Leads are treated differently by the sales team according to which priority they are assigned. ● Time Until Verification: The time from when a company signs up as a Lead until it turns into a Verified. ● Channel: The client acquisition channel through which this company found out about Toptal (i.e. How they ended up on Toptal.com). ● Display: Leads coming in from display advertising. For example, the Google Display Network. ● AdWords: Leads coming in from Google AdWords, a paid search channel. ● Organic: Leads coming in from Google organic (unpaid) search results. Since this is not an advertising channel, the weekly spend here is always $0. ● LinkedIn: Leads coming in from the LinkedIn advertising platform.\nThe visual can be viewed and interacted with at link\nThe link to the slides can be found here"
  },
  {
    "objectID": "posts/2023_03_23_toptal-analytics/index.html#about-the-project",
    "href": "posts/2023_03_23_toptal-analytics/index.html#about-the-project",
    "title": "Leads Acquisition Analysis",
    "section": "",
    "text": "Live Demo: here\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\nAssume that you are partnering with Toptal’s Growth team; a highly analytical, experimental team that is responsible for growing Toptal’s revenue by scaling and optimizing client acquisition channels. The growth team wants your help in measuring and reporting on their business performance.\nFor this exercise, 2 datasets are provided. GrowthOperations.csv consists of all the leads that Toptal’s Growth Team has obtained over the past 48 weeks. GrowthSpend.csv shows how much the Growth Team has spent each week on each client acquisition channel over the past 48 weeks.\n\n\n\n\nChoose appropriate KPIs you recommend the Growth team use to measure their performance.\nDesign a dashboard that the Growth team can use to regularly monitor performance You can choose to use SQL, Microsoft Excel, Google Sheets, or any data manipulation tool to analyze the data. But you need to use Tableau to build the dashboard. ○ The purpose of the exercise is to choose and define meaningful KPIs and insights, to be presented in an effective design, using Tableau as a tool. ○ Feel free to include any additional documents or sheets you create to analyze the data, in addition to the Tableau file.\nPrepare slide for a presentation on the following: ● An Executive Summary on the Growth team performance ○ Include specific recommendations for improving performance. Why did you make these recommendations?\n\n● A more detailed dive into Growth’s team performance over the past 48 weeks: ○ Identify and discuss the most problematic channel. Why is it the most problematic? ○ Are there major shifts in performance over time? ○ In what ways does lead priority predict the way in which leads will move through the funnel? ● Definitions of the KPIs you have chosen and explain why you chose them ● Screenshots of your dashboard design showing overall layout and functionality ● A list of any assumptions you made and the justification of why you think they are reasonable\n\n\n\n● Lead: A company that has signed up on Toptal.com and is interested in learning more about Toptal. ● Verified: A company that has provided us their billing information after having a call with a sales representative. Treat this as a strong indicator that they are going to convert into a paying client. This is the primary metric that the Growth Team is responsible for on a weekly basis. ● Lead Priority: A priority assigned to each lead according to a machine learning algorithm that indicates how likely the lead is to become a Verified, based on Toptal’s historical data. Leads are treated differently by the sales team according to which priority they are assigned. ● Time Until Verification: The time from when a company signs up as a Lead until it turns into a Verified. ● Channel: The client acquisition channel through which this company found out about Toptal (i.e. How they ended up on Toptal.com). ● Display: Leads coming in from display advertising. For example, the Google Display Network. ● AdWords: Leads coming in from Google AdWords, a paid search channel. ● Organic: Leads coming in from Google organic (unpaid) search results. Since this is not an advertising channel, the weekly spend here is always $0. ● LinkedIn: Leads coming in from the LinkedIn advertising platform.\nThe visual can be viewed and interacted with at link\nThe link to the slides can be found here"
  },
  {
    "objectID": "posts/2023_03_23_toptal-analytics/index.html#contact",
    "href": "posts/2023_03_23_toptal-analytics/index.html#contact",
    "title": "Leads Acquisition Analysis",
    "section": "Contact",
    "text": "Contact\nEdun Joshua Olubori - connect on linkedin - joshuaolubori@gmail.com"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "",
    "text": "The data comes from the FBI’s National Instant Criminal Background Check System. The NICS is used by to determine whether a prospective buyer is eligible to buy firearms or explosives. Gun shops call into this system to ensure that each customer does not have a criminal record or isn’t otherwise ineligible to make a purchase. The data has been supplemented with state level data from census.gov.\n\nMandated by the Brady Handgun Violence Prevention Act of 1993 and launched by the FBI on November 30, 1998, NICS is used by Federal Firearms Licensees (FFLs) to instantly determine whether a prospective buyer is eligible to buy firearms or explosives. Before ringing up the sale, cashiers call in a check to the FBI or to other designated agencies to ensure that each customer does not have a criminal record or isn’t otherwise ineligible to make a purchase. More than 100 million such checks have been made in the last decade, leading to more than 700,000 denials.\n\nThese statistics represent the number of firearm background checks initiated through the NICS. They do not represent the number of firearms sold. According to The Trace, 2015: The FBI’s background check numbers come with caveats: As seen in the late February-early March 2014 bubble, many checks are for concealed carry permits, not actual gun sales. Kentucky runs a new check on each concealed carry license holder each month. And of course, the FBI’s numbers don’t include private gun sales, many of which do not require a background check. Despite those vagaries, the FBI’s NICS numbers are widely accepted as the best proxy for total gun sales in a given time period.\nIn 2015, New York Times ran a similar analysis with the data and estimated sales from background checks with a method suggested by a researcher.\nAccording to them:\n\nTo convert background checks into estimated sales, we relied on a method suggested in the Small Arms Survey by Jurgen Brauer, a professor at Georgia Regents University. Each long gun and handgun check was counted as 1.1 sales. Each multiple-gun check was counted as two sales. Permit checks and other types of checks were omitted. The multiplier is an estimate based on Mr. Brauer’s interviews with gun shop owners.\n\nThis is the method I applied in this analysis.\nThe NICS is in a wide format and contains a total of 23 variables, out of which I deem only 5 is relevant to this analysis:\n\nmonth (monthly data from November 1998 to September 2017)\nstate (55 U.S. states and offshore territories)\nhandgun (number of check calls)\nlong-gun (number of check calls)\nmultiple (number of check calls)\n\nThe census data is a wide-table format data with 66 variables measuring quantities for 50 U.S. states. I have selected only 18 variables that I deem relevant to this analysis. The starred variables are just a few of the variables that are present in the raw data that I will not include in this analysis.\n\n\n\n\n\n\n\ns/n\nVariable\n\n\n\n\n1\nPopulation estimates, July 1, 2016, (V2016)\n\n\n2\nPersons under 18 years, percent, July 1, 2016, (V2016)\n\n\n3\nPersons 65 years and over, percent, July 1, 2016,(V2016)\n\n\n4\nFemale persons, percent, July 1, 2016, (V2016)\n\n\n5\nWhite alone, percent, July 1, 2016, (V2016)\n\n\n6\nBlack or African American alone, percent, July 1, 2016, (V2016)\n\n\n7\nAsian alone, percent, July 1, 2016, (V2016)\n\n\n8\nMedian value of owner-occupied housing units, 2011-2015\n\n\n9\nMedian gross rent, 2011-2015\n\n\n10\nPersons per household, 2011-2015\n\n\n11\nHigh school graduate or higher, percent of persons age 25 years+, 2011-2015\n\n\n12\nBachelor’s degree or higher, percent of persons age 25 years+, 2011-2015\n\n\n13\nPersons without health insurance, under age 65 years, percent\n\n\n14\nMedian household income (in 2015 dollars), 2011-2015\n\n\n15\nPer capita income in past 12 months (in 2015 dollars), 2011-2015\n\n\n16\nPersons in poverty, percent\n\n\n17\nAll firms, 2012\n\n\n*\nPersons under 5 years, percent, July 1, 2016, (V2016)\n\n\n*\nMinority-owned firms, 2012\n\n\n*\nPopulation per square mile, 2010\n\n\n*\nTotal employer establishments, 2015\n\n\n*\nTotal employment, 2015\n\n\n*\nTotal annual payroll, 2015 ($1,000)\n\n\n*\nTotal employment, percent change, 2014-2015\n\n\n*\nTotal nonemployer establishments, 2015\n\n\n\nThe 18th variable is state but in the raw data, it would have to be created by transposing the dataframe. The starred variables are just some of the ones I deem irrelevant."
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#introduction",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#introduction",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "",
    "text": "The data comes from the FBI’s National Instant Criminal Background Check System. The NICS is used by to determine whether a prospective buyer is eligible to buy firearms or explosives. Gun shops call into this system to ensure that each customer does not have a criminal record or isn’t otherwise ineligible to make a purchase. The data has been supplemented with state level data from census.gov.\n\nMandated by the Brady Handgun Violence Prevention Act of 1993 and launched by the FBI on November 30, 1998, NICS is used by Federal Firearms Licensees (FFLs) to instantly determine whether a prospective buyer is eligible to buy firearms or explosives. Before ringing up the sale, cashiers call in a check to the FBI or to other designated agencies to ensure that each customer does not have a criminal record or isn’t otherwise ineligible to make a purchase. More than 100 million such checks have been made in the last decade, leading to more than 700,000 denials.\n\nThese statistics represent the number of firearm background checks initiated through the NICS. They do not represent the number of firearms sold. According to The Trace, 2015: The FBI’s background check numbers come with caveats: As seen in the late February-early March 2014 bubble, many checks are for concealed carry permits, not actual gun sales. Kentucky runs a new check on each concealed carry license holder each month. And of course, the FBI’s numbers don’t include private gun sales, many of which do not require a background check. Despite those vagaries, the FBI’s NICS numbers are widely accepted as the best proxy for total gun sales in a given time period.\nIn 2015, New York Times ran a similar analysis with the data and estimated sales from background checks with a method suggested by a researcher.\nAccording to them:\n\nTo convert background checks into estimated sales, we relied on a method suggested in the Small Arms Survey by Jurgen Brauer, a professor at Georgia Regents University. Each long gun and handgun check was counted as 1.1 sales. Each multiple-gun check was counted as two sales. Permit checks and other types of checks were omitted. The multiplier is an estimate based on Mr. Brauer’s interviews with gun shop owners.\n\nThis is the method I applied in this analysis.\nThe NICS is in a wide format and contains a total of 23 variables, out of which I deem only 5 is relevant to this analysis:\n\nmonth (monthly data from November 1998 to September 2017)\nstate (55 U.S. states and offshore territories)\nhandgun (number of check calls)\nlong-gun (number of check calls)\nmultiple (number of check calls)\n\nThe census data is a wide-table format data with 66 variables measuring quantities for 50 U.S. states. I have selected only 18 variables that I deem relevant to this analysis. The starred variables are just a few of the variables that are present in the raw data that I will not include in this analysis.\n\n\n\n\n\n\n\ns/n\nVariable\n\n\n\n\n1\nPopulation estimates, July 1, 2016, (V2016)\n\n\n2\nPersons under 18 years, percent, July 1, 2016, (V2016)\n\n\n3\nPersons 65 years and over, percent, July 1, 2016,(V2016)\n\n\n4\nFemale persons, percent, July 1, 2016, (V2016)\n\n\n5\nWhite alone, percent, July 1, 2016, (V2016)\n\n\n6\nBlack or African American alone, percent, July 1, 2016, (V2016)\n\n\n7\nAsian alone, percent, July 1, 2016, (V2016)\n\n\n8\nMedian value of owner-occupied housing units, 2011-2015\n\n\n9\nMedian gross rent, 2011-2015\n\n\n10\nPersons per household, 2011-2015\n\n\n11\nHigh school graduate or higher, percent of persons age 25 years+, 2011-2015\n\n\n12\nBachelor’s degree or higher, percent of persons age 25 years+, 2011-2015\n\n\n13\nPersons without health insurance, under age 65 years, percent\n\n\n14\nMedian household income (in 2015 dollars), 2011-2015\n\n\n15\nPer capita income in past 12 months (in 2015 dollars), 2011-2015\n\n\n16\nPersons in poverty, percent\n\n\n17\nAll firms, 2012\n\n\n*\nPersons under 5 years, percent, July 1, 2016, (V2016)\n\n\n*\nMinority-owned firms, 2012\n\n\n*\nPopulation per square mile, 2010\n\n\n*\nTotal employer establishments, 2015\n\n\n*\nTotal employment, 2015\n\n\n*\nTotal annual payroll, 2015 ($1,000)\n\n\n*\nTotal employment, percent change, 2014-2015\n\n\n*\nTotal nonemployer establishments, 2015\n\n\n\nThe 18th variable is state but in the raw data, it would have to be created by transposing the dataframe. The starred variables are just some of the ones I deem irrelevant."
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#research-questions",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#research-questions",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Research Questions",
    "text": "Research Questions\nIn this analysis, I will answer the following research questions:\n\n\nHow many guns were sold in the U.S. in selected years?\nWhat kind of gun was bought the most in those years?\nWhat are the overall monthly trends of gun purchases in those years?\n\n\n\n\nWhat are the top 5 states in terms of gun purchases in the U.S. in those years?\nWhat are the most bought gun types in those states?\nWhat kind of gun was bought the most in one of those states?\nWhat are the overall monthly trends of gun purchases in those states?\nWhich states had the highest growth in gun sales over the span of a decade?"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#data-wrangling-a",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#data-wrangling-a",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Data Wrangling (a)",
    "text": "Data Wrangling (a)\n\nGeneral Properties\nLoading and inspecting the census data\n\n# Importing packages.\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt, seaborn as sns\n#% matplotlib inline\n\ndf_census_raw = pd.read_csv('Data/US_Census_Data.csv')\ndf_census_raw.head()\n\n\n\n\n\n\n\n\nFact\nFact Note\nAlabama\nAlaska\nArizona\nArkansas\nCalifornia\nColorado\nConnecticut\nDelaware\n...\nSouth Dakota\nTennessee\nTexas\nUtah\nVermont\nVirginia\nWashington\nWest Virginia\nWisconsin\nWyoming\n\n\n\n\n0\nPopulation estimates, July 1, 2016, (V2016)\nNaN\n4,863,300\n741,894\n6,931,071\n2,988,248\n39,250,017\n5,540,545\n3,576,452\n952,065\n...\n865454\n6651194\n27,862,596\n3,051,217\n624,594\n8,411,808\n7,288,000\n1,831,102\n5,778,708\n585,501\n\n\n1\nPopulation estimates base, April 1, 2010, (V2...\nNaN\n4,780,131\n710,249\n6,392,301\n2,916,025\n37,254,522\n5,029,324\n3,574,114\n897,936\n...\n814195\n6346298\n25,146,100\n2,763,888\n625,741\n8,001,041\n6,724,545\n1,853,011\n5,687,289\n563,767\n\n\n2\nPopulation, percent change - April 1, 2010 (es...\nNaN\n1.70%\n4.50%\n8.40%\n2.50%\n5.40%\n10.20%\n0.10%\n6.00%\n...\n0.063\n0.048\n10.80%\n10.40%\n-0.20%\n5.10%\n8.40%\n-1.20%\n1.60%\n3.90%\n\n\n3\nPopulation, Census, April 1, 2010\nNaN\n4,779,736\n710,231\n6,392,017\n2,915,918\n37,253,956\n5,029,196\n3,574,097\n897,934\n...\n814180\n6346105\n25,145,561\n2,763,885\n625,741\n8,001,024\n6,724,540\n1,852,994\n5,686,986\n563,626\n\n\n4\nPersons under 5 years, percent, July 1, 2016, ...\nNaN\n6.00%\n7.30%\n6.30%\n6.40%\n6.30%\n6.10%\n5.20%\n5.80%\n...\n0.071\n0.061\n7.20%\n8.30%\n4.90%\n6.10%\n6.20%\n5.50%\n5.80%\n6.50%\n\n\n\n\n5 rows × 52 columns\n\n\n\n\n# shape of the dataframe\ndf_census_raw.shape\n\n(85, 52)\n\n\n\n# viewing all columns in the dataframe\ndf_census_raw.columns\n\nIndex(['Fact', 'Fact Note', 'Alabama', 'Alaska', 'Arizona', 'Arkansas',\n       'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida',\n       'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas',\n       'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts',\n       'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana',\n       'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico',\n       'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma',\n       'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n       'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia',\n       'Washington', 'West Virginia', 'Wisconsin', 'Wyoming'],\n      dtype='object')\n\n\n\nDropping Irrelevant Columns\nIn this section, I will drop all irrelevant columns from the census dataset, reshape the dataframe and then rename columns for clarity\n\n# dropping the 'Fact Note' column\ndf_census_raw.drop('Fact Note', axis=1, inplace=True)\ndf_census_raw.head()\n\n\n\n\n\n\n\n\nFact\nAlabama\nAlaska\nArizona\nArkansas\nCalifornia\nColorado\nConnecticut\nDelaware\nFlorida\n...\nSouth Dakota\nTennessee\nTexas\nUtah\nVermont\nVirginia\nWashington\nWest Virginia\nWisconsin\nWyoming\n\n\n\n\n0\nPopulation estimates, July 1, 2016, (V2016)\n4,863,300\n741,894\n6,931,071\n2,988,248\n39,250,017\n5,540,545\n3,576,452\n952,065\n20,612,439\n...\n865454\n6651194\n27,862,596\n3,051,217\n624,594\n8,411,808\n7,288,000\n1,831,102\n5,778,708\n585,501\n\n\n1\nPopulation estimates base, April 1, 2010, (V2...\n4,780,131\n710,249\n6,392,301\n2,916,025\n37,254,522\n5,029,324\n3,574,114\n897,936\n18,804,592\n...\n814195\n6346298\n25,146,100\n2,763,888\n625,741\n8,001,041\n6,724,545\n1,853,011\n5,687,289\n563,767\n\n\n2\nPopulation, percent change - April 1, 2010 (es...\n1.70%\n4.50%\n8.40%\n2.50%\n5.40%\n10.20%\n0.10%\n6.00%\n9.60%\n...\n0.063\n0.048\n10.80%\n10.40%\n-0.20%\n5.10%\n8.40%\n-1.20%\n1.60%\n3.90%\n\n\n3\nPopulation, Census, April 1, 2010\n4,779,736\n710,231\n6,392,017\n2,915,918\n37,253,956\n5,029,196\n3,574,097\n897,934\n18,801,310\n...\n814180\n6346105\n25,145,561\n2,763,885\n625,741\n8,001,024\n6,724,540\n1,852,994\n5,686,986\n563,626\n\n\n4\nPersons under 5 years, percent, July 1, 2016, ...\n6.00%\n7.30%\n6.30%\n6.40%\n6.30%\n6.10%\n5.20%\n5.80%\n5.50%\n...\n0.071\n0.061\n7.20%\n8.30%\n4.90%\n6.10%\n6.20%\n5.50%\n5.80%\n6.50%\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n# reshaping the dataframe\ndf_T = df_census_raw.transpose(copy=False)\n\n\n# observing the result\ndf_T.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n\n\n\n\nFact\nPopulation estimates, July 1, 2016, (V2016)\nPopulation estimates base, April 1, 2010, (V2...\nPopulation, percent change - April 1, 2010 (es...\nPopulation, Census, April 1, 2010\nPersons under 5 years, percent, July 1, 2016, ...\nPersons under 5 years, percent, April 1, 2010\nPersons under 18 years, percent, July 1, 2016,...\nPersons under 18 years, percent, April 1, 2010\nPersons 65 years and over, percent, July 1, 2...\nPersons 65 years and over, percent, April 1, 2010\n...\nNaN\nValue Flags\n-\nD\nF\nFN\nNaN\nS\nX\nZ\n\n\nAlabama\n4,863,300\n4,780,131\n1.70%\n4,779,736\n6.00%\n6.40%\n22.60%\n23.70%\n16.10%\n13.80%\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAlaska\n741,894\n710,249\n4.50%\n710,231\n7.30%\n7.60%\n25.20%\n26.40%\n10.40%\n7.70%\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nArizona\n6,931,071\n6,392,301\n8.40%\n6,392,017\n6.30%\n7.10%\n23.50%\n25.50%\n16.90%\n13.80%\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nArkansas\n2,988,248\n2,916,025\n2.50%\n2,915,918\n6.40%\n6.80%\n23.60%\n24.40%\n16.30%\n14.40%\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 85 columns\n\n\n\n\n# checking out the new shape\ndf_T.shape\n\n(51, 85)\n\n\n\n# selecting the columns of interest\ndf_T = df_T.iloc[:, 0:65]\ndf_T.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n\n\n\n\nFact\nPopulation estimates, July 1, 2016, (V2016)\nPopulation estimates base, April 1, 2010, (V2...\nPopulation, percent change - April 1, 2010 (es...\nPopulation, Census, April 1, 2010\nPersons under 5 years, percent, July 1, 2016, ...\nPersons under 5 years, percent, April 1, 2010\nPersons under 18 years, percent, July 1, 2016,...\nPersons under 18 years, percent, April 1, 2010\nPersons 65 years and over, percent, July 1, 2...\nPersons 65 years and over, percent, April 1, 2010\n...\nAll firms, 2012\nMen-owned firms, 2012\nWomen-owned firms, 2012\nMinority-owned firms, 2012\nNonminority-owned firms, 2012\nVeteran-owned firms, 2012\nNonveteran-owned firms, 2012\nPopulation per square mile, 2010\nLand area in square miles, 2010\nFIPS Code\n\n\nAlabama\n4,863,300\n4,780,131\n1.70%\n4,779,736\n6.00%\n6.40%\n22.60%\n23.70%\n16.10%\n13.80%\n...\n374,153\n203,604\n137,630\n92,219\n272,651\n41,943\n316,984\n94.4\n50,645.33\n\"01\"\n\n\nAlaska\n741,894\n710,249\n4.50%\n710,231\n7.30%\n7.60%\n25.20%\n26.40%\n10.40%\n7.70%\n...\n68,032\n35,402\n22,141\n13,688\n51,147\n7,953\n56,091\n1.2\n570,640.95\n\"02\"\n\n\nArizona\n6,931,071\n6,392,301\n8.40%\n6,392,017\n6.30%\n7.10%\n23.50%\n25.50%\n16.90%\n13.80%\n...\n499,926\n245,243\n182,425\n135,313\n344,981\n46,780\n427,582\n56.3\n113,594.08\n\"04\"\n\n\nArkansas\n2,988,248\n2,916,025\n2.50%\n2,915,918\n6.40%\n6.80%\n23.60%\n24.40%\n16.30%\n14.40%\n...\n231,959\n123,158\n75,962\n35,982\n189,029\n25,915\n192,988\n56\n52,035.48\n\"05\"\n\n\n\n\n5 rows × 65 columns\n\n\n\n\ncolumns = df_T.iloc[0, :]\ndf_T.rename(columns=columns, inplace = True)\ndf_T.head()\n\n\n\n\n\n\n\n\nPopulation estimates, July 1, 2016, (V2016)\nPopulation estimates base, April 1, 2010, (V2016)\nPopulation, percent change - April 1, 2010 (estimates base) to July 1, 2016, (V2016)\nPopulation, Census, April 1, 2010\nPersons under 5 years, percent, July 1, 2016, (V2016)\nPersons under 5 years, percent, April 1, 2010\nPersons under 18 years, percent, July 1, 2016, (V2016)\nPersons under 18 years, percent, April 1, 2010\nPersons 65 years and over, percent, July 1, 2016, (V2016)\nPersons 65 years and over, percent, April 1, 2010\n...\nAll firms, 2012\nMen-owned firms, 2012\nWomen-owned firms, 2012\nMinority-owned firms, 2012\nNonminority-owned firms, 2012\nVeteran-owned firms, 2012\nNonveteran-owned firms, 2012\nPopulation per square mile, 2010\nLand area in square miles, 2010\nFIPS Code\n\n\n\n\nFact\nPopulation estimates, July 1, 2016, (V2016)\nPopulation estimates base, April 1, 2010, (V2...\nPopulation, percent change - April 1, 2010 (es...\nPopulation, Census, April 1, 2010\nPersons under 5 years, percent, July 1, 2016, ...\nPersons under 5 years, percent, April 1, 2010\nPersons under 18 years, percent, July 1, 2016,...\nPersons under 18 years, percent, April 1, 2010\nPersons 65 years and over, percent, July 1, 2...\nPersons 65 years and over, percent, April 1, 2010\n...\nAll firms, 2012\nMen-owned firms, 2012\nWomen-owned firms, 2012\nMinority-owned firms, 2012\nNonminority-owned firms, 2012\nVeteran-owned firms, 2012\nNonveteran-owned firms, 2012\nPopulation per square mile, 2010\nLand area in square miles, 2010\nFIPS Code\n\n\nAlabama\n4,863,300\n4,780,131\n1.70%\n4,779,736\n6.00%\n6.40%\n22.60%\n23.70%\n16.10%\n13.80%\n...\n374,153\n203,604\n137,630\n92,219\n272,651\n41,943\n316,984\n94.4\n50,645.33\n\"01\"\n\n\nAlaska\n741,894\n710,249\n4.50%\n710,231\n7.30%\n7.60%\n25.20%\n26.40%\n10.40%\n7.70%\n...\n68,032\n35,402\n22,141\n13,688\n51,147\n7,953\n56,091\n1.2\n570,640.95\n\"02\"\n\n\nArizona\n6,931,071\n6,392,301\n8.40%\n6,392,017\n6.30%\n7.10%\n23.50%\n25.50%\n16.90%\n13.80%\n...\n499,926\n245,243\n182,425\n135,313\n344,981\n46,780\n427,582\n56.3\n113,594.08\n\"04\"\n\n\nArkansas\n2,988,248\n2,916,025\n2.50%\n2,915,918\n6.40%\n6.80%\n23.60%\n24.40%\n16.30%\n14.40%\n...\n231,959\n123,158\n75,962\n35,982\n189,029\n25,915\n192,988\n56\n52,035.48\n\"05\"\n\n\n\n\n5 rows × 65 columns\n\n\n\n\n# dropping redundant column index\ndf_T.drop('Fact', axis=0, inplace=True)\ndf_T.head()\n\n\n\n\n\n\n\n\nPopulation estimates, July 1, 2016, (V2016)\nPopulation estimates base, April 1, 2010, (V2016)\nPopulation, percent change - April 1, 2010 (estimates base) to July 1, 2016, (V2016)\nPopulation, Census, April 1, 2010\nPersons under 5 years, percent, July 1, 2016, (V2016)\nPersons under 5 years, percent, April 1, 2010\nPersons under 18 years, percent, July 1, 2016, (V2016)\nPersons under 18 years, percent, April 1, 2010\nPersons 65 years and over, percent, July 1, 2016, (V2016)\nPersons 65 years and over, percent, April 1, 2010\n...\nAll firms, 2012\nMen-owned firms, 2012\nWomen-owned firms, 2012\nMinority-owned firms, 2012\nNonminority-owned firms, 2012\nVeteran-owned firms, 2012\nNonveteran-owned firms, 2012\nPopulation per square mile, 2010\nLand area in square miles, 2010\nFIPS Code\n\n\n\n\nAlabama\n4,863,300\n4,780,131\n1.70%\n4,779,736\n6.00%\n6.40%\n22.60%\n23.70%\n16.10%\n13.80%\n...\n374,153\n203,604\n137,630\n92,219\n272,651\n41,943\n316,984\n94.4\n50,645.33\n\"01\"\n\n\nAlaska\n741,894\n710,249\n4.50%\n710,231\n7.30%\n7.60%\n25.20%\n26.40%\n10.40%\n7.70%\n...\n68,032\n35,402\n22,141\n13,688\n51,147\n7,953\n56,091\n1.2\n570,640.95\n\"02\"\n\n\nArizona\n6,931,071\n6,392,301\n8.40%\n6,392,017\n6.30%\n7.10%\n23.50%\n25.50%\n16.90%\n13.80%\n...\n499,926\n245,243\n182,425\n135,313\n344,981\n46,780\n427,582\n56.3\n113,594.08\n\"04\"\n\n\nArkansas\n2,988,248\n2,916,025\n2.50%\n2,915,918\n6.40%\n6.80%\n23.60%\n24.40%\n16.30%\n14.40%\n...\n231,959\n123,158\n75,962\n35,982\n189,029\n25,915\n192,988\n56\n52,035.48\n\"05\"\n\n\nCalifornia\n39,250,017\n37,254,522\n5.40%\n37,253,956\n6.30%\n6.80%\n23.20%\n25.00%\n13.60%\n11.40%\n...\n3,548,449\n1,852,580\n1,320,085\n1,619,857\n1,819,107\n252,377\n3,176,341\n239.1\n155,779.22\n\"06\"\n\n\n\n\n5 rows × 65 columns\n\n\n\n\n# inspecting the shape of the reshaped dataframe\ndf_T.shape\n\n(50, 65)\n\n\n\n# inspecting all states present in the census data\nstate = df_T.index\nstate\n\nIndex(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado',\n       'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho',\n       'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana',\n       'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n       'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada',\n       'New Hampshire', 'New Jersey', 'New Mexico', 'New York',\n       'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon',\n       'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota',\n       'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n       'West Virginia', 'Wisconsin', 'Wyoming'],\n      dtype='object')\n\n\n\n# dropping irrelevant columns\n\ndf_T.drop(columns=['Total employer establishments, 2015',\n'Total employment, 2015',\n'Total annual payroll, 2015 ($1,000)',\n'Total employment, percent change, 2014-2015',\n'Total nonemployer establishments, 2015'], inplace=True)\n\ndf_T.head()\n\n\n\n\n\n\n\n\nPopulation estimates, July 1, 2016, (V2016)\nPopulation estimates base, April 1, 2010, (V2016)\nPopulation, percent change - April 1, 2010 (estimates base) to July 1, 2016, (V2016)\nPopulation, Census, April 1, 2010\nPersons under 5 years, percent, July 1, 2016, (V2016)\nPersons under 5 years, percent, April 1, 2010\nPersons under 18 years, percent, July 1, 2016, (V2016)\nPersons under 18 years, percent, April 1, 2010\nPersons 65 years and over, percent, July 1, 2016, (V2016)\nPersons 65 years and over, percent, April 1, 2010\n...\nAll firms, 2012\nMen-owned firms, 2012\nWomen-owned firms, 2012\nMinority-owned firms, 2012\nNonminority-owned firms, 2012\nVeteran-owned firms, 2012\nNonveteran-owned firms, 2012\nPopulation per square mile, 2010\nLand area in square miles, 2010\nFIPS Code\n\n\n\n\nAlabama\n4,863,300\n4,780,131\n1.70%\n4,779,736\n6.00%\n6.40%\n22.60%\n23.70%\n16.10%\n13.80%\n...\n374,153\n203,604\n137,630\n92,219\n272,651\n41,943\n316,984\n94.4\n50,645.33\n\"01\"\n\n\nAlaska\n741,894\n710,249\n4.50%\n710,231\n7.30%\n7.60%\n25.20%\n26.40%\n10.40%\n7.70%\n...\n68,032\n35,402\n22,141\n13,688\n51,147\n7,953\n56,091\n1.2\n570,640.95\n\"02\"\n\n\nArizona\n6,931,071\n6,392,301\n8.40%\n6,392,017\n6.30%\n7.10%\n23.50%\n25.50%\n16.90%\n13.80%\n...\n499,926\n245,243\n182,425\n135,313\n344,981\n46,780\n427,582\n56.3\n113,594.08\n\"04\"\n\n\nArkansas\n2,988,248\n2,916,025\n2.50%\n2,915,918\n6.40%\n6.80%\n23.60%\n24.40%\n16.30%\n14.40%\n...\n231,959\n123,158\n75,962\n35,982\n189,029\n25,915\n192,988\n56\n52,035.48\n\"05\"\n\n\nCalifornia\n39,250,017\n37,254,522\n5.40%\n37,253,956\n6.30%\n6.80%\n23.20%\n25.00%\n13.60%\n11.40%\n...\n3,548,449\n1,852,580\n1,320,085\n1,619,857\n1,819,107\n252,377\n3,176,341\n239.1\n155,779.22\n\"06\"\n\n\n\n\n5 rows × 60 columns\n\n\n\n\n# dropping more irrelevant columns\n\ndf_T.drop(columns=['Men-owned firms, 2012',\n'Women-owned firms, 2012',\n'Minority-owned firms, 2012',\n'Nonminority-owned firms, 2012',\n'Veteran-owned firms, 2012',\n'Nonveteran-owned firms, 2012',\n'Population per square mile, 2010',\n'Land area in square miles, 2010',\n'FIPS Code',\n'In civilian labor force, total, percent of population age 16 years+, 2011-2015',\n'In civilian labor force, female, percent of population age 16 years+, 2011-2015',\n'Total accommodation and food services sales, 2012 ($1,000)',\n'Total health care and social assistance receipts/revenue, 2012 ($1,000)',\n'Total manufacturers shipments, 2012 ($1,000)',\n'Total merchant wholesaler sales, 2012 ($1,000)',\n'Total retail sales, 2012 ($1,000)',\n'Total retail sales per capita, 2012',\n'Mean travel time to work (minutes), workers age 16 years+, 2011-2015',\n'With a disability, under age 65 years, percent, 2011-2015',\n'Living in same house 1 year ago, percent of persons age 1 year+, 2011-2015',\n'Language other than English spoken at home, percent of persons age 5 years+, 2011-2015',\n'Building permits, 2016',\n'Households, 2011-2015',\n'Median selected monthly owner costs -with a mortgage, 2011-2015',\n'Median selected monthly owner costs -without a mortgage, 2011-2015',\n'Native Hawaiian and Other Pacific Islander alone, percent, July 1, 2016,  (V2016)',\n'Two or More Races, percent, July 1, 2016,  (V2016)',\n'Hispanic or Latino, percent, July 1, 2016,  (V2016)',\n'White alone, not Hispanic or Latino, percent, July 1, 2016,  (V2016)',\n'Veterans, 2011-2015',\n'Foreign born persons, percent, 2011-2015',\n'Housing units,  July 1, 2016,  (V2016)',\n'Housing units, April 1, 2010',\n'Owner-occupied housing unit rate, 2011-2015',\n'American Indian and Alaska Native alone, percent, July 1, 2016,  (V2016)',\n'Persons 65 years and over, percent, April 1, 2010',\n'Female persons, percent, April 1, 2010',\n'Persons under 18 years, percent, April 1, 2010',\n'Persons under 5 years, percent, April 1, 2010',\n'Population estimates base, April 1, 2010,  (V2016)',\n'Population, percent change - April 1, 2010 (estimates base) to July 1, 2016,  (V2016)',\n'Population, Census, April 1, 2010'], inplace=True)\n\ndf_T.head(3)\n\n\n\n\n\n\n\n\nPopulation estimates, July 1, 2016, (V2016)\nPersons under 5 years, percent, July 1, 2016, (V2016)\nPersons under 18 years, percent, July 1, 2016, (V2016)\nPersons 65 years and over, percent, July 1, 2016, (V2016)\nFemale persons, percent, July 1, 2016, (V2016)\nWhite alone, percent, July 1, 2016, (V2016)\nBlack or African American alone, percent, July 1, 2016, (V2016)\nAsian alone, percent, July 1, 2016, (V2016)\nMedian value of owner-occupied housing units, 2011-2015\nMedian gross rent, 2011-2015\nPersons per household, 2011-2015\nHigh school graduate or higher, percent of persons age 25 years+, 2011-2015\nBachelor's degree or higher, percent of persons age 25 years+, 2011-2015\nPersons without health insurance, under age 65 years, percent\nMedian household income (in 2015 dollars), 2011-2015\nPer capita income in past 12 months (in 2015 dollars), 2011-2015\nPersons in poverty, percent\nAll firms, 2012\n\n\n\n\nAlabama\n4,863,300\n6.00%\n22.60%\n16.10%\n51.60%\n69.30%\n26.80%\n1.40%\n$125,500\n$717\n2.55\n84.30%\n23.50%\n10.70%\n$43,623\n$24,091\n17.10%\n374,153\n\n\nAlaska\n741,894\n7.30%\n25.20%\n10.40%\n47.70%\n66.10%\n3.80%\n6.30%\n$250,000\n$1,146\n2.81\n92.10%\n28.00%\n15.50%\n$72,515\n$33,413\n9.90%\n68,032\n\n\nArizona\n6,931,071\n6.30%\n23.50%\n16.90%\n50.30%\n83.30%\n4.90%\n3.40%\n$167,500\n$913\n2.69\n86.00%\n27.50%\n11.90%\n$50,255\n$25,848\n16.40%\n499,926\n\n\n\n\n\n\n\n\n# checking dataframe\ndf_T.shape\n\n(50, 18)\n\n\n\n# checking columns\ndf_T.columns\n\nIndex(['Population estimates, July 1, 2016,  (V2016)',\n       'Persons under 5 years, percent, July 1, 2016,  (V2016)',\n       'Persons under 18 years, percent, July 1, 2016,  (V2016)',\n       'Persons 65 years and over, percent,  July 1, 2016,  (V2016)',\n       'Female persons, percent,  July 1, 2016,  (V2016)',\n       'White alone, percent, July 1, 2016,  (V2016)',\n       'Black or African American alone, percent, July 1, 2016,  (V2016)',\n       'Asian alone, percent, July 1, 2016,  (V2016)',\n       'Median value of owner-occupied housing units, 2011-2015',\n       'Median gross rent, 2011-2015', 'Persons per household, 2011-2015',\n       'High school graduate or higher, percent of persons age 25 years+, 2011-2015',\n       'Bachelor's degree or higher, percent of persons age 25 years+, 2011-2015',\n       'Persons  without health insurance, under age 65 years, percent',\n       'Median household income (in 2015 dollars), 2011-2015',\n       'Per capita income in past 12 months (in 2015 dollars), 2011-2015',\n       'Persons in poverty, percent', 'All firms, 2012'],\n      dtype='object')\n\n\n\n# renaming the columns for clarity\n\ndf_T.rename(columns={'Population estimates, July 1, 2016,  (V2016)':'pop_2016',\n'Persons under 5 years, percent, July 1, 2016,  (V2016)':'under_5',\n'Persons under 18 years, percent, July 1, 2016,  (V2016)':'under_18',\n'Persons 65 years and over, percent,  July 1, 2016,  (V2016)':'65_above',\n'Female persons, percent,  July 1, 2016,  (V2016)':'female_pct',\n'White alone, percent, July 1, 2016,  (V2016)':'white_pct',\n'Black or African American alone, percent, July 1, 2016,  (V2016)':'black_pct',\n'Asian alone, percent, July 1, 2016,  (V2016)':'asian_pct',\n'Median value of owner-occupied housing units, 2011-2015':'median_hu',\n'Median gross rent, 2011-2015':'median_gross_rent',\n'Persons per household, 2011-2015':'pe_hh',\n'High school graduate or higher, percent of persons age 25 years+, 2011-2015':'high_school_grad_higher',\n'Bachelor\\'s degree or higher, percent of persons age 25 years+, 2011-2015':'bsc_higher',\n'Persons  without health insurance, under age 65 years, percent':'pe_whi',\n'Median household income (in 2015 dollars), 2011-2015':'median_hh_income',\n'Per capita income in past 12 months (in 2015 dollars), 2011-2015':'per_cap_inc',\n'Persons in poverty, percent':'poverty_pct',\n'All firms, 2012':'firms'\n}, inplace=True)\n\ndf_T.head()\n\n\n\n\n\n\n\n\npop_2016\nunder_5\nunder_18\n65_above\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\nmedian_hu\nmedian_gross_rent\npe_hh\nhigh_school_grad_higher\nbsc_higher\npe_whi\nmedian_hh_income\nper_cap_inc\npoverty_pct\nfirms\n\n\n\n\nAlabama\n4,863,300\n6.00%\n22.60%\n16.10%\n51.60%\n69.30%\n26.80%\n1.40%\n$125,500\n$717\n2.55\n84.30%\n23.50%\n10.70%\n$43,623\n$24,091\n17.10%\n374,153\n\n\nAlaska\n741,894\n7.30%\n25.20%\n10.40%\n47.70%\n66.10%\n3.80%\n6.30%\n$250,000\n$1,146\n2.81\n92.10%\n28.00%\n15.50%\n$72,515\n$33,413\n9.90%\n68,032\n\n\nArizona\n6,931,071\n6.30%\n23.50%\n16.90%\n50.30%\n83.30%\n4.90%\n3.40%\n$167,500\n$913\n2.69\n86.00%\n27.50%\n11.90%\n$50,255\n$25,848\n16.40%\n499,926\n\n\nArkansas\n2,988,248\n6.40%\n23.60%\n16.30%\n50.90%\n79.40%\n15.70%\n1.60%\n$111,400\n$677\n2.53\n84.80%\n21.10%\n9.30%\n$41,371\n$22,798\n17.20%\n231,959\n\n\nCalifornia\n39,250,017\n6.30%\n23.20%\n13.60%\n50.30%\n72.70%\n6.50%\n14.80%\n$385,500\n$1,255\n2.96\n81.80%\n31.40%\n8.30%\n$61,818\n$30,318\n14.30%\n3,548,449\n\n\n\n\n\n\n\n\n# I subsequently decided to drop 'under_5' as it relates little to the problem statement\n\ndf_T.drop(columns='under_5', inplace=True)\n\n\n\n\nIn this section I will assess the data to spot irregularities and then clean the data\nUsing pandas to explore df_T to answer the following questions about the characteristics of the datasets:\n\nnumber of states in each dataset\nnumber of columns in each dataset\nduplicate rows in each dataset\ndatatypes of columns\nfeatures with missing values\nnumber of non-null unique values for features in each dataset\nwhat those unique values are and counts for each\nNumber of rows with missing values in each dataset\nTypes of fuels present in each dataset\n\n\n# checking out the head of the dataframe\ndf_T.head()\n\n\n\n\n\n\n\n\npop_2016\nunder_18\n65_above\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\nmedian_hu\nmedian_gross_rent\npe_hh\nhigh_school_grad_higher\nbsc_higher\npe_whi\nmedian_hh_income\nper_cap_inc\npoverty_pct\nfirms\n\n\n\n\nAlabama\n4,863,300\n22.60%\n16.10%\n51.60%\n69.30%\n26.80%\n1.40%\n$125,500\n$717\n2.55\n84.30%\n23.50%\n10.70%\n$43,623\n$24,091\n17.10%\n374,153\n\n\nAlaska\n741,894\n25.20%\n10.40%\n47.70%\n66.10%\n3.80%\n6.30%\n$250,000\n$1,146\n2.81\n92.10%\n28.00%\n15.50%\n$72,515\n$33,413\n9.90%\n68,032\n\n\nArizona\n6,931,071\n23.50%\n16.90%\n50.30%\n83.30%\n4.90%\n3.40%\n$167,500\n$913\n2.69\n86.00%\n27.50%\n11.90%\n$50,255\n$25,848\n16.40%\n499,926\n\n\nArkansas\n2,988,248\n23.60%\n16.30%\n50.90%\n79.40%\n15.70%\n1.60%\n$111,400\n$677\n2.53\n84.80%\n21.10%\n9.30%\n$41,371\n$22,798\n17.20%\n231,959\n\n\nCalifornia\n39,250,017\n23.20%\n13.60%\n50.30%\n72.70%\n6.50%\n14.80%\n$385,500\n$1,255\n2.96\n81.80%\n31.40%\n8.30%\n$61,818\n$30,318\n14.30%\n3,548,449\n\n\n\n\n\n\n\n\n# number of states\nprint(f'There are {df_T.shape[0]} rows (representing each state in the contiguous U.S. and Alaska) in this dataset')\n\nThere are 50 rows (representing each state in the contiguous U.S. and Alaska) in this dataset\n\n\n\n# number of columns\nprint(f'There are {df_T.shape[1]} columns')\n\nThere are 17 columns\n\n\n\n# number of duplicate rows\ndf_T.duplicated().sum()\n\n0\n\n\n\n# datatypes of each column\ndf_T.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 50 entries, Alabama to Wyoming\nData columns (total 17 columns):\n #   Column                   Non-Null Count  Dtype \n---  ------                   --------------  ----- \n 0   pop_2016                 50 non-null     object\n 1   under_18                 50 non-null     object\n 2   65_above                 50 non-null     object\n 3   female_pct               50 non-null     object\n 4   white_pct                50 non-null     object\n 5   black_pct                50 non-null     object\n 6   asian_pct                50 non-null     object\n 7   median_hu                50 non-null     object\n 8   median_gross_rent        50 non-null     object\n 9   pe_hh                    50 non-null     object\n 10  high_school_grad_higher  50 non-null     object\n 11  bsc_higher               50 non-null     object\n 12  pe_whi                   50 non-null     object\n 13  median_hh_income         50 non-null     object\n 14  per_cap_inc              50 non-null     object\n 15  poverty_pct              50 non-null     object\n 16  firms                    50 non-null     object\ndtypes: object(17)\nmemory usage: 7.0+ KB\n\n\nThe datatypes are oddly all in object type. This was because of the transpose function I used earlier in the script to convert the data from wide to long According to the pandas documentation, when transpose() is called upon a dataframe with heterogeneous datatypes, all the columns are converted to the object datatype.\nI will fix this later.\n\n# features with missing values\ndf_T.isnull().sum()\n\npop_2016                   0\nunder_18                   0\n65_above                   0\nfemale_pct                 0\nwhite_pct                  0\nblack_pct                  0\nasian_pct                  0\nmedian_hu                  0\nmedian_gross_rent          0\npe_hh                      0\nhigh_school_grad_higher    0\nbsc_higher                 0\npe_whi                     0\nmedian_hh_income           0\nper_cap_inc                0\npoverty_pct                0\nfirms                      0\ndtype: int64\n\n\n\n# number of rows with missing values\ndf_T.isnull().any(axis=1).sum()\n\n0\n\n\n\n# saving for later\ndf_T.to_csv('Data/df_T_01.csv')\n\n\n# reading in the data\ndf_U = pd.read_csv('Data/df_T_01.csv')\ndf_U.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\npop_2016\nunder_18\n65_above\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\nmedian_hu\nmedian_gross_rent\npe_hh\nhigh_school_grad_higher\nbsc_higher\npe_whi\nmedian_hh_income\nper_cap_inc\npoverty_pct\nfirms\n\n\n\n\n0\nAlabama\n4,863,300\n22.60%\n16.10%\n51.60%\n69.30%\n26.80%\n1.40%\n$125,500\n$717\n2.55\n84.30%\n23.50%\n10.70%\n$43,623\n$24,091\n17.10%\n374,153\n\n\n1\nAlaska\n741,894\n25.20%\n10.40%\n47.70%\n66.10%\n3.80%\n6.30%\n$250,000\n$1,146\n2.81\n92.10%\n28.00%\n15.50%\n$72,515\n$33,413\n9.90%\n68,032\n\n\n2\nArizona\n6,931,071\n23.50%\n16.90%\n50.30%\n83.30%\n4.90%\n3.40%\n$167,500\n$913\n2.69\n86.00%\n27.50%\n11.90%\n$50,255\n$25,848\n16.40%\n499,926\n\n\n3\nArkansas\n2,988,248\n23.60%\n16.30%\n50.90%\n79.40%\n15.70%\n1.60%\n$111,400\n$677\n2.53\n84.80%\n21.10%\n9.30%\n$41,371\n$22,798\n17.20%\n231,959\n\n\n4\nCalifornia\n39,250,017\n23.20%\n13.60%\n50.30%\n72.70%\n6.50%\n14.80%\n$385,500\n$1,255\n2.96\n81.80%\n31.40%\n8.30%\n$61,818\n$30,318\n14.30%\n3,548,449\n\n\n\n\n\n\n\n\n# dropping renaming column\ndf_U.rename(columns={'Unnamed: 0':'state'}, inplace=True)\ndf_U.head()\n\n\n\n\n\n\n\n\nstate\npop_2016\nunder_18\n65_above\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\nmedian_hu\nmedian_gross_rent\npe_hh\nhigh_school_grad_higher\nbsc_higher\npe_whi\nmedian_hh_income\nper_cap_inc\npoverty_pct\nfirms\n\n\n\n\n0\nAlabama\n4,863,300\n22.60%\n16.10%\n51.60%\n69.30%\n26.80%\n1.40%\n$125,500\n$717\n2.55\n84.30%\n23.50%\n10.70%\n$43,623\n$24,091\n17.10%\n374,153\n\n\n1\nAlaska\n741,894\n25.20%\n10.40%\n47.70%\n66.10%\n3.80%\n6.30%\n$250,000\n$1,146\n2.81\n92.10%\n28.00%\n15.50%\n$72,515\n$33,413\n9.90%\n68,032\n\n\n2\nArizona\n6,931,071\n23.50%\n16.90%\n50.30%\n83.30%\n4.90%\n3.40%\n$167,500\n$913\n2.69\n86.00%\n27.50%\n11.90%\n$50,255\n$25,848\n16.40%\n499,926\n\n\n3\nArkansas\n2,988,248\n23.60%\n16.30%\n50.90%\n79.40%\n15.70%\n1.60%\n$111,400\n$677\n2.53\n84.80%\n21.10%\n9.30%\n$41,371\n$22,798\n17.20%\n231,959\n\n\n4\nCalifornia\n39,250,017\n23.20%\n13.60%\n50.30%\n72.70%\n6.50%\n14.80%\n$385,500\n$1,255\n2.96\n81.80%\n31.40%\n8.30%\n$61,818\n$30,318\n14.30%\n3,548,449\n\n\n\n\n\n\n\nOne of the reasons the data is stored as the object type is because of the ‘%’, ‘,’, and ‘$’ symbols. In the following cells, I remove these offending characters\n\n# extracting digits from the following columns using regex\n\nimport re\n\ncols_extract = ['under_18','65_above', 'female_pct', 'white_pct', 'black_pct', 'asian_pct','high_school_grad_higher', 'bsc_higher', 'pe_whi', 'poverty_pct']\n\nfor col in cols_extract:\n    df_U[col]= df_U[col].str.extract(r'(\\d*\\.?\\d+)').astype(float)\n\ndf_U.head()\n\n\n\n\n\n\n\n\nstate\npop_2016\nunder_18\n65_above\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\nmedian_hu\nmedian_gross_rent\npe_hh\nhigh_school_grad_higher\nbsc_higher\npe_whi\nmedian_hh_income\nper_cap_inc\npoverty_pct\nfirms\n\n\n\n\n0\nAlabama\n4,863,300\n22.6\n16.1\n51.6\n69.3\n26.8\n1.4\n$125,500\n$717\n2.55\n84.3\n23.5\n10.7\n$43,623\n$24,091\n17.1\n374,153\n\n\n1\nAlaska\n741,894\n25.2\n10.4\n47.7\n66.1\n3.8\n6.3\n$250,000\n$1,146\n2.81\n92.1\n28.0\n15.5\n$72,515\n$33,413\n9.9\n68,032\n\n\n2\nArizona\n6,931,071\n23.5\n16.9\n50.3\n83.3\n4.9\n3.4\n$167,500\n$913\n2.69\n86.0\n27.5\n11.9\n$50,255\n$25,848\n16.4\n499,926\n\n\n3\nArkansas\n2,988,248\n23.6\n16.3\n50.9\n79.4\n15.7\n1.6\n$111,400\n$677\n2.53\n84.8\n21.1\n9.3\n$41,371\n$22,798\n17.2\n231,959\n\n\n4\nCalifornia\n39,250,017\n23.2\n13.6\n50.3\n72.7\n6.5\n14.8\n$385,500\n$1,255\n2.96\n81.8\n31.4\n8.3\n$61,818\n$30,318\n14.3\n3,548,449\n\n\n\n\n\n\n\n\n# removing the dollar sign symbol using a lambda function and converting to the float datatype\ncols_extract = ['pop_2016', 'median_hu', 'median_gross_rent', 'median_hh_income', 'per_cap_inc', 'firms']\n\nfor col in cols_extract:\n    df_U[col]= df_U[col].apply(lambda x: x.replace('$','').replace(',','') if isinstance(x, str) else x).astype(float)\ndf_U.head()\n\n\n\n\n\n\n\n\nstate\npop_2016\nunder_18\n65_above\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\nmedian_hu\nmedian_gross_rent\npe_hh\nhigh_school_grad_higher\nbsc_higher\npe_whi\nmedian_hh_income\nper_cap_inc\npoverty_pct\nfirms\n\n\n\n\n0\nAlabama\n4863300.0\n22.6\n16.1\n51.6\n69.3\n26.8\n1.4\n125500.0\n717.0\n2.55\n84.3\n23.5\n10.7\n43623.0\n24091.0\n17.1\n374153.0\n\n\n1\nAlaska\n741894.0\n25.2\n10.4\n47.7\n66.1\n3.8\n6.3\n250000.0\n1146.0\n2.81\n92.1\n28.0\n15.5\n72515.0\n33413.0\n9.9\n68032.0\n\n\n2\nArizona\n6931071.0\n23.5\n16.9\n50.3\n83.3\n4.9\n3.4\n167500.0\n913.0\n2.69\n86.0\n27.5\n11.9\n50255.0\n25848.0\n16.4\n499926.0\n\n\n3\nArkansas\n2988248.0\n23.6\n16.3\n50.9\n79.4\n15.7\n1.6\n111400.0\n677.0\n2.53\n84.8\n21.1\n9.3\n41371.0\n22798.0\n17.2\n231959.0\n\n\n4\nCalifornia\n39250017.0\n23.2\n13.6\n50.3\n72.7\n6.5\n14.8\n385500.0\n1255.0\n2.96\n81.8\n31.4\n8.3\n61818.0\n30318.0\n14.3\n3548449.0\n\n\n\n\n\n\n\n\n# checking the transformed datatypes\ndf_U.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50 entries, 0 to 49\nData columns (total 18 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   state                    50 non-null     object \n 1   pop_2016                 50 non-null     float64\n 2   under_18                 50 non-null     float64\n 3   65_above                 50 non-null     float64\n 4   female_pct               50 non-null     float64\n 5   white_pct                50 non-null     float64\n 6   black_pct                50 non-null     float64\n 7   asian_pct                50 non-null     float64\n 8   median_hu                50 non-null     float64\n 9   median_gross_rent        50 non-null     float64\n 10  pe_hh                    50 non-null     float64\n 11  high_school_grad_higher  50 non-null     float64\n 12  bsc_higher               50 non-null     float64\n 13  pe_whi                   50 non-null     float64\n 14  median_hh_income         50 non-null     float64\n 15  per_cap_inc              50 non-null     float64\n 16  poverty_pct              50 non-null     float64\n 17  firms                    50 non-null     float64\ndtypes: float64(17), object(1)\nmemory usage: 7.2+ KB\n\n\n\n# shape of the new dataframe\ndf_U.shape\n\n(50, 18)\n\n\n\n# tail of the new dataframe\ndf_U.tail()\n\n\n\n\n\n\n\n\nstate\npop_2016\nunder_18\n65_above\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\nmedian_hu\nmedian_gross_rent\npe_hh\nhigh_school_grad_higher\nbsc_higher\npe_whi\nmedian_hh_income\nper_cap_inc\npoverty_pct\nfirms\n\n\n\n\n45\nVirginia\n8411808.0\n22.2\n14.6\n50.8\n70.0\n19.8\n6.6\n245000.0\n1116.0\n2.62\n88.3\n36.3\n10.1\n65015.0\n34152.0\n11.0\n653193.0\n\n\n46\nWashington\n7288000.0\n22.4\n14.8\n50.0\n80.0\n4.1\n8.6\n259500.0\n1014.0\n2.56\n90.4\n32.9\n6.9\n61062.0\n31762.0\n11.3\n541522.0\n\n\n47\nWest Virginia\n1831102.0\n20.5\n18.8\n50.5\n93.6\n3.6\n0.8\n103800.0\n643.0\n2.43\n85.0\n19.2\n6.5\n41751.0\n23450.0\n17.9\n114435.0\n\n\n48\nWisconsin\n5778708.0\n22.3\n16.1\n50.3\n87.5\n6.6\n2.8\n165800.0\n776.0\n2.43\n91.0\n27.8\n6.2\n53357.0\n28340.0\n11.8\n432980.0\n\n\n49\nWyoming\n585501.0\n23.7\n15.0\n48.9\n92.8\n1.3\n1.0\n194800.0\n789.0\n2.49\n92.3\n25.7\n13.4\n58840.0\n29803.0\n11.3\n62427.0\n\n\n\n\n\n\n\n\n# summary statistics\ndf_U.describe()\n\n\n\n\n\n\n\n\npop_2016\nunder_18\n65_above\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\nmedian_hu\nmedian_gross_rent\npe_hh\nhigh_school_grad_higher\nbsc_higher\npe_whi\nmedian_hh_income\nper_cap_inc\npoverty_pct\nfirms\n\n\n\n\ncount\n5.000000e+01\n50.000000\n50.00000\n50.000000\n50.000000\n50.000000\n50.00000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.00000\n5.000000e+01\n\n\nmean\n6.448927e+06\n17.425780\n11.81044\n38.511740\n60.397020\n8.360900\n3.63118\n192026.000000\n879.880000\n2.582000\n67.424100\n22.363160\n7.078540\n54311.860000\n28491.780000\n10.05498\n5.548918e+05\n\n\nstd\n7.271769e+06\n9.936137\n6.84738\n21.584467\n35.861896\n9.917937\n5.79106\n79742.431821\n181.106761\n0.164105\n37.882927\n13.383697\n5.044314\n8949.529115\n4103.284534\n6.26358\n6.645229e+05\n\n\nmin\n5.855010e+05\n0.197000\n0.14500\n0.487000\n0.685000\n0.020000\n0.01500\n103100.000000\n643.000000\n2.320000\n0.842000\n0.241000\n0.051000\n39665.000000\n21057.000000\n0.10700\n6.242700e+04\n\n\n25%\n1.850106e+06\n19.025000\n10.42500\n48.000000\n34.175000\n0.650000\n0.80000\n136600.000000\n743.500000\n2.482500\n81.825000\n19.575000\n3.225000\n47526.000000\n25443.750000\n7.80000\n1.478222e+05\n\n\n50%\n4.559320e+06\n22.250000\n15.00000\n50.300000\n77.400000\n4.700000\n2.35000\n165900.000000\n818.500000\n2.550000\n87.850000\n27.200000\n7.050000\n52601.000000\n27669.500000\n11.35000\n3.535155e+05\n\n\n75%\n7.198768e+06\n23.450000\n16.10000\n50.900000\n87.275000\n12.400000\n4.55000\n237300.000000\n1001.500000\n2.637500\n90.625000\n31.075000\n10.100000\n60599.000000\n30977.000000\n14.25000\n5.933612e+05\n\n\nmax\n3.925002e+07\n30.200000\n19.90000\n51.600000\n94.800000\n37.700000\n37.70000\n515300.000000\n1438.000000\n3.150000\n92.800000\n40.500000\n18.600000\n74551.000000\n38803.000000\n20.80000\n3.548449e+06\n\n\n\n\n\n\n\n\n# saving for later\ndf_U.to_csv('Data/df_census_cleaned.csv')\n\nNow, on to wrangle the second dataset in this project – NCIS gun data"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#data-wrangling-b",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#data-wrangling-b",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Data Wrangling (b)",
    "text": "Data Wrangling (b)\n\nGeneral Properties\nLoading and inspecting the census data\n\n# reading the data in\ndf_gun = pd.read_excel('Data/gun_data.xlsx')\ndf_gun.head()\n\n\n\n\n\n\n\n\nmonth\nstate\npermit\npermit_recheck\nhandgun\nlong_gun\nother\nmultiple\nadmin\nprepawn_handgun\n...\nreturned_other\nrentals_handgun\nrentals_long_gun\nprivate_sale_handgun\nprivate_sale_long_gun\nprivate_sale_other\nreturn_to_seller_handgun\nreturn_to_seller_long_gun\nreturn_to_seller_other\ntotals\n\n\n\n\n0\n2017-09\nAlabama\n16717.0\n0.0\n5734.0\n6320.0\n221.0\n317\n0.0\n15.0\n...\n0.0\n0.0\n0.0\n9.0\n16.0\n3.0\n0.0\n0.0\n3.0\n32019\n\n\n1\n2017-09\nAlaska\n209.0\n2.0\n2320.0\n2930.0\n219.0\n160\n0.0\n5.0\n...\n0.0\n0.0\n0.0\n17.0\n24.0\n1.0\n0.0\n0.0\n0.0\n6303\n\n\n2\n2017-09\nArizona\n5069.0\n382.0\n11063.0\n7946.0\n920.0\n631\n0.0\n13.0\n...\n0.0\n0.0\n0.0\n38.0\n12.0\n2.0\n0.0\n0.0\n0.0\n28394\n\n\n3\n2017-09\nArkansas\n2935.0\n632.0\n4347.0\n6063.0\n165.0\n366\n51.0\n12.0\n...\n0.0\n0.0\n0.0\n13.0\n23.0\n0.0\n0.0\n2.0\n1.0\n17747\n\n\n4\n2017-09\nCalifornia\n57839.0\n0.0\n37165.0\n24581.0\n2984.0\n0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n123506\n\n\n\n\n5 rows × 27 columns\n\n\n\n\n\nIn this section I will assess the data to spot irregularities and then clean the data\nUsing pandas to explore df_T to answer the following questions about the characteristics of the datasets:\n\nnumber of states in each dataset\nnumber of columns in each dataset\nduplicate rows in each dataset\ndatatypes of columns\nfeatures with missing values\nnumber of non-null unique values for features in each dataset\nwhat those unique values are and counts for each\nNumber of rows with missing values in each dataset\nTypes of fuels present in each dataset\n\n\n# selecting relevant columns\ncol_gun = ['state', 'month', 'handgun', 'long_gun', 'multiple']\ndf_gun_raw =  df_gun[col_gun]\ndf_gun_raw.head()\n\n\n\n\n\n\n\n\nstate\nmonth\nhandgun\nlong_gun\nmultiple\n\n\n\n\n0\nAlabama\n2017-09\n5734.0\n6320.0\n317\n\n\n1\nAlaska\n2017-09\n2320.0\n2930.0\n160\n\n\n2\nArizona\n2017-09\n11063.0\n7946.0\n631\n\n\n3\nArkansas\n2017-09\n4347.0\n6063.0\n366\n\n\n4\nCalifornia\n2017-09\n37165.0\n24581.0\n0\n\n\n\n\n\n\n\n\n# number of rows\nprint(f'Rows: {df_gun_raw.shape[0]}')\n\n# number of columns\nprint(f'Columns: {df_gun_raw.shape[1]}')\n\nRows: 12485\nColumns: 5\n\n\n\n# duplicate rows \ndf_gun_raw.duplicated().sum()\n\n0\n\n\n\n# datatypes of columns\ndf_gun_raw.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12485 entries, 0 to 12484\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   state     12485 non-null  object \n 1   month     12485 non-null  object \n 2   handgun   12465 non-null  float64\n 3   long_gun  12466 non-null  float64\n 4   multiple  12485 non-null  int64  \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 487.8+ KB\n\n\n\n# number of non-null unique values for features\ndf_gun_raw['state'].nunique()\n\n55\n\n\n\n# what those unique values are and counts for each\ndf_gun_raw.state.value_counts()\n\nstate\nAlabama                 227\nPuerto Rico             227\nNevada                  227\nNew Hampshire           227\nNew Jersey              227\nNew Mexico              227\nNew York                227\nNorth Carolina          227\nNorth Dakota            227\nOhio                    227\nOklahoma                227\nOregon                  227\nPennsylvania            227\nRhode Island            227\nMontana                 227\nSouth Carolina          227\nSouth Dakota            227\nTennessee               227\nTexas                   227\nUtah                    227\nVermont                 227\nVirgin Islands          227\nVirginia                227\nWashington              227\nWest Virginia           227\nWisconsin               227\nNebraska                227\nMissouri                227\nAlaska                  227\nIdaho                   227\nArizona                 227\nArkansas                227\nCalifornia              227\nColorado                227\nConnecticut             227\nDelaware                227\nDistrict of Columbia    227\nFlorida                 227\nGeorgia                 227\nGuam                    227\nHawaii                  227\nIllinois                227\nMississippi             227\nIndiana                 227\nIowa                    227\nKansas                  227\nKentucky                227\nLouisiana               227\nMaine                   227\nMariana Islands         227\nMaryland                227\nMassachusetts           227\nMichigan                227\nMinnesota               227\nWyoming                 227\nName: count, dtype: int64\n\n\n\n# observing the result\ndf_gun_raw.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12485 entries, 0 to 12484\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   state     12485 non-null  object \n 1   month     12485 non-null  object \n 2   handgun   12465 non-null  float64\n 3   long_gun  12466 non-null  float64\n 4   multiple  12485 non-null  int64  \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 487.8+ KB\n\n\n\n# renaming the column appropriately\ndf_gun_raw.rename(columns={'month':'date'}, inplace=True)\ndf_gun_raw.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12485 entries, 0 to 12484\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   state     12485 non-null  object \n 1   date      12485 non-null  object \n 2   handgun   12465 non-null  float64\n 3   long_gun  12466 non-null  float64\n 4   multiple  12485 non-null  int64  \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 487.8+ KB\n\n\n\n# converting to datetime\ndf_gun_raw['date'] = pd.to_datetime(df_gun_raw['date'], format='%Y-%m')\ndf_gun_raw['date'].head()\n\n0   2017-09-01\n1   2017-09-01\n2   2017-09-01\n3   2017-09-01\n4   2017-09-01\nName: date, dtype: datetime64[ns]\n\n\n\ndf_gun_raw.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12485 entries, 0 to 12484\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype         \n---  ------    --------------  -----         \n 0   state     12485 non-null  object        \n 1   date      12485 non-null  datetime64[ns]\n 2   handgun   12465 non-null  float64       \n 3   long_gun  12466 non-null  float64       \n 4   multiple  12485 non-null  int64         \ndtypes: datetime64[ns](1), float64(2), int64(1), object(1)\nmemory usage: 487.8+ KB\n\n\n\ndf_gun_raw.date.dt.year\n\n0        2017\n1        2017\n2        2017\n3        2017\n4        2017\n         ... \n12480    1998\n12481    1998\n12482    1998\n12483    1998\n12484    1998\nName: date, Length: 12485, dtype: int32\n\n\n\n# extracting year and month data from the date column\ndf_gun_raw['year'] = df_gun_raw['date'].dt.year\ndf_gun_raw['month'] = df_gun_raw['date'].dt.month\ndf_gun_raw.head(1)\n\n\n\n\n\n\n\n\nstate\ndate\nhandgun\nlong_gun\nmultiple\nyear\nmonth\n\n\n\n\n0\nAlabama\n2017-09-01\n5734.0\n6320.0\n317\n2017\n9\n\n\n\n\n\n\n\n\n# dropping the redundant date column\ndf_gun_raw.drop(columns='date', inplace=True)\ndf_gun_raw.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nyear\nmonth\n\n\n\n\n0\nAlabama\n5734.0\n6320.0\n317\n2017\n9\n\n\n1\nAlaska\n2320.0\n2930.0\n160\n2017\n9\n\n\n2\nArizona\n11063.0\n7946.0\n631\n2017\n9\n\n\n3\nArkansas\n4347.0\n6063.0\n366\n2017\n9\n\n\n4\nCalifornia\n37165.0\n24581.0\n0\n2017\n9\n\n\n\n\n\n\n\n\n# observing the tail of the data\ndf_gun_raw.tail()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nyear\nmonth\n\n\n\n\n12480\nVirginia\n14.0\n2.0\n8\n1998\n11\n\n\n12481\nWashington\n65.0\n286.0\n8\n1998\n11\n\n\n12482\nWest Virginia\n149.0\n251.0\n5\n1998\n11\n\n\n12483\nWisconsin\n25.0\n214.0\n2\n1998\n11\n\n\n12484\nWyoming\n45.0\n49.0\n5\n1998\n11\n\n\n\n\n\n\n\n\n# checking out metadata of dataframe\ndf_gun_raw.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12485 entries, 0 to 12484\nData columns (total 6 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   state     12485 non-null  object \n 1   handgun   12465 non-null  float64\n 2   long_gun  12466 non-null  float64\n 3   multiple  12485 non-null  int64  \n 4   year      12485 non-null  int32  \n 5   month     12485 non-null  int32  \ndtypes: float64(2), int32(2), int64(1), object(1)\nmemory usage: 487.8+ KB\n\n\n\n# saving data for later\ndf_gun_raw.to_csv('Data/df_gun_cleaned.csv')"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#further-wrangling",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#further-wrangling",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Further Wrangling",
    "text": "Further Wrangling\n\nIn this section, I will further wrangle the dataframes and derive calculated columns\n\n# reading in the census data\ndf1=pd.read_csv('Data/df_census_cleaned.csv', index_col=0)\ndf1.head()\n\n\n\n\n\n\n\n\nstate\npop_2016\nunder_18\n65_above\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\nmedian_hu\nmedian_gross_rent\npe_hh\nhigh_school_grad_higher\nbsc_higher\npe_whi\nmedian_hh_income\nper_cap_inc\npoverty_pct\nfirms\n\n\n\n\n0\nAlabama\n4863300.0\n22.6\n16.1\n51.6\n69.3\n26.8\n1.4\n125500.0\n717.0\n2.55\n84.3\n23.5\n10.7\n43623.0\n24091.0\n17.1\n374153.0\n\n\n1\nAlaska\n741894.0\n25.2\n10.4\n47.7\n66.1\n3.8\n6.3\n250000.0\n1146.0\n2.81\n92.1\n28.0\n15.5\n72515.0\n33413.0\n9.9\n68032.0\n\n\n2\nArizona\n6931071.0\n23.5\n16.9\n50.3\n83.3\n4.9\n3.4\n167500.0\n913.0\n2.69\n86.0\n27.5\n11.9\n50255.0\n25848.0\n16.4\n499926.0\n\n\n3\nArkansas\n2988248.0\n23.6\n16.3\n50.9\n79.4\n15.7\n1.6\n111400.0\n677.0\n2.53\n84.8\n21.1\n9.3\n41371.0\n22798.0\n17.2\n231959.0\n\n\n4\nCalifornia\n39250017.0\n23.2\n13.6\n50.3\n72.7\n6.5\n14.8\n385500.0\n1255.0\n2.96\n81.8\n31.4\n8.3\n61818.0\n30318.0\n14.3\n3548449.0\n\n\n\n\n\n\n\n\n# reading in the gun sales data\ndf2=pd.read_csv('Data/df_gun_cleaned.csv', index_col=0)\ndf2.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nyear\nmonth\n\n\n\n\n0\nAlabama\n5734.0\n6320.0\n317\n2017\n9\n\n\n1\nAlaska\n2320.0\n2930.0\n160\n2017\n9\n\n\n2\nArizona\n11063.0\n7946.0\n631\n2017\n9\n\n\n3\nArkansas\n4347.0\n6063.0\n366\n2017\n9\n\n\n4\nCalifornia\n37165.0\n24581.0\n0\n2017\n9\n\n\n\n\n\n\n\n\n# checking out the shape of the census dataframe\ndf1.shape\n\n(50, 18)\n\n\n\n# checking out the shape of the gun dataframe\ndf2.shape\n\n(12485, 6)\n\n\n\n# I want to have a calculated column for the % of adult population (between 18 & 65),\n# as I think this would be more relevant to the analysis than the data on children and elderly population\n\ndf1['adults_below_65'] = 100 - (df1['under_18']  +  df1['65_above'])\ndf1.head()\n\n\n\n\n\n\n\n\nstate\npop_2016\nunder_18\n65_above\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\nmedian_hu\nmedian_gross_rent\npe_hh\nhigh_school_grad_higher\nbsc_higher\npe_whi\nmedian_hh_income\nper_cap_inc\npoverty_pct\nfirms\nadults_below_65\n\n\n\n\n0\nAlabama\n4863300.0\n22.6\n16.1\n51.6\n69.3\n26.8\n1.4\n125500.0\n717.0\n2.55\n84.3\n23.5\n10.7\n43623.0\n24091.0\n17.1\n374153.0\n61.3\n\n\n1\nAlaska\n741894.0\n25.2\n10.4\n47.7\n66.1\n3.8\n6.3\n250000.0\n1146.0\n2.81\n92.1\n28.0\n15.5\n72515.0\n33413.0\n9.9\n68032.0\n64.4\n\n\n2\nArizona\n6931071.0\n23.5\n16.9\n50.3\n83.3\n4.9\n3.4\n167500.0\n913.0\n2.69\n86.0\n27.5\n11.9\n50255.0\n25848.0\n16.4\n499926.0\n59.6\n\n\n3\nArkansas\n2988248.0\n23.6\n16.3\n50.9\n79.4\n15.7\n1.6\n111400.0\n677.0\n2.53\n84.8\n21.1\n9.3\n41371.0\n22798.0\n17.2\n231959.0\n60.1\n\n\n4\nCalifornia\n39250017.0\n23.2\n13.6\n50.3\n72.7\n6.5\n14.8\n385500.0\n1255.0\n2.96\n81.8\n31.4\n8.3\n61818.0\n30318.0\n14.3\n3548449.0\n63.2\n\n\n\n\n\n\n\n\n# dropping the afore-used columns\ndf1.drop(columns=['under_18', '65_above'], inplace=True)\ndf1.head()\n\n\n\n\n\n\n\n\nstate\npop_2016\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\nmedian_hu\nmedian_gross_rent\npe_hh\nhigh_school_grad_higher\nbsc_higher\npe_whi\nmedian_hh_income\nper_cap_inc\npoverty_pct\nfirms\nadults_below_65\n\n\n\n\n0\nAlabama\n4863300.0\n51.6\n69.3\n26.8\n1.4\n125500.0\n717.0\n2.55\n84.3\n23.5\n10.7\n43623.0\n24091.0\n17.1\n374153.0\n61.3\n\n\n1\nAlaska\n741894.0\n47.7\n66.1\n3.8\n6.3\n250000.0\n1146.0\n2.81\n92.1\n28.0\n15.5\n72515.0\n33413.0\n9.9\n68032.0\n64.4\n\n\n2\nArizona\n6931071.0\n50.3\n83.3\n4.9\n3.4\n167500.0\n913.0\n2.69\n86.0\n27.5\n11.9\n50255.0\n25848.0\n16.4\n499926.0\n59.6\n\n\n3\nArkansas\n2988248.0\n50.9\n79.4\n15.7\n1.6\n111400.0\n677.0\n2.53\n84.8\n21.1\n9.3\n41371.0\n22798.0\n17.2\n231959.0\n60.1\n\n\n4\nCalifornia\n39250017.0\n50.3\n72.7\n6.5\n14.8\n385500.0\n1255.0\n2.96\n81.8\n31.4\n8.3\n61818.0\n30318.0\n14.3\n3548449.0\n63.2\n\n\n\n\n\n\n\n\n# grouping the columns in similar group\n\ndemographic = [\n    'state',\n    \"pop_2016\",\n    \"female_pct\",\n    \"white_pct\",\n    \"black_pct\",\n    \"asian_pct\",\n    \"pe_hh\",\n    \"pe_whi\",\n    \"adults_below_65\"\n]\n\neducation = [\n    \"high_school_grad_higher\",\n    \"bsc_higher\"\n]\n\nfinances = [\n    \"median_hu\",\n    \"median_gross_rent\",\n    \"poverty_pct\",\n    \"median_hh_income\",\n    \"per_cap_inc\",\n    \"firms\"\n]\n\n\n# new column ordering\nnew_col_order = (demographic + education + finances)\n\n# checking if the number of items in new_col_order is the same as in the original column\nset(df1.columns) == set(new_col_order)\n\nTrue\n\n\n\n# proceeding to reorder\ndf1 = df1[new_col_order]\ndf1.head()\n\n\n\n\n\n\n\n\nstate\npop_2016\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\npe_hh\npe_whi\nadults_below_65\nhigh_school_grad_higher\nbsc_higher\nmedian_hu\nmedian_gross_rent\npoverty_pct\nmedian_hh_income\nper_cap_inc\nfirms\n\n\n\n\n0\nAlabama\n4863300.0\n51.6\n69.3\n26.8\n1.4\n2.55\n10.7\n61.3\n84.3\n23.5\n125500.0\n717.0\n17.1\n43623.0\n24091.0\n374153.0\n\n\n1\nAlaska\n741894.0\n47.7\n66.1\n3.8\n6.3\n2.81\n15.5\n64.4\n92.1\n28.0\n250000.0\n1146.0\n9.9\n72515.0\n33413.0\n68032.0\n\n\n2\nArizona\n6931071.0\n50.3\n83.3\n4.9\n3.4\n2.69\n11.9\n59.6\n86.0\n27.5\n167500.0\n913.0\n16.4\n50255.0\n25848.0\n499926.0\n\n\n3\nArkansas\n2988248.0\n50.9\n79.4\n15.7\n1.6\n2.53\n9.3\n60.1\n84.8\n21.1\n111400.0\n677.0\n17.2\n41371.0\n22798.0\n231959.0\n\n\n4\nCalifornia\n39250017.0\n50.3\n72.7\n6.5\n14.8\n2.96\n8.3\n63.2\n81.8\n31.4\n385500.0\n1255.0\n14.3\n61818.0\n30318.0\n3548449.0\n\n\n\n\n\n\n\n\n# saving for later\ndf1.to_csv('Data/df_census_cleaned_1.csv')"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#exploratory-data-analysis-1",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#exploratory-data-analysis-1",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Exploratory Data Analysis 1",
    "text": "Exploratory Data Analysis 1\n\n(Census Data)\n\n# Reading in the dataset, and viewing heads, tails and  and random sample of the rows \ndf1 = pd.read_csv('Data/df_census_cleaned_1.csv', index_col=0)\ndf1.head()\n\n\n\n\n\n\n\n\nstate\npop_2016\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\npe_hh\npe_whi\nadults_below_65\nhigh_school_grad_higher\nbsc_higher\nmedian_hu\nmedian_gross_rent\npoverty_pct\nmedian_hh_income\nper_cap_inc\nfirms\n\n\n\n\n0\nAlabama\n4863300.0\n51.6\n69.3\n26.8\n1.4\n2.55\n10.7\n61.3\n84.3\n23.5\n125500.0\n717.0\n17.1\n43623.0\n24091.0\n374153.0\n\n\n1\nAlaska\n741894.0\n47.7\n66.1\n3.8\n6.3\n2.81\n15.5\n64.4\n92.1\n28.0\n250000.0\n1146.0\n9.9\n72515.0\n33413.0\n68032.0\n\n\n2\nArizona\n6931071.0\n50.3\n83.3\n4.9\n3.4\n2.69\n11.9\n59.6\n86.0\n27.5\n167500.0\n913.0\n16.4\n50255.0\n25848.0\n499926.0\n\n\n3\nArkansas\n2988248.0\n50.9\n79.4\n15.7\n1.6\n2.53\n9.3\n60.1\n84.8\n21.1\n111400.0\n677.0\n17.2\n41371.0\n22798.0\n231959.0\n\n\n4\nCalifornia\n39250017.0\n50.3\n72.7\n6.5\n14.8\n2.96\n8.3\n63.2\n81.8\n31.4\n385500.0\n1255.0\n14.3\n61818.0\n30318.0\n3548449.0\n\n\n\n\n\n\n\n\n# viewing tail of data\ndf1.tail()\n\n\n\n\n\n\n\n\nstate\npop_2016\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\npe_hh\npe_whi\nadults_below_65\nhigh_school_grad_higher\nbsc_higher\nmedian_hu\nmedian_gross_rent\npoverty_pct\nmedian_hh_income\nper_cap_inc\nfirms\n\n\n\n\n45\nVirginia\n8411808.0\n50.8\n70.0\n19.8\n6.6\n2.62\n10.1\n63.2\n88.3\n36.3\n245000.0\n1116.0\n11.0\n65015.0\n34152.0\n653193.0\n\n\n46\nWashington\n7288000.0\n50.0\n80.0\n4.1\n8.6\n2.56\n6.9\n62.8\n90.4\n32.9\n259500.0\n1014.0\n11.3\n61062.0\n31762.0\n541522.0\n\n\n47\nWest Virginia\n1831102.0\n50.5\n93.6\n3.6\n0.8\n2.43\n6.5\n60.7\n85.0\n19.2\n103800.0\n643.0\n17.9\n41751.0\n23450.0\n114435.0\n\n\n48\nWisconsin\n5778708.0\n50.3\n87.5\n6.6\n2.8\n2.43\n6.2\n61.6\n91.0\n27.8\n165800.0\n776.0\n11.8\n53357.0\n28340.0\n432980.0\n\n\n49\nWyoming\n585501.0\n48.9\n92.8\n1.3\n1.0\n2.49\n13.4\n61.3\n92.3\n25.7\n194800.0\n789.0\n11.3\n58840.0\n29803.0\n62427.0\n\n\n\n\n\n\n\n\n# viewing random sample\ndf1.sample(n=5)\n\n\n\n\n\n\n\n\nstate\npop_2016\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\npe_hh\npe_whi\nadults_below_65\nhigh_school_grad_higher\nbsc_higher\nmedian_hu\nmedian_gross_rent\npoverty_pct\nmedian_hh_income\nper_cap_inc\nfirms\n\n\n\n\n48\nWisconsin\n5778708.0\n50.300\n87.500\n6.600\n2.800\n2.43\n6.200\n61.600\n91.000\n27.800\n165800.0\n776.0\n11.800\n53357.0\n28340.0\n432980.0\n\n\n14\nIowa\n3134693.0\n50.300\n91.400\n3.700\n2.500\n2.42\n5.000\n60.300\n91.500\n26.700\n129200.0\n697.0\n11.800\n53183.0\n27950.0\n259121.0\n\n\n41\nTennessee\n6651194.0\n0.512\n0.787\n0.171\n0.018\n2.53\n0.106\n99.617\n0.855\n0.249\n142100.0\n764.0\n0.158\n45219.0\n25227.0\n550453.0\n\n\n12\nIllinois\n12801539.0\n50.900\n77.200\n14.700\n5.500\n2.63\n7.400\n62.500\n87.900\n32.300\n173800.0\n907.0\n13.000\n57574.0\n30494.0\n1135017.0\n\n\n1\nAlaska\n741894.0\n47.700\n66.100\n3.800\n6.300\n2.81\n15.500\n64.400\n92.100\n28.000\n250000.0\n1146.0\n9.900\n72515.0\n33413.0\n68032.0\n\n\n\n\n\n\n\n\n# Getting the dimensions of the DataFrame with the .shape attribute:\ndf1.shape\n\n(50, 17)\n\n\n\n#  List the data type of each column, the number of non-missing values, and memory usage with the .info method:\ndf1.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 50 entries, 0 to 49\nData columns (total 17 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   state                    50 non-null     object \n 1   pop_2016                 50 non-null     float64\n 2   female_pct               50 non-null     float64\n 3   white_pct                50 non-null     float64\n 4   black_pct                50 non-null     float64\n 5   asian_pct                50 non-null     float64\n 6   pe_hh                    50 non-null     float64\n 7   pe_whi                   50 non-null     float64\n 8   adults_below_65          50 non-null     float64\n 9   high_school_grad_higher  50 non-null     float64\n 10  bsc_higher               50 non-null     float64\n 11  median_hu                50 non-null     float64\n 12  median_gross_rent        50 non-null     float64\n 13  poverty_pct              50 non-null     float64\n 14  median_hh_income         50 non-null     float64\n 15  per_cap_inc              50 non-null     float64\n 16  firms                    50 non-null     float64\ndtypes: float64(16), object(1)\nmemory usage: 7.0+ KB\n\n\nThere are 50 rows in this dataset representing each state of the U.S measured by 16 features of interest The data is clean: there are no null values, and the datatypes are appropriate"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#data-dictionary",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#data-dictionary",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\n\n\ns/n\nVariable\nDescription\n\n\n\n\n0\nstate\nEach of the 50 states of the contiguous U.S.\n\n\n1\npop_2016\nPopulation estimates by July 1, 2016\n\n\n2\nfemale_pct\nFemale persons, percent, by July 1, 2016\n\n\n3\nwhite_pct\nWhite alone, percent, by July 1, 2016\n\n\n4\nblack_pct\nBlack or African American alone, percent by July 1, 2016\n\n\n5\nasian_pct\nAsian alone, percent, July 1, 2016\n\n\n6\npe_hh\nPersons per household, 2011-2015\n\n\n7\npe_whi\nPersons without health insurance, under age 65 years, percent\n\n\n8\nadults_below_65\nadults older than 18, younger than 65, percent, 2016\n\n\n9\nhigh_school_grad_higher\nHigh school graduate or higher, percent of persons age 25 years+, 2011-2015\n\n\n10\nbsc_higher\nBachelor\\’s degree or higher, percent of persons age 25 years+, 2011-2015\n\n\n11\nmedian_hu\nMedian value of owner-occupied housing units, 2011-2015\n\n\n12\nmedian_gross_rent\nMedian gross rent, 2011-2015\n\n\n13\npoverty_pct\nPersons in poverty, percent\n\n\n14\nmedian_hh_income\nMedian household income (in 2015 dollars), 2011-2015\n\n\n15\nper_cap_inc\nPer capita income in past 12 months (in 2015 dollars), 2011-2015\n\n\n16\nfirms\nAll firms, 2012\n\n\n\n\n# displaying summary statistics\ndf1.describe()\n\n\n\n\n\n\n\n\npop_2016\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\npe_hh\npe_whi\nadults_below_65\nhigh_school_grad_higher\nbsc_higher\nmedian_hu\nmedian_gross_rent\npoverty_pct\nmedian_hh_income\nper_cap_inc\nfirms\n\n\n\n\ncount\n5.000000e+01\n50.000000\n50.000000\n50.000000\n50.00000\n50.000000\n50.000000\n50.00000\n50.000000\n50.000000\n50.000000\n50.000000\n50.00000\n50.000000\n50.000000\n5.000000e+01\n\n\nmean\n6.448927e+06\n38.511740\n60.397020\n8.360900\n3.63118\n2.582000\n7.078540\n70.76378\n67.424100\n22.363160\n192026.000000\n879.880000\n10.05498\n54311.860000\n28491.780000\n5.548918e+05\n\n\nstd\n7.271769e+06\n21.584467\n35.861896\n9.917937\n5.79106\n0.164105\n5.044314\n16.41933\n37.882927\n13.383697\n79742.431821\n181.106761\n6.26358\n8949.529115\n4103.284534\n6.645229e+05\n\n\nmin\n5.855010e+05\n0.487000\n0.685000\n0.020000\n0.01500\n2.320000\n0.051000\n58.90000\n0.842000\n0.241000\n103100.000000\n643.000000\n0.10700\n39665.000000\n21057.000000\n6.242700e+04\n\n\n25%\n1.850106e+06\n48.000000\n34.175000\n0.650000\n0.80000\n2.482500\n3.225000\n61.15000\n81.825000\n19.575000\n136600.000000\n743.500000\n7.80000\n47526.000000\n25443.750000\n1.478222e+05\n\n\n50%\n4.559320e+06\n50.300000\n77.400000\n4.700000\n2.35000\n2.550000\n7.050000\n62.25000\n87.850000\n27.200000\n165900.000000\n818.500000\n11.35000\n52601.000000\n27669.500000\n3.535155e+05\n\n\n75%\n7.198768e+06\n50.900000\n87.275000\n12.400000\n4.55000\n2.637500\n10.100000\n64.30000\n90.625000\n31.075000\n237300.000000\n1001.500000\n14.25000\n60599.000000\n30977.000000\n5.933612e+05\n\n\nmax\n3.925002e+07\n51.600000\n94.800000\n37.700000\n37.70000\n3.150000\n18.600000\n99.63800\n92.800000\n40.500000\n515300.000000\n1438.000000\n20.80000\n74551.000000\n38803.000000\n3.548449e+06\n\n\n\n\n\n\n\n\n# plotting the kde of some variables to have a sense of the data distribution\nsns.set_style('darkgrid')\nfig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(20, 4))\n\ndf1[\"pop_2016\"].plot.kde(ax=ax1)\nax1.set_title('KDE plot for pop_2016')\n\ndf1[\"female_pct\"].plot.kde(ax=ax2)\nax2.set_title('KDE plot for female_pct')\n\ndf1[\"white_pct\"].plot.kde(ax=ax3)\nax3.set_title('KDE plot for white_pct')\n\ndf1[\"pe_hh\"].plot.kde(ax=ax4)\nax4.set_title('KDE plot for pe_hh')\n\ndf1[\"median_hu\"].plot.kde(ax=ax5)\nax5.set_title('KDE plot for median_hu')\n\nplt.show()\n\n\n\n\nUpon viewing the kde plot, there seems to be some unusually low values for the percentages columns. This was a result of some of the percentage columns being formatted as decimals After inspection, I identified the offending rows as rows[30:42]. To correct these rows, I simply target rows having values less than a benchmark below which the values cannot possibly be valid and multiply by 100\n\n# correcting `female_pct` column\nmask1 = df1['female_pct'] &lt; 5\ncol1 = ['female_pct']\ndf1.loc[mask1, col1] = df1.loc[mask1, col1] * 100\n\n\n# checking that values are reasonable\ndf1.female_pct.describe()\n\ncount    50.000000\nmean     50.564000\nstd       0.801213\nmin      47.700000\n25%      50.200000\n50%      50.650000\n75%      51.175000\nmax      51.600000\nName: female_pct, dtype: float64\n\n\n\n# checking offending rows\ndf1.white_pct[30:42]\n\n30    0.826\n31    0.699\n32    0.710\n33    0.879\n34    0.825\n35    0.746\n36    0.874\n37    0.824\n38    0.844\n39    0.685\n40    0.852\n41    0.787\nName: white_pct, dtype: float64\n\n\n\n# standardizing\nmask3 = df1['white_pct'] &lt; 1\ncol3 = ['white_pct']\ndf1.loc[mask3, col3] = df1.loc[mask3, col3] * 100\n\n\n# checking result\ndf1.white_pct[30:42]\n\n30    82.6\n31    69.9\n32    71.0\n33    87.9\n34    82.5\n35    74.6\n36    87.4\n37    82.4\n38    84.4\n39    68.5\n40    85.2\n41    78.7\nName: white_pct, dtype: float64\n\n\n\n# fishing rows\ndf1.black_pct.sort_values()\n\n40     0.020\n36     0.021\n30     0.025\n33     0.029\n35     0.078\n38     0.081\n37     0.118\n34     0.128\n41     0.171\n31     0.177\n32     0.222\n39     0.275\n25     0.600\n11     0.800\n49     1.300\n44     1.300\n43     1.400\n28     1.500\n18     1.500\n10     2.200\n47     3.600\n14     3.700\n1      3.800\n46     4.100\n5      4.500\n2      4.900\n26     5.000\n22     6.200\n15     6.200\n4      6.500\n48     6.600\n16     8.300\n20     8.600\n27     9.600\n13     9.700\n24    11.800\n6     11.800\n42    12.600\n21    14.200\n12    14.700\n29    15.000\n3     15.700\n8     16.800\n45    19.800\n7     22.600\n0     26.800\n19    30.700\n9     32.000\n17    32.600\n23    37.700\nName: black_pct, dtype: float64\n\n\n\n# checking for a particular row to see if indeed the value is valid, which it is\ndf1.loc[25]\n\nstate                        Montana\npop_2016                   1042520.0\nfemale_pct                      49.7\nwhite_pct                       89.2\nblack_pct                        0.6\nasian_pct                        0.8\npe_hh                           2.41\npe_whi                           9.8\nadults_below_65                 60.5\nhigh_school_grad_higher         92.8\nbsc_higher                      29.5\nmedian_hu                   193500.0\nmedian_gross_rent              711.0\npoverty_pct                     13.3\nmedian_hh_income             47169.0\nper_cap_inc                  26381.0\nfirms                       112419.0\nName: 25, dtype: object\n\n\n\n# correcting 'black_pct\nmask2 = df1['black_pct'] &lt; 0.3\ncol2 = ['black_pct']\ndf1.loc[mask2, col2] = df1.loc[mask2, col2] * 100\n\n\n# checking result\ndf1.black_pct.describe()\n\ncount    50.000000\nmean     11.024000\nstd       9.631868\nmin       0.600000\n25%       3.625000\n50%       8.200000\n75%      15.525000\nmax      37.700000\nName: black_pct, dtype: float64\n\n\n\n# fishing out offending rows\ndf1.asian_pct.sort_values()\n\n40     0.015\n33     0.015\n39     0.016\n30     0.017\n41     0.018\n35     0.022\n34     0.022\n32     0.029\n37     0.035\n38     0.036\n36     0.045\n31     0.089\n47     0.800\n25     0.800\n49     1.000\n23     1.100\n18     1.200\n0      1.400\n16     1.500\n11     1.500\n3      1.600\n44     1.800\n17     1.800\n24     2.000\n13     2.200\n14     2.500\n26     2.500\n43     2.500\n28     2.700\n48     2.800\n8      2.900\n15     3.000\n21     3.100\n5      3.300\n2      3.400\n7      4.000\n9      4.100\n6      4.700\n42     4.800\n22     4.900\n12     5.500\n1      6.300\n19     6.600\n45     6.600\n20     6.700\n46     8.600\n27     8.700\n29     9.800\n4     14.800\n10    37.700\nName: asian_pct, dtype: float64\n\n\n\nmask4 = df1['asian_pct'] &lt; 0.3\ncol4 = ['asian_pct']\ndf1.loc[mask4, col4] = df1.loc[mask4, col4] * 100\n\n\n# checking results\ndf1.asian_pct.describe()\n\ncount    50.000000\nmean      4.342000\nstd       5.561973\nmin       0.800000\n25%       1.725000\n50%       2.850000\n75%       4.775000\nmax      37.700000\nName: asian_pct, dtype: float64\n\n\n\n# correcting 'high_school_grad_higher'\nmask5 = df1['high_school_grad_higher'] &lt; 5\ncol5 = ['high_school_grad_higher']\ndf1.loc[mask5, col5] = df1.loc[mask5, col5] * 100\n\n\n# checking result\ndf1.bsc_higher.describe()\n\ncount    50.000000\nmean     22.363160\nstd      13.383697\nmin       0.241000\n25%      19.575000\n50%      27.200000\n75%      31.075000\nmax      40.500000\nName: bsc_higher, dtype: float64\n\n\n\ndf1.bsc_higher.sort_values()\n\n35     0.241\n41     0.249\n39     0.258\n34     0.261\n30     0.263\n40     0.270\n33     0.277\n32     0.284\n37     0.286\n36     0.308\n38     0.319\n31     0.342\n47    19.200\n23    20.700\n3     21.100\n16    22.300\n17    22.500\n27    23.000\n0     23.500\n13    24.100\n49    25.700\n11    25.900\n14    26.700\n21    26.900\n24    27.100\n8     27.300\n2     27.500\n42    27.600\n48    27.800\n1     28.000\n9     28.800\n18    29.000\n26    29.300\n25    29.500\n7     30.000\n10    30.800\n15    31.000\n43    31.100\n4     31.400\n12    32.300\n46    32.900\n22    33.700\n28    34.900\n44    36.000\n45    36.300\n29    36.800\n6     37.600\n19    37.900\n5     38.100\n20    40.500\nName: bsc_higher, dtype: float64\n\n\n\n#\nmask6 = df1['bsc_higher'] &lt; 2\ncol6 = ['bsc_higher']\ndf1.loc[mask6, col6] = df1.loc[mask6, col6] * 100\n\n\ndf1.bsc_higher.describe()\n\ncount    50.00000\nmean     29.01200\nstd       4.93404\nmin      19.20000\n25%      25.95000\n50%      28.20000\n75%      31.77500\nmax      40.50000\nName: bsc_higher, dtype: float64\n\n\n\n#\nmask7 = df1['pe_whi'] &lt; 1\ncol7 = ['pe_whi']\ndf1.loc[mask7, col7] = df1.loc[mask7, col7] * 100\n\n\n# correcting the last % column\nmask8 = df1['poverty_pct'] &lt; 1\ncol8 = ['poverty_pct']\ndf1.loc[mask8, col8] = df1.loc[mask8, col8] * 100\n\n\n# checking the summary statistics again\ndf1.describe()\n\n\n\n\n\n\n\n\npop_2016\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\npe_hh\npe_whi\nadults_below_65\nhigh_school_grad_higher\nbsc_higher\nmedian_hu\nmedian_gross_rent\npoverty_pct\nmedian_hh_income\nper_cap_inc\nfirms\n\n\n\n\ncount\n5.000000e+01\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n50.00000\n50.000000\n50.00000\n50.000000\n50.000000\n50.000000\n50.000000\n50.000000\n5.000000e+01\n\n\nmean\n6.448927e+06\n50.564000\n79.308000\n11.024000\n4.342000\n2.582000\n9.310000\n70.76378\n88.224000\n29.01200\n192026.000000\n879.880000\n13.518000\n54311.860000\n28491.780000\n5.548918e+05\n\n\nstd\n7.271769e+06\n0.801213\n12.335908\n9.631868\n5.561973\n0.164105\n3.456095\n16.41933\n3.089585\n4.93404\n79742.431821\n181.106761\n3.019778\n8949.529115\n4103.284534\n6.645229e+05\n\n\nmin\n5.855010e+05\n47.700000\n25.800000\n0.600000\n0.800000\n2.320000\n2.900000\n58.90000\n81.800000\n19.20000\n103100.000000\n643.000000\n7.300000\n39665.000000\n21057.000000\n6.242700e+04\n\n\n25%\n1.850106e+06\n50.200000\n72.475000\n3.625000\n1.725000\n2.482500\n6.625000\n61.15000\n85.600000\n25.95000\n136600.000000\n743.500000\n11.300000\n47526.000000\n25443.750000\n1.478222e+05\n\n\n50%\n4.559320e+06\n50.650000\n82.100000\n8.200000\n2.850000\n2.550000\n9.350000\n62.25000\n88.850000\n28.20000\n165900.000000\n818.500000\n13.300000\n52601.000000\n27669.500000\n3.535155e+05\n\n\n75%\n7.198768e+06\n51.175000\n87.500000\n15.525000\n4.775000\n2.637500\n11.550000\n64.30000\n90.850000\n31.77500\n237300.000000\n1001.500000\n15.375000\n60599.000000\n30977.000000\n5.933612e+05\n\n\nmax\n3.925002e+07\n51.600000\n94.800000\n37.700000\n37.700000\n3.150000\n18.600000\n99.63800\n92.800000\n40.50000\n515300.000000\n1438.000000\n20.800000\n74551.000000\n38803.000000\n3.548449e+06\n\n\n\n\n\n\n\n\n# checking distribution again\n# plotting the kde of some variables to have a sense of the data distribution\n\nfig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(20, 4))\n\ndf1[\"pop_2016\"].plot.kde(ax=ax1)\nax1.set_title('KDE plot for pop_2016')\n\ndf1[\"female_pct\"].plot.kde(ax=ax2)\nax2.set_title('KDE plot for female_pct')\n\ndf1[\"white_pct\"].plot.kde(ax=ax3)\nax3.set_title('KDE plot for white_pct')\n\ndf1[\"pe_hh\"].plot.kde(ax=ax4)\nax4.set_title('KDE plot for pe_hh')\n\ndf1[\"median_hu\"].plot.kde(ax=ax5)\nax5.set_title('KDE plot for median_hu')\n\nplt.show()\n\n\n\n\nNow, the min values have no unusual numbers\nProceeding to EDA…"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#preliminary-exploration-on-the-census-data",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#preliminary-exploration-on-the-census-data",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Preliminary Exploration on the Census Data",
    "text": "Preliminary Exploration on the Census Data\nSince the census dataset isn’t the focus of this analysis, I will just do some brief exploration of the dataframe\n\nWhat are the most populated and least states?\nWhich states have highest and least black population?\nWhich states have highest and lowest median household income?\n\n\n# What are the most populated and least states?\na = df1[['state', 'pop_2016']].nlargest(5,'pop_2016')\nb = df1[['state', 'pop_2016']].nsmallest(5,'pop_2016')\n\nc_list = [a, b]\npd.concat(c_list, keys=['most populated', 'least pupulated'], axis='rows')\n\n\n\n\n\n\n\n\n\nstate\npop_2016\n\n\n\n\nmost populated\n4\nCalifornia\n39250017.0\n\n\n42\nTexas\n27862596.0\n\n\n8\nFlorida\n20612439.0\n\n\n31\nNew York\n19745289.0\n\n\n12\nIllinois\n12801539.0\n\n\nleast pupulated\n49\nWyoming\n585501.0\n\n\n44\nVermont\n624594.0\n\n\n1\nAlaska\n741894.0\n\n\n33\nNorth Dakota\n757952.0\n\n\n40\nSouth Dakota\n865454.0\n\n\n\n\n\n\n\n\n# Which states have highest and least black population?\na = df1[['state', 'black_pct']].nlargest(5,'black_pct')\nb = df1[['state', 'black_pct']].nsmallest(5,'black_pct')\n\nc_list = [a, b]\npd.concat(c_list, keys=['high black population', 'low black population'], axis='rows')\n\n\n\n\n\n\n\n\n\nstate\nblack_pct\n\n\n\n\nhigh black population\n23\nMississippi\n37.7\n\n\n17\nLouisiana\n32.6\n\n\n9\nGeorgia\n32.0\n\n\n19\nMaryland\n30.7\n\n\n39\nSouth Carolina\n27.5\n\n\nlow black population\n25\nMontana\n0.6\n\n\n11\nIdaho\n0.8\n\n\n44\nVermont\n1.3\n\n\n49\nWyoming\n1.3\n\n\n43\nUtah\n1.4\n\n\n\n\n\n\n\n\n# Which states have highest and lowest median household income?\na = df1[['state', 'median_hh_income']].nlargest(5,'median_hh_income')\nb = df1[['state', 'median_hh_income']].nsmallest(5,'median_hh_income')\nc_list = [a, b]\npd.concat(c_list, keys=['high median household income ($)', 'low median household income ($)'], axis='rows')\n\n\n\n\n\n\n\n\n\nstate\nmedian_hh_income\n\n\n\n\nhigh median household income ($)\n19\nMaryland\n74551.0\n\n\n1\nAlaska\n72515.0\n\n\n29\nNew Jersey\n72093.0\n\n\n6\nConnecticut\n70331.0\n\n\n10\nHawaii\n69515.0\n\n\nlow median household income ($)\n23\nMississippi\n39665.0\n\n\n3\nArkansas\n41371.0\n\n\n47\nWest Virginia\n41751.0\n\n\n0\nAlabama\n43623.0\n\n\n16\nKentucky\n43740.0\n\n\n\n\n\n\n\n\n# saving for later\ndf1.to_csv('Data/df_census_cleaned_v2.csv', index=0)"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#preparing-gun-sales-data-for-eda",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#preparing-gun-sales-data-for-eda",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Preparing Gun Sales Data for EDA",
    "text": "Preparing Gun Sales Data for EDA\nIn this section, I will prepare the dataframe for EDA:\n\n# Reading in the dataset, and viewing heads, tails and  and random sample of the rows \ndf2 = pd.read_csv('Data/df_gun_cleaned.csv', index_col=0)\ndf2.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nyear\nmonth\n\n\n\n\n0\nAlabama\n5734.0\n6320.0\n317\n2017\n9\n\n\n1\nAlaska\n2320.0\n2930.0\n160\n2017\n9\n\n\n2\nArizona\n11063.0\n7946.0\n631\n2017\n9\n\n\n3\nArkansas\n4347.0\n6063.0\n366\n2017\n9\n\n\n4\nCalifornia\n37165.0\n24581.0\n0\n2017\n9\n\n\n\n\n\n\n\n\n# checking tail of data\ndf2.tail()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nyear\nmonth\n\n\n\n\n12480\nVirginia\n14.0\n2.0\n8\n1998\n11\n\n\n12481\nWashington\n65.0\n286.0\n8\n1998\n11\n\n\n12482\nWest Virginia\n149.0\n251.0\n5\n1998\n11\n\n\n12483\nWisconsin\n25.0\n214.0\n2\n1998\n11\n\n\n12484\nWyoming\n45.0\n49.0\n5\n1998\n11\n\n\n\n\n\n\n\n\n# viewing random sample\ndf2.sample(n=5)\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nyear\nmonth\n\n\n\n\n5493\nVermont\n619.0\n683.0\n46\n2009\n6\n\n\n7244\nOregon\n4271.0\n8203.0\n290\n2006\n10\n\n\n4102\nNew Jersey\n2449.0\n1754.0\n80\n2011\n7\n\n\n10568\nDistrict of Columbia\n1.0\n0.0\n0\n2001\n9\n\n\n12033\nSouth Carolina\n4369.0\n4793.0\n77\n1999\n7\n\n\n\n\n\n\n\n\n# Getting the dimensions of the DataFrame with the .shape attribute:\ndf2.shape\n\n(12485, 6)\n\n\n\n# checking for nulls\ndf2.isnull().sum()\n\nstate        0\nhandgun     20\nlong_gun    19\nmultiple     0\nyear         0\nmonth        0\ndtype: int64\n\n\n\n# revealing the nulls\nmask = df2.long_gun.isnull()\ndf2[mask]\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nyear\nmonth\n\n\n\n\n7914\nVirgin Islands\nNaN\nNaN\n0\n2005\n10\n\n\n7969\nVirgin Islands\nNaN\nNaN\n0\n2005\n9\n\n\n8024\nVirgin Islands\nNaN\nNaN\n0\n2005\n8\n\n\n8079\nVirgin Islands\nNaN\nNaN\n0\n2005\n7\n\n\n8134\nVirgin Islands\nNaN\nNaN\n0\n2005\n6\n\n\n8189\nVirgin Islands\nNaN\nNaN\n0\n2005\n5\n\n\n8299\nVirgin Islands\nNaN\nNaN\n0\n2005\n3\n\n\n8354\nVirgin Islands\nNaN\nNaN\n0\n2005\n2\n\n\n8409\nVirgin Islands\nNaN\nNaN\n0\n2005\n1\n\n\n8491\nMariana Islands\nNaN\nNaN\n0\n2004\n11\n\n\n9674\nVirgin Islands\nNaN\nNaN\n0\n2003\n2\n\n\n12011\nMariana Islands\nNaN\nNaN\n0\n1999\n7\n\n\n12066\nMariana Islands\nNaN\nNaN\n0\n1999\n6\n\n\n12121\nMariana Islands\nNaN\nNaN\n0\n1999\n5\n\n\n12176\nMariana Islands\nNaN\nNaN\n0\n1999\n4\n\n\n12231\nMariana Islands\nNaN\nNaN\n0\n1999\n3\n\n\n12286\nMariana Islands\nNaN\nNaN\n0\n1999\n2\n\n\n12341\nMariana Islands\nNaN\nNaN\n0\n1999\n1\n\n\n12396\nMariana Islands\nNaN\nNaN\n0\n1998\n12\n\n\n\n\n\n\n\nAfter doing some research, I could not find adequate data on gun purchases on these islands. If any, I figured it is going to be insubstantial anyway. So I proceed to fill these NaNs with zero.\n\n# fill nans with 0\ndf2.long_gun.fillna(0, inplace=True)\ndf2.handgun.fillna(0, inplace=True)\n\n\n# inspecting\ndf2.isnull().sum()\n\nstate       0\nhandgun     0\nlong_gun    0\nmultiple    0\nyear        0\nmonth       0\ndtype: int64\n\n\n\n#  List the data type of each column, the number of non-missing values, and memory usage with the .info method:\ndf2.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 12485 entries, 0 to 12484\nData columns (total 6 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   state     12485 non-null  object \n 1   handgun   12485 non-null  float64\n 2   long_gun  12485 non-null  float64\n 3   multiple  12485 non-null  int64  \n 4   year      12485 non-null  int64  \n 5   month     12485 non-null  int64  \ndtypes: float64(2), int64(3), object(1)\nmemory usage: 682.8+ KB\n\n\n\n# saving for later\ndf2.to_csv('Data/df2_gun_cleaned_v2.csv', index=False)\n\n\ndf2 = pd.read_csv('Data/df2_gun_cleaned_v2.csv')\ndf2.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nyear\nmonth\n\n\n\n\n0\nAlabama\n5734.0\n6320.0\n317\n2017\n9\n\n\n1\nAlaska\n2320.0\n2930.0\n160\n2017\n9\n\n\n2\nArizona\n11063.0\n7946.0\n631\n2017\n9\n\n\n3\nArkansas\n4347.0\n6063.0\n366\n2017\n9\n\n\n4\nCalifornia\n37165.0\n24581.0\n0\n2017\n9\n\n\n\n\n\n\n\nThere are 12485 rows in this dataset representing each month per state, from 1998 - 2017. The data is clean: there are no null values, and the datatypes are appropriately assigned."
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#data-dictionary-1",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#data-dictionary-1",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\n\n\ns/n\nVariable\nDescription\n\n\n\n\n0\nstate\nEach of the 50 states of the contiguous U.S.\n\n\n1\nhandgun\nPopulation estimates by July 1, 2016\n\n\n2\nlong_gun\nFemale persons, percent, by July 1, 2016\n\n\n3\nmultiple\nWhite alone, percent, by July 1, 2016\n\n\n4\nyear\nBlack or African American alone, percent by July 1, 2016\n\n\n5\nmonth\nAsian alone, percent, July 1, 2016\n\n\n\n\n# displaying summary statistics\ndf2.describe()\n\n\n\n\n\n\n\n\nhandgun\nlong_gun\nmultiple\nyear\nmonth\n\n\n\n\ncount\n12485.000000\n12485.000000\n12485.000000\n12485.000000\n12485.000000\n\n\nmean\n5931.364277\n7798.960833\n268.603364\n2007.792952\n6.484581\n\n\nstd\n8614.954626\n9307.740581\n783.185073\n5.468296\n3.451944\n\n\nmin\n0.000000\n0.000000\n0.000000\n1998.000000\n1.000000\n\n\n25%\n857.000000\n2063.000000\n15.000000\n2003.000000\n3.000000\n\n\n50%\n3046.000000\n5114.000000\n125.000000\n2008.000000\n6.000000\n\n\n75%\n7277.000000\n10374.000000\n301.000000\n2013.000000\n9.000000\n\n\nmax\n107224.000000\n108058.000000\n38907.000000\n2017.000000\n12.000000\n\n\n\n\n\n\n\n\n# Following a [NY Times recommendation], gun sales can be approximated from check calls \n# by multiplying checks for handgun and long_gun by a factor of 1.1 and that of multiple by 2\n\ndf2['handgun'] = df2['handgun'].mul(1.1)\ndf2['long_gun'] = df2['long_gun'].mul(1.1)\ndf2['multiple'] = df2['multiple'].mul(2)\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nyear\nmonth\n\n\n\n\n0\nAlabama\n6307.4\n6952.0\n634\n2017\n9\n\n\n1\nAlaska\n2552.0\n3223.0\n320\n2017\n9\n\n\n2\nArizona\n12169.3\n8740.6\n1262\n2017\n9\n\n\n3\nArkansas\n4781.7\n6669.3\n732\n2017\n9\n\n\n4\nCalifornia\n40881.5\n27039.1\n0\n2017\n9\n\n\n\n\n\n\n\n\n# I am interested in gun trends in 2006, 2015 and 2016\n# so I extract data for those years\n\ndf2_2006 = df2[df2['year'] == 2006]\ndf2_2015 = df2[df2['year'] == 2015]\ndf2_2016 = df2[df2['year'] == 2016]\n\n\n# Since the year variable is now redundant, I opt to drop them\n# I use the reset_index method to have the index start from 0\n\ndf2_2006 = df2_2006.drop(columns='year').reset_index(drop=True)\ndf2_2015 = df2_2015.drop(columns='year').reset_index(drop=True)\ndf2_2016 = df2_2016.drop(columns='year').reset_index(drop=True)\n\n\n# checking the result\ndf2_2006.head(3)\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\n\n\n\n\n0\nAlabama\n11031.9\n28608.8\n882\n12\n\n\n1\nAlaska\n2190.1\n3166.9\n162\n12\n\n\n2\nArizona\n9188.3\n12136.3\n690\n12\n\n\n\n\n\n\n\n\n# saving for later\ndf2_2006.to_csv('Data/df2_2006.csv')\ndf2_2015.to_csv('Data/df2_2015.csv')\ndf2_2016.to_csv('Data/df2_2016.csv')\n\nViewing the head of the three dataframes\n\ndf2_2006 = pd.read_csv('Data/df2_2006.csv', index_col=0)\ndf2_2015 = pd.read_csv('Data/df2_2015.csv', index_col=0)\ndf2_2016 = pd.read_csv('Data/df2_2016.csv', index_col=0)\n\ndf2_2006.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\n\n\n\n\n0\nAlabama\n11031.9\n28608.8\n882\n12\n\n\n1\nAlaska\n2190.1\n3166.9\n162\n12\n\n\n2\nArizona\n9188.3\n12136.3\n690\n12\n\n\n3\nArkansas\n4814.7\n17008.2\n460\n12\n\n\n4\nCalifornia\n17902.5\n22993.3\n15684\n12\n\n\n\n\n\n\n\n\ndf2_2015.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\n\n\n\n\n0\nAlabama\n52365.5\n37081.0\n3504\n12\n\n\n1\nAlaska\n5791.5\n5101.8\n428\n12\n\n\n2\nArizona\n28118.2\n16197.5\n1990\n12\n\n\n3\nArkansas\n15158.0\n16765.1\n1042\n12\n\n\n4\nCalifornia\n81838.9\n53638.2\n0\n12\n\n\n\n\n\n\n\n\ndf2_2016.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\n\n\n\n\n0\nAlabama\n15106.3\n19270.9\n1066\n12\n\n\n1\nAlaska\n4495.7\n4067.8\n382\n12\n\n\n2\nArizona\n19826.4\n15163.5\n1406\n12\n\n\n3\nArkansas\n10111.2\n14776.3\n770\n12\n\n\n4\nCalifornia\n70013.9\n102546.4\n0\n12"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#exploratory-data-analysis-2",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#exploratory-data-analysis-2",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Exploratory Data Analysis 2",
    "text": "Exploratory Data Analysis 2\n\n(Gun Sales Data)\nIn this section, I will answer the following research questions:\n\nYear Level Questions: - How many guns were sold in the U.S. in 2006? - What kind of gun was bought the most? - What are the overall monthly trends of gun purchases in 2006?\nState Level Questions\n\nWhat are the top 5 states in terms of gun purchases in the U.S. in 2006?\nWhat are the most bought gun types in those states?\nWhat kind of gun was bought the most in one of those states?\nWhat are the overall monthly trends of gun purchases in those states?\n\nWhich states had the highest growth in gun sales over the span of a decade?"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#section",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#section",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "2006",
    "text": "2006\n\ndf2_2006 = pd.read_csv('Data/df2_2006.csv', index_col=0)\ndf2_2006.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\n\n\n\n\n0\nAlabama\n11031.9\n28608.8\n882\n12\n\n\n1\nAlaska\n2190.1\n3166.9\n162\n12\n\n\n2\nArizona\n9188.3\n12136.3\n690\n12\n\n\n3\nArkansas\n4814.7\n17008.2\n460\n12\n\n\n4\nCalifornia\n17902.5\n22993.3\n15684\n12\n\n\n\n\n\n\n\n\ndf2_2006.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nhandgun\n660.0\n4068.875000\n4725.841945\n0.0\n671.00\n2560.25\n5929.00\n27432.9\n\n\nlong_gun\n660.0\n7982.651667\n9051.901614\n0.0\n2091.10\n5649.60\n10799.80\n68093.3\n\n\nmultiple\n660.0\n394.293939\n779.232091\n0.0\n55.50\n232.00\n522.00\n15684.0\n\n\nmonth\n660.0\n6.500000\n3.454671\n1.0\n3.75\n6.50\n9.25\n12.0\n\n\n\n\n\n\n\n\n# plotting the kde of the variables to have a sense of the data distribution\n# check how to change line color\n\nsns.set_style('darkgrid')\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 4))\n\ndf2_2006[\"handgun\"].plot.kde(ax=ax1)\nax1.set_title('KDE plot for handgun')\n\ndf2_2006[\"long_gun\"].plot.kde(ax=ax2)\nax2.set_title('KDE plot for long_gun')\n\ndf2_2006[\"multiple\"].plot.kde(ax=ax3)\nax3.set_title('KDE plot for multiple')\n\ndf2_2006[\"month\"].plot.kde(ax=ax4)\nax4.set_title('KDE plot for month')\n\nplt.show()\n\n\n\n\n\nYear Level Questions\n\nHow many guns were sold in the U.S. in 2006?\nWhat kind of gun was bought the most?\nWhat are the overall monthly trends of gun purchases in 2006?\n\nHow many guns were sold in the U.S. in 2006?\n\ndf2_2006[['handgun', 'long_gun', 'multiple']].sum().sum()\n\n8214241.6000000015\n\n\nWhat type of gun sold the most?\n\nsns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(8,6))\n\nd = df2_2006[['handgun', 'long_gun', 'multiple']].sum().sort_values(ascending=False)\n\ndf2_2006[['handgun', 'long_gun', 'multiple']].sum().plot.barh(table=d)\nplt.setp(ax.get_xticklabels(), y=.05)\nplt.setp(ax.get_xticklines(), visible=False)\n\nprint(df2_2006[['handgun', 'long_gun', 'multiple']].sum().sort_values(ascending=False))\n\nlong_gun    5268550.1\nhandgun     2685457.5\nmultiple     260234.0\ndtype: float64\n\n\n\n\n\nWhat is the trend of gun sales throughout the year in 2006?\n\ndf2_2006.groupby('month')[['handgun', 'long_gun', 'multiple']].sum().plot.line(title='Monthly Trends of Gun Sales in 2006')\n\n&lt;Axes: title={'center': 'Monthly Trends of Gun Sales in 2006'}, xlabel='month'&gt;\n\n\n\n\n\n\n\nState Level Questions\n\nWhat are the top 5 states in terms of gun purchases in the U.S. in 2006?\nWhat are the most bought gun types in those states?\nWhat are the overall monthly trends of gun purchases in those states?\n\nWhat are the top 5 states in terms of gun purchases in the U.S. in 2006?\n\ndf2_2006.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\n\n\n\n\n0\nAlabama\n11031.9\n28608.8\n882\n12\n\n\n1\nAlaska\n2190.1\n3166.9\n162\n12\n\n\n2\nArizona\n9188.3\n12136.3\n690\n12\n\n\n3\nArkansas\n4814.7\n17008.2\n460\n12\n\n\n4\nCalifornia\n17902.5\n22993.3\n15684\n12\n\n\n\n\n\n\n\n\ndf2_2006['total'] = df2_2006['handgun'] + df2_2006['long_gun'] + df2_2006['multiple']\ndf2_2006.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\ntotal\n\n\n\n\n0\nAlabama\n11031.9\n28608.8\n882\n12\n40522.7\n\n\n1\nAlaska\n2190.1\n3166.9\n162\n12\n5519.0\n\n\n2\nArizona\n9188.3\n12136.3\n690\n12\n22014.6\n\n\n3\nArkansas\n4814.7\n17008.2\n460\n12\n22282.9\n\n\n4\nCalifornia\n17902.5\n22993.3\n15684\n12\n56579.8\n\n\n\n\n\n\n\n\nsns.set_style('darkgrid')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\ndf2_2006.groupby('state')['total'].sum().sort_values(ascending=False).head().plot.barh(ax=ax1, title='Top 5 States by Gun Sales')\ndf2_2006.groupby('state')[['handgun', 'long_gun', 'multiple','total']].sum().nlargest(5, 'total').plot.bar(ax=ax2, title='Breakdown of Gun Sales by Type')\n\n&lt;Axes: title={'center': 'Breakdown of Gun Sales by Type'}, xlabel='state'&gt;\n\n\n\n\n\nWhat are the top 5 states in terms of gun purchases in the U.S. in 2006?\nWhat are the most bought gun types in those states?\n\n# What are the overall monthly trends of gun purchases overall and in those top 5 states?\n# i.e month on x, sales on y, faceted by states\n\ndf2_2006.groupby('month')['total'].sum().plot.line(title = 'Overall Monthly Trend of Gun Sales in the U.S. in 2006')\n\n&lt;Axes: title={'center': 'Overall Monthly Trend of Gun Sales in the U.S. in 2006'}, xlabel='month'&gt;\n\n\n\n\n\n\nfig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(25, 5))\ncrit_1 = df2_2006.state == 'Texas'\ncrit_2 = df2_2006.state == 'Pennsylvania'\ncrit_3 = df2_2006.state == 'California'\ncrit_4 = df2_2006.state == 'Florida'\ncrit_5 = df2_2006.state == 'Ohio'\n\ndf2_2006[crit_1].groupby('month')['total'].sum().plot.line(\n    ax=ax1, title='Monthly Gun Sales Trends in Texas')\ndf2_2006[crit_2].groupby('month')['total'].sum().plot.line(\n    ax=ax2, title='Monthly Gun Sales Trends in Pennsylvania')\ndf2_2006[crit_3].groupby('month')['total'].sum().plot.line(\n    ax=ax3, title='Monthly Gun Sales Trends in California')\ndf2_2006[crit_4].groupby('month')['total'].sum().plot.line(\n    ax=ax4, title='Monthly Gun Sales Trends in Florida')\ndf2_2006[crit_5].groupby('month')['total'].sum().plot.line(\n    ax=ax5, title='Monthly Gun Sales Trends in Ohio')\n\n&lt;Axes: title={'center': 'Monthly Gun Sales Trends in Ohio'}, xlabel='month'&gt;"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#section-1",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#section-1",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "2016",
    "text": "2016\n\n# reading in the data\ndf2_2016 = pd.read_csv('Data/df2_2016.csv', index_col=0)\ndf2_2016.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\n\n\n\n\n0\nAlabama\n15106.3\n19270.9\n1066\n12\n\n\n1\nAlaska\n4495.7\n4067.8\n382\n12\n\n\n2\nArizona\n19826.4\n15163.5\n1406\n12\n\n\n3\nArkansas\n10111.2\n14776.3\n770\n12\n\n\n4\nCalifornia\n70013.9\n102546.4\n0\n12\n\n\n\n\n\n\n\n\n# deriving summary statistics\ndf2_2016.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nhandgun\n660.0\n13475.830000\n15766.508622\n0.0\n2685.925\n7737.95\n18134.325\n79888.6\n\n\nlong_gun\n660.0\n9980.851667\n10916.172474\n0.0\n3268.650\n6524.10\n14035.450\n102546.4\n\n\nmultiple\n660.0\n780.978788\n1099.305994\n0.0\n16.000\n401.00\n1058.000\n7782.0\n\n\nmonth\n660.0\n6.500000\n3.454671\n1.0\n3.750\n6.50\n9.250\n12.0\n\n\n\n\n\n\n\n\n# plotting the kde of the variables to have a sense of the data distribution\n# check how to change line color\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20, 4))\n\ndf2_2016[\"handgun\"].plot.kde(ax=ax1)\nax1.set_title('KDE plot for handgun')\n\ndf2_2016[\"long_gun\"].plot.kde(ax=ax2)\nax2.set_title('KDE plot for long_gun')\n\ndf2_2016[\"multiple\"].plot.kde(ax=ax3)\nax3.set_title('KDE plot for multiple')\n\ndf2_2016[\"month\"].plot.kde(ax=ax4)\nax4.set_title('KDE plot for month')\n\nplt.show()\n\n\n\n\n\nYear Level Questions\n\nHow many guns were sold in the U.S. in 2016?\nWhat kind of gun was bought the most?\nWhat are the overall monthly trends of gun purchases in 2016?\n\nHow many guns were sold in the U.S. in 2016?\n\ndf2_2016[['handgun', 'long_gun', 'multiple']].sum().sum()\n\n15996855.9\n\n\nwhat type of gun sold the most?\n\nfig, ax = plt.subplots(figsize=(8,6))\n\nd = df2_2016[['handgun', 'long_gun', 'multiple']].sum().sort_values(ascending=False)\n\ndf2_2016[['handgun', 'long_gun', 'multiple']].sum().plot.barh(table=d)\nplt.setp(ax.get_xticklabels(), y=.05)\nplt.setp(ax.get_xticklines(), visible=False)\n\nprint(df2_2016[['handgun', 'long_gun', 'multiple']].sum().sort_values(ascending=False))\n\nhandgun     8894047.8\nlong_gun    6587362.1\nmultiple     515446.0\ndtype: float64\n\n\n\n\n\nTrend of gun sales throughout the year in 2016\n\ndf2_2016.groupby('month')[['handgun', 'long_gun', 'multiple']].sum().plot.line(title='Monthly Trends of Gun Sales in 2016')\n\n&lt;Axes: title={'center': 'Monthly Trends of Gun Sales in 2016'}, xlabel='month'&gt;\n\n\n\n\n\n\n\nState Level Questions\n\nWhat are the top 5 states in terms of gun purchases in the U.S. in 2016?\nWhat are the most bought gun types in those states?\nWhat was the most purchased gun in those states?\nWhat are the overall monthly trends of gun purchases in those states?\n\n\ndf2_2016.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\n\n\n\n\n0\nAlabama\n15106.3\n19270.9\n1066\n12\n\n\n1\nAlaska\n4495.7\n4067.8\n382\n12\n\n\n2\nArizona\n19826.4\n15163.5\n1406\n12\n\n\n3\nArkansas\n10111.2\n14776.3\n770\n12\n\n\n4\nCalifornia\n70013.9\n102546.4\n0\n12\n\n\n\n\n\n\n\n\ndf2_2016['total'] = df2_2016['handgun'] + df2_2016['long_gun'] + df2_2016['multiple']\ndf2_2016.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\ntotal\n\n\n\n\n0\nAlabama\n15106.3\n19270.9\n1066\n12\n35443.2\n\n\n1\nAlaska\n4495.7\n4067.8\n382\n12\n8945.5\n\n\n2\nArizona\n19826.4\n15163.5\n1406\n12\n36395.9\n\n\n3\nArkansas\n10111.2\n14776.3\n770\n12\n25657.5\n\n\n4\nCalifornia\n70013.9\n102546.4\n0\n12\n172560.3\n\n\n\n\n\n\n\nWhat are the top 5 states in terms of gun purchases in the U.S. in 2016? What are the most bought gun types in those states?\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\ndf2_2016.groupby('state')['total'].sum().sort_values(ascending=False).head().plot.barh(ax=ax1, title='Top 5 States by Gun Sales')\ndf2_2016.groupby('state')[['handgun', 'long_gun', 'multiple','total']].sum().nlargest(5, 'total').plot.bar(ax=ax2, title='Breakdown of Gun Sales by Type')\n\n&lt;Axes: title={'center': 'Breakdown of Gun Sales by Type'}, xlabel='state'&gt;\n\n\n\n\n\nWhat are the overall monthly trends of gun purchases?\n\ndf2_2016.groupby('month')['total'].sum().plot.line(title = 'Overall Monthly Trend of Gun Sales in the U.S. in 2016')\n\n&lt;Axes: title={'center': 'Overall Monthly Trend of Gun Sales in the U.S. in 2016'}, xlabel='month'&gt;\n\n\n\n\n\n\nfig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize=(25,5))\ncrit_1 = df2_2016.state == 'Texas'\ncrit_2 = df2_2016.state == 'California'\ncrit_3 = df2_2016.state == 'Florida'\ncrit_4 = df2_2016.state == 'Pennsylvania'\ncrit_5 = df2_2016.state == 'Ohio'\n\ndf2_2016[crit_1].groupby('month')['total'].sum().plot.line(ax=ax1, title='Monthly Gun Sales Trends in Texas')\ndf2_2016[crit_2].groupby('month')['total'].sum().plot.line(ax=ax2, title='Monthly Gun Sales Trends in California')\ndf2_2016[crit_3].groupby('month')['total'].sum().plot.line(ax=ax3, title='Monthly Gun Sales Trends in Florida')\ndf2_2016[crit_4].groupby('month')['total'].sum().plot.line(ax=ax4, title='Monthly Gun Sales Trends in Pennsylvania')\ndf2_2016[crit_5].groupby('month')['total'].sum().plot.line(ax=ax5, title='Monthly Gun Sales Trends in Ohio')\n\n&lt;Axes: title={'center': 'Monthly Gun Sales Trends in Ohio'}, xlabel='month'&gt;\n\n\n\n\n\nWhich states have the highest growth in gun registrations over the span of a decade? \nTo answer this question, I will:\n\nLoad the 2016 dataframe\nCreate a new column named total which is the sum of the three form of gun sales\nDrop the individual form of gun sales as they are now irrelevant to the research question\nRename the columns for clarity\nFollow the above procedure for the 2006 dataframe\nMerge both dataframes on state \nSubtract total_2016 from total_2006’ to get a column for change\nCreate a percentage change column\nSort the dataframe by %_change and visualize the result\n\n\n# 2016 dataframe is loaded\ndf2_2016.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\ntotal\n\n\n\n\n0\nAlabama\n15106.3\n19270.9\n1066\n12\n35443.2\n\n\n1\nAlaska\n4495.7\n4067.8\n382\n12\n8945.5\n\n\n2\nArizona\n19826.4\n15163.5\n1406\n12\n36395.9\n\n\n3\nArkansas\n10111.2\n14776.3\n770\n12\n25657.5\n\n\n4\nCalifornia\n70013.9\n102546.4\n0\n12\n172560.3\n\n\n\n\n\n\n\n\ndf2_2016['total'] = df2_2016['handgun'] + df2_2016['long_gun'] + df2_2016['multiple']\n\n\n# aggregating df2 on the gun sales columns\ndf2_2016 = df2_2016.groupby('state')[['handgun','long_gun','multiple', 'total']].sum()\n\n\n# dropping columns\ncols = ['handgun', 'long_gun', 'multiple']\ndf2_2016.drop(columns=cols, inplace=True)\ndf2_2016.head()\n\n\n\n\n\n\n\n\ntotal\n\n\nstate\n\n\n\n\n\nAlabama\n315684.6\n\n\nAlaska\n86447.8\n\n\nArizona\n319165.2\n\n\nArkansas\n187400.4\n\n\nCalifornia\n1226395.5\n\n\n\n\n\n\n\n\n# renaming for clarity\ndf2_2016.rename(columns=lambda x: x + \"_2016\", inplace=True)\ndf2_2016.head()\n\n\n\n\n\n\n\n\ntotal_2016\n\n\nstate\n\n\n\n\n\nAlabama\n315684.6\n\n\nAlaska\n86447.8\n\n\nArizona\n319165.2\n\n\nArkansas\n187400.4\n\n\nCalifornia\n1226395.5\n\n\n\n\n\n\n\nNow I follow the sme procedures for the 2006 dataframe\n\ndf2_2006.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\ntotal\n\n\n\n\n0\nAlabama\n11031.9\n28608.8\n882\n12\n40522.7\n\n\n1\nAlaska\n2190.1\n3166.9\n162\n12\n5519.0\n\n\n2\nArizona\n9188.3\n12136.3\n690\n12\n22014.6\n\n\n3\nArkansas\n4814.7\n17008.2\n460\n12\n22282.9\n\n\n4\nCalifornia\n17902.5\n22993.3\n15684\n12\n56579.8\n\n\n\n\n\n\n\n\ndf2_2006['total'] = df2_2006['handgun'] + df2_2006['long_gun'] + df2_2006['multiple']\n\n\ndf2_2006 = df2_2006.groupby('state')[['handgun','long_gun','multiple', 'total']].sum()\n\n\ncols = ['handgun', 'long_gun', 'multiple']\ndf2_2006.drop(columns=cols, inplace=True)\ndf2_2006.head()\n\n\n\n\n\n\n\n\ntotal\n\n\nstate\n\n\n\n\n\nAlabama\n232324.7\n\n\nAlaska\n48404.4\n\n\nArizona\n177284.5\n\n\nArkansas\n142397.6\n\n\nCalifornia\n504970.9\n\n\n\n\n\n\n\n\n# rename columns for clarity\ndf2_2006.rename(columns=lambda x: x + \"_2006\", inplace=True)\ndf2_2006.head()\n\n\n\n\n\n\n\n\ntotal_2006\n\n\nstate\n\n\n\n\n\nAlabama\n232324.7\n\n\nAlaska\n48404.4\n\n\nArizona\n177284.5\n\n\nArkansas\n142397.6\n\n\nCalifornia\n504970.9\n\n\n\n\n\n\n\nNow, I merge both dataframes on ‘state’\n\n# merging the dataframe\ncombined_df = df2_2016.merge(df2_2006, on='state', how='inner')\ncombined_df.head()\n\n\n\n\n\n\n\n\ntotal_2016\ntotal_2006\n\n\nstate\n\n\n\n\n\n\nAlabama\n315684.6\n232324.7\n\n\nAlaska\n86447.8\n48404.4\n\n\nArizona\n319165.2\n177284.5\n\n\nArkansas\n187400.4\n142397.6\n\n\nCalifornia\n1226395.5\n504970.9\n\n\n\n\n\n\n\n\n# add column for total_change\ncombined_df['total_change'] = combined_df['total_2016'] - combined_df['total_2006']\ncombined_df.head()\n\n\n\n\n\n\n\n\ntotal_2016\ntotal_2006\ntotal_change\n\n\nstate\n\n\n\n\n\n\n\nAlabama\n315684.6\n232324.7\n83359.9\n\n\nAlaska\n86447.8\n48404.4\n38043.4\n\n\nArizona\n319165.2\n177284.5\n141880.7\n\n\nArkansas\n187400.4\n142397.6\n45002.8\n\n\nCalifornia\n1226395.5\n504970.9\n721424.6\n\n\n\n\n\n\n\n\n# adding a column for % change and rounding to 2 d.f.\ncombined_df['%_change'] = ((combined_df['total_change'] / combined_df['total_2006']) * 100).round(2)\ncombined_df.head()\n\n\n\n\n\n\n\n\ntotal_2016\ntotal_2006\ntotal_change\n%_change\n\n\nstate\n\n\n\n\n\n\n\n\nAlabama\n315684.6\n232324.7\n83359.9\n35.88\n\n\nAlaska\n86447.8\n48404.4\n38043.4\n78.59\n\n\nArizona\n319165.2\n177284.5\n141880.7\n80.03\n\n\nArkansas\n187400.4\n142397.6\n45002.8\n31.60\n\n\nCalifornia\n1226395.5\n504970.9\n721424.6\n142.86\n\n\n\n\n\n\n\n\n# states with the greatest changes in gun sales from 2006 to 2016\ncombined_df.sort_values(by='%_change', ascending=False)\n\n\n\n\n\n\n\n\ntotal_2016\ntotal_2006\ntotal_change\n%_change\n\n\nstate\n\n\n\n\n\n\n\n\nGuam\n2233.9\n390.9\n1843.0\n471.48\n\n\nIllinois\n606898.6\n181086.6\n425812.0\n235.14\n\n\nNew Jersey\n131282.8\n41572.1\n89710.7\n215.80\n\n\nConnecticut\n182806.8\n58656.7\n124150.1\n211.66\n\n\nMassachusetts\n134658.8\n43577.9\n91080.9\n209.01\n\n\nDelaware\n54120.0\n18561.4\n35558.6\n191.57\n\n\nDistrict of Columbia\n811.3\n279.0\n532.3\n190.79\n\n\nRhode Island\n30910.6\n11853.8\n19056.8\n160.77\n\n\nIndiana\n519104.0\n200853.4\n318250.6\n158.45\n\n\nNew Hampshire\n124641.0\n48451.3\n76189.7\n157.25\n\n\nFlorida\n1126678.0\n448412.9\n678265.1\n151.26\n\n\nMissouri\n550714.2\n222703.8\n328010.4\n147.29\n\n\nCalifornia\n1226395.5\n504970.9\n721424.6\n142.86\n\n\nTennessee\n604447.0\n257610.1\n346836.9\n134.64\n\n\nWashington\n417704.5\n190076.1\n227628.4\n119.76\n\n\nColorado\n493366.8\n227299.8\n266067.0\n117.06\n\n\nOhio\n735052.3\n343179.8\n391872.5\n114.19\n\n\nNew York\n368982.5\n173129.4\n195853.1\n113.13\n\n\nVirginia\n556531.8\n265476.9\n291054.9\n109.63\n\n\nMaryland\n137290.7\n65591.9\n71698.8\n109.31\n\n\nSouth Carolina\n250854.6\n126091.3\n124763.3\n98.95\n\n\nTexas\n1297197.5\n657275.5\n639922.0\n97.36\n\n\nMaine\n110103.5\n56676.4\n53427.1\n94.27\n\n\nOklahoma\n347686.5\n179211.3\n168475.2\n94.01\n\n\nOregon\n353919.5\n182485.6\n171433.9\n93.94\n\n\nWisconsin\n381738.4\n205512.6\n176225.8\n85.75\n\n\nVermont\n42660.3\n23028.2\n19632.1\n85.25\n\n\nPuerto Rico\n17264.5\n9504.8\n7759.7\n81.64\n\n\nSouth Dakota\n99014.2\n54671.6\n44342.6\n81.11\n\n\nNevada\n122135.3\n67496.1\n54639.2\n80.95\n\n\nArizona\n319165.2\n177284.5\n141880.7\n80.03\n\n\nAlaska\n86447.8\n48404.4\n38043.4\n78.59\n\n\nNew Mexico\n145071.8\n83798.5\n61273.3\n73.12\n\n\nLouisiana\n358999.0\n214295.7\n144703.3\n67.53\n\n\nWest Virginia\n201117.9\n122062.6\n79055.3\n64.77\n\n\nMinnesota\n315347.5\n193136.1\n122211.4\n63.28\n\n\nNorth Dakota\n61330.8\n38198.7\n23132.1\n60.56\n\n\nIdaho\n117137.4\n72988.4\n44149.0\n60.49\n\n\nKansas\n178892.4\n112153.3\n66739.1\n59.51\n\n\nMichigan\n328993.7\n209450.2\n119543.5\n57.07\n\n\nPennsylvania\n880410.3\n571519.3\n308891.0\n54.05\n\n\nMississippi\n243585.2\n168252.6\n75332.6\n44.77\n\n\nMontana\n109175.1\n75877.6\n33297.5\n43.88\n\n\nWyoming\n55812.4\n39540.8\n16271.6\n41.15\n\n\nAlabama\n315684.6\n232324.7\n83359.9\n35.88\n\n\nGeorgia\n370340.7\n273600.3\n96740.4\n35.36\n\n\nArkansas\n187400.4\n142397.6\n45002.8\n31.60\n\n\nNorth Carolina\n218489.4\n166084.1\n52405.3\n31.55\n\n\nKentucky\n288612.1\n226781.0\n61831.1\n27.26\n\n\nUtah\n114069.4\n95817.3\n18252.1\n19.05\n\n\nVirgin Islands\n365.3\n354.5\n10.8\n3.05\n\n\nNebraska\n31196.1\n32570.0\n-1373.9\n-4.22\n\n\nIowa\n41988.4\n51636.0\n-9647.6\n-18.68\n\n\nMariana Islands\n17.6\n25.3\n-7.7\n-30.43\n\n\nHawaii\n0.0\n0.0\n0.0\nNaN\n\n\n\n\n\n\n\nVisualizing results\n\ncombined_df['%_change'].head(10).plot.line(\n    x='state', y='%_change', title='', figsize=(20, 4))\n\n&lt;Axes: xlabel='state'&gt;"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#which-census-data-variables-are-most-associated-with-gun-sales-data",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#which-census-data-variables-are-most-associated-with-gun-sales-data",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Which census data variables are most associated with gun sales data?",
    "text": "Which census data variables are most associated with gun sales data?\nTo answer this question, I proceed merge the dataframes\n\n# reading in the census data\ndf1 = pd.read_csv('Data/df_census_cleaned_v2.csv')\ndf1.head()\n\n\n\n\n\n\n\n\nstate\npop_2016\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\npe_hh\npe_whi\nadults_below_65\nhigh_school_grad_higher\nbsc_higher\nmedian_hu\nmedian_gross_rent\npoverty_pct\nmedian_hh_income\nper_cap_inc\nfirms\n\n\n\n\n0\nAlabama\n4863300.0\n51.6\n69.3\n26.8\n1.4\n2.55\n10.7\n61.3\n84.3\n23.5\n125500.0\n717.0\n17.1\n43623.0\n24091.0\n374153.0\n\n\n1\nAlaska\n741894.0\n47.7\n66.1\n3.8\n6.3\n2.81\n15.5\n64.4\n92.1\n28.0\n250000.0\n1146.0\n9.9\n72515.0\n33413.0\n68032.0\n\n\n2\nArizona\n6931071.0\n50.3\n83.3\n4.9\n3.4\n2.69\n11.9\n59.6\n86.0\n27.5\n167500.0\n913.0\n16.4\n50255.0\n25848.0\n499926.0\n\n\n3\nArkansas\n2988248.0\n50.9\n79.4\n15.7\n1.6\n2.53\n9.3\n60.1\n84.8\n21.1\n111400.0\n677.0\n17.2\n41371.0\n22798.0\n231959.0\n\n\n4\nCalifornia\n39250017.0\n50.3\n72.7\n6.5\n14.8\n2.96\n8.3\n63.2\n81.8\n31.4\n385500.0\n1255.0\n14.3\n61818.0\n30318.0\n3548449.0\n\n\n\n\n\n\n\n\nData Dictionary\n\n\n\n\n\n\n\n\ns/n\nVariable\nDescription\n\n\n\n\n0\nstate\nEach of the 50 states of the contiguous U.S.\n\n\n1\npop_2016\nPopulation estimates by July 1, 2016\n\n\n2\nfemale_pct\nFemale persons, percent, by July 1, 2016\n\n\n3\nwhite_pct\nWhite alone, percent, by July 1, 2016\n\n\n4\nblack_pct\nBlack or African American alone, percent by July 1, 2016\n\n\n5\nasian_pct\nAsian alone, percent, July 1, 2016\n\n\n6\npe_hh\nPersons per household, 2011-2015\n\n\n7\npe_whi\nPersons without health insurance, under age 65 years, percent\n\n\n8\nadults_below_65\nadults older than 18, younger than 65, percent, 2016\n\n\n9\nhigh_school_grad_higher\nHigh school graduate or higher, percent of persons age 25 years+, 2011-2015\n\n\n10\nbsc_higher\nBachelor’s degree or higher, percent of persons age 25 years+, 2011-2015\n\n\n11\nmedian_hu\nMedian value of owner-occupied housing units, 2011-2015\n\n\n12\nmedian_gross_rent\nMedian gross rent, 2011-2015\n\n\n13\npoverty_pct\nPersons in poverty, percent\n\n\n14\nmedian_hh_income\nMedian household income (in 2015 dollars), 2011-2015\n\n\n15\nper_cap_inc\nPer capita income in past 12 months (in 2015 dollars), 2011-2015\n\n\n16\nfirms\nAll firms, 2012\n\n\n\n\n# reading in the gun data \ndf2 = pd.read_csv('Data/df2_2016.csv', index_col=0)\ndf2.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\nmonth\n\n\n\n\n0\nAlabama\n15106.3\n19270.9\n1066\n12\n\n\n1\nAlaska\n4495.7\n4067.8\n382\n12\n\n\n2\nArizona\n19826.4\n15163.5\n1406\n12\n\n\n3\nArkansas\n10111.2\n14776.3\n770\n12\n\n\n4\nCalifornia\n70013.9\n102546.4\n0\n12\n\n\n\n\n\n\n\n\n\nData Dictionary\n\n\n\n\n\n\n\n\ns/n\nVariable\nDescription\n\n\n\n\n0\nstate\nEach of the 55 states and territories of the .U.S.\n\n\n1\nhandgun\nhandgun checks, transformed into sales by a factor of 1.1\n\n\n2\nlong_gun\nlong-gun checks, transformed into sales by a factor of 1.1\n\n\n3\nmultiple\nmultiple gun checks, transformed into sales by a factor of 2\n\n\n4\nmonth\nmonths of 2016\n\n\n\n\n# number of unique states in census dataframe\ndf1.state.nunique()\n\n50\n\n\n\n# number of unique states in gun sales dataframe\ndf2.state.nunique()\n\n55\n\n\n\n# getting the uncommon territories in both datasets\ns1 = df1.state\ns2 = df2.state.unique()\ncommon_states = s1[s1.isin(s2)]\n\ns3 = df2.query(\"state not in @common_states\")\ns3.loc[:, 'state'].unique()\n\narray(['District of Columbia', 'Guam', 'Mariana Islands', 'Puerto Rico',\n       'Virgin Islands'], dtype=object)\n\n\nThese territories are part of the commonwealth of the United States but are not part of the mainland.\n\n# creating 'total' column\ndf2['total'] = df2['handgun'] + df2['long_gun'] + df2['multiple']\n\n\n# aggregating df2 on the gun sales columns\ndf2_agg = df2.groupby('state')[['handgun','long_gun','multiple', 'total']].sum()\ndf2_agg\n\n\n\n\n\n\n\n\nhandgun\nlong_gun\nmultiple\ntotal\n\n\nstate\n\n\n\n\n\n\n\n\nAlabama\n168435.3\n134159.3\n13090\n315684.6\n\n\nAlaska\n41240.1\n40575.7\n4632\n86447.8\n\n\nArizona\n183462.4\n119886.8\n15816\n319165.2\n\n\nArkansas\n88268.4\n90332.0\n8800\n187400.4\n\n\nCalifornia\n616390.5\n610005.0\n0\n1226395.5\n\n\nColorado\n266752.2\n198028.6\n28586\n493366.8\n\n\nConnecticut\n134612.5\n48194.3\n0\n182806.8\n\n\nDelaware\n28131.4\n23920.6\n2068\n54120.0\n\n\nDistrict of Columbia\n742.5\n52.8\n16\n811.3\n\n\nFlorida\n728538.8\n347855.2\n50284\n1126678.0\n\n\nGeorgia\n214280.0\n141104.7\n14956\n370340.7\n\n\nGuam\n1373.9\n726.0\n134\n2233.9\n\n\nHawaii\n0.0\n0.0\n0\n0.0\n\n\nIdaho\n51175.3\n61150.1\n4812\n117137.4\n\n\nIllinois\n387652.1\n194980.5\n24266\n606898.6\n\n\nIndiana\n306000.2\n194939.8\n18164\n519104.0\n\n\nIowa\n3069.0\n38779.4\n140\n41988.4\n\n\nKansas\n89227.6\n81122.8\n8542\n178892.4\n\n\nKentucky\n154793.1\n119933.0\n13886\n288612.1\n\n\nLouisiana\n188746.8\n153034.2\n17218\n358999.0\n\n\nMaine\n53729.5\n51458.0\n4916\n110103.5\n\n\nMariana Islands\n0.0\n17.6\n0\n17.6\n\n\nMaryland\n57535.5\n79081.2\n674\n137290.7\n\n\nMassachusetts\n83574.7\n46124.1\n4960\n134658.8\n\n\nMichigan\n181365.8\n141997.9\n5630\n328993.7\n\n\nMinnesota\n147358.2\n157897.3\n10092\n315347.5\n\n\nMississippi\n130955.0\n102104.2\n10526\n243585.2\n\n\nMissouri\n302229.4\n222978.8\n25506\n550714.2\n\n\nMontana\n41301.7\n62517.4\n5356\n109175.1\n\n\nNebraska\n1888.7\n29143.4\n164\n31196.1\n\n\nNevada\n72773.8\n41915.5\n7446\n122135.3\n\n\nNew Hampshire\n75381.9\n49149.1\n110\n124641.0\n\n\nNew Jersey\n77273.9\n54008.9\n0\n131282.8\n\n\nNew Mexico\n76377.4\n61230.4\n7464\n145071.8\n\n\nNew York\n146613.5\n217437.0\n4932\n368982.5\n\n\nNorth Carolina\n18254.5\n194610.9\n5624\n218489.4\n\n\nNorth Dakota\n21972.5\n37414.3\n1944\n61330.8\n\n\nOhio\n434027.0\n268359.3\n32666\n735052.3\n\n\nOklahoma\n182799.1\n143565.4\n21322\n347686.5\n\n\nOregon\n196587.6\n157331.9\n0\n353919.5\n\n\nPennsylvania\n706455.2\n173955.1\n0\n880410.3\n\n\nPuerto Rico\n14056.9\n2657.6\n550\n17264.5\n\n\nRhode Island\n15688.2\n11224.4\n3998\n30910.6\n\n\nSouth Carolina\n145720.3\n96154.3\n8980\n250854.6\n\n\nSouth Dakota\n40172.0\n54474.2\n4368\n99014.2\n\n\nTennessee\n351085.9\n231177.1\n22184\n604447.0\n\n\nTexas\n701223.6\n529769.9\n66204\n1297197.5\n\n\nUtah\n52141.1\n59546.3\n2382\n114069.4\n\n\nVermont\n20152.0\n20738.3\n1770\n42660.3\n\n\nVirgin Islands\n292.6\n62.7\n10\n365.3\n\n\nVirginia\n325559.3\n230972.5\n0\n556531.8\n\n\nWashington\n235516.6\n166131.9\n16056\n417704.5\n\n\nWest Virginia\n101336.4\n89215.5\n10566\n201117.9\n\n\nWisconsin\n204930.0\n175762.4\n1046\n381738.4\n\n\nWyoming\n24825.9\n28396.5\n2590\n55812.4\n\n\n\n\n\n\n\nMerging on df1’s state column\n\nmerged_df = df2_agg.merge(df1, on='state', how='inner')\nmerged_df.head(20)\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\ntotal\npop_2016\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\n...\npe_whi\nadults_below_65\nhigh_school_grad_higher\nbsc_higher\nmedian_hu\nmedian_gross_rent\npoverty_pct\nmedian_hh_income\nper_cap_inc\nfirms\n\n\n\n\n0\nAlabama\n168435.3\n134159.3\n13090\n315684.6\n4863300.0\n51.6\n69.3\n26.8\n1.4\n...\n10.7\n61.3\n84.3\n23.5\n125500.0\n717.0\n17.1\n43623.0\n24091.0\n374153.0\n\n\n1\nAlaska\n41240.1\n40575.7\n4632\n86447.8\n741894.0\n47.7\n66.1\n3.8\n6.3\n...\n15.5\n64.4\n92.1\n28.0\n250000.0\n1146.0\n9.9\n72515.0\n33413.0\n68032.0\n\n\n2\nArizona\n183462.4\n119886.8\n15816\n319165.2\n6931071.0\n50.3\n83.3\n4.9\n3.4\n...\n11.9\n59.6\n86.0\n27.5\n167500.0\n913.0\n16.4\n50255.0\n25848.0\n499926.0\n\n\n3\nArkansas\n88268.4\n90332.0\n8800\n187400.4\n2988248.0\n50.9\n79.4\n15.7\n1.6\n...\n9.3\n60.1\n84.8\n21.1\n111400.0\n677.0\n17.2\n41371.0\n22798.0\n231959.0\n\n\n4\nCalifornia\n616390.5\n610005.0\n0\n1226395.5\n39250017.0\n50.3\n72.7\n6.5\n14.8\n...\n8.3\n63.2\n81.8\n31.4\n385500.0\n1255.0\n14.3\n61818.0\n30318.0\n3548449.0\n\n\n5\nColorado\n266752.2\n198028.6\n28586\n493366.8\n5540545.0\n49.7\n87.5\n4.5\n3.3\n...\n8.6\n63.8\n90.7\n38.1\n247800.0\n1002.0\n11.0\n60629.0\n32217.0\n547352.0\n\n\n6\nConnecticut\n134612.5\n48194.3\n0\n182806.8\n3576452.0\n51.2\n80.6\n11.8\n4.7\n...\n5.7\n62.8\n89.9\n37.6\n270500.0\n1075.0\n9.8\n70331.0\n38803.0\n326693.0\n\n\n7\nDelaware\n28131.4\n23920.6\n2068\n54120.0\n952065.0\n51.6\n70.1\n22.6\n4.0\n...\n6.6\n61.0\n88.4\n30.0\n231500.0\n1018.0\n11.7\n60509.0\n30554.0\n73418.0\n\n\n8\nFlorida\n728538.8\n347855.2\n50284\n1126678.0\n20612439.0\n51.1\n77.6\n16.8\n2.9\n...\n15.3\n60.0\n86.9\n27.3\n159000.0\n1002.0\n14.7\n47507.0\n26829.0\n2100187.0\n\n\n9\nGeorgia\n214280.0\n141104.7\n14956\n370340.7\n10310371.0\n51.3\n61.2\n32.0\n4.1\n...\n14.8\n62.5\n85.4\n28.8\n148100.0\n879.0\n16.0\n49620.0\n25737.0\n929864.0\n\n\n10\nHawaii\n0.0\n0.0\n0\n0.0\n1428557.0\n49.8\n25.8\n2.2\n37.7\n...\n4.2\n61.3\n91.0\n30.8\n515300.0\n1438.0\n9.3\n69515.0\n29822.0\n118454.0\n\n\n11\nIdaho\n51175.3\n61150.1\n4812\n117137.4\n1683140.0\n49.9\n93.3\n0.8\n1.5\n...\n11.8\n58.9\n89.5\n25.9\n162900.0\n743.0\n14.4\n47583.0\n23399.0\n146642.0\n\n\n12\nIllinois\n387652.1\n194980.5\n24266\n606898.6\n12801539.0\n50.9\n77.2\n14.7\n5.5\n...\n7.4\n62.5\n87.9\n32.3\n173800.0\n907.0\n13.0\n57574.0\n30494.0\n1135017.0\n\n\n13\nIndiana\n306000.2\n194939.8\n18164\n519104.0\n6633053.0\n50.7\n85.6\n9.7\n2.2\n...\n9.4\n61.3\n87.8\n24.1\n124200.0\n745.0\n14.1\n49255.0\n25346.0\n479059.0\n\n\n14\nIowa\n3069.0\n38779.4\n140\n41988.4\n3134693.0\n50.3\n91.4\n3.7\n2.5\n...\n5.0\n60.3\n91.5\n26.7\n129200.0\n697.0\n11.8\n53183.0\n27950.0\n259121.0\n\n\n15\nKansas\n89227.6\n81122.8\n8542\n178892.4\n2907289.0\n50.2\n86.6\n6.2\n3.0\n...\n10.1\n60.4\n90.2\n31.0\n132000.0\n757.0\n12.1\n52205.0\n27706.0\n239118.0\n\n\n16\nKentucky\n154793.1\n119933.0\n13886\n288612.1\n4436974.0\n50.7\n88.0\n8.3\n1.5\n...\n6.0\n61.6\n84.2\n22.3\n123200.0\n675.0\n18.5\n43740.0\n24063.0\n331546.0\n\n\n17\nLouisiana\n188746.8\n153034.2\n17218\n358999.0\n4681666.0\n51.1\n63.2\n32.6\n1.8\n...\n11.9\n61.8\n83.4\n22.5\n144100.0\n788.0\n20.2\n45047.0\n24981.0\n414291.0\n\n\n18\nMaine\n53729.5\n51458.0\n4916\n110103.5\n1331479.0\n51.0\n94.8\n1.5\n1.2\n...\n9.9\n61.5\n91.6\n29.0\n173800.0\n777.0\n12.5\n49331.0\n27655.0\n139570.0\n\n\n19\nMaryland\n57535.5\n79081.2\n674\n137290.7\n6016447.0\n51.6\n59.3\n30.7\n6.6\n...\n7.0\n63.0\n89.4\n37.9\n286900.0\n1230.0\n9.7\n74551.0\n36897.0\n531953.0\n\n\n\n\n20 rows × 21 columns\n\n\n\n\n# checking for nans throughout the dataframe\nmerged_df.isnull().sum().sum()\n\n0\n\n\n\nmerged_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50 entries, 0 to 49\nData columns (total 21 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   state                    50 non-null     object \n 1   handgun                  50 non-null     float64\n 2   long_gun                 50 non-null     float64\n 3   multiple                 50 non-null     int64  \n 4   total                    50 non-null     float64\n 5   pop_2016                 50 non-null     float64\n 6   female_pct               50 non-null     float64\n 7   white_pct                50 non-null     float64\n 8   black_pct                50 non-null     float64\n 9   asian_pct                50 non-null     float64\n 10  pe_hh                    50 non-null     float64\n 11  pe_whi                   50 non-null     float64\n 12  adults_below_65          50 non-null     float64\n 13  high_school_grad_higher  50 non-null     float64\n 14  bsc_higher               50 non-null     float64\n 15  median_hu                50 non-null     float64\n 16  median_gross_rent        50 non-null     float64\n 17  poverty_pct              50 non-null     float64\n 18  median_hh_income         50 non-null     float64\n 19  per_cap_inc              50 non-null     float64\n 20  firms                    50 non-null     float64\ndtypes: float64(19), int64(1), object(1)\nmemory usage: 8.3+ KB\n\n\n\nmerged_df.shape\n\n(50, 21)\n\n\n\n# saving for later\nmerged_df.to_csv('Data/merged_cleaned_data.csv', index=False)"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#exploratory-data-analysisunivariate",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#exploratory-data-analysisunivariate",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Exploratory Data Analysis(Univariate)",
    "text": "Exploratory Data Analysis(Univariate)\n(Merged dataframe)\n\nLet’s explore the first 5 columns, representing the gun sales data\n\nmerged_df = pd.read_csv('Data/merged_cleaned_data.csv')\nmerged_df.head()\n\n\n\n\n\n\n\n\nstate\nhandgun\nlong_gun\nmultiple\ntotal\npop_2016\nfemale_pct\nwhite_pct\nblack_pct\nasian_pct\n...\npe_whi\nadults_below_65\nhigh_school_grad_higher\nbsc_higher\nmedian_hu\nmedian_gross_rent\npoverty_pct\nmedian_hh_income\nper_cap_inc\nfirms\n\n\n\n\n0\nAlabama\n168435.3\n134159.3\n13090\n315684.6\n4863300.0\n51.6\n69.3\n26.8\n1.4\n...\n10.7\n61.3\n84.3\n23.5\n125500.0\n717.0\n17.1\n43623.0\n24091.0\n374153.0\n\n\n1\nAlaska\n41240.1\n40575.7\n4632\n86447.8\n741894.0\n47.7\n66.1\n3.8\n6.3\n...\n15.5\n64.4\n92.1\n28.0\n250000.0\n1146.0\n9.9\n72515.0\n33413.0\n68032.0\n\n\n2\nArizona\n183462.4\n119886.8\n15816\n319165.2\n6931071.0\n50.3\n83.3\n4.9\n3.4\n...\n11.9\n59.6\n86.0\n27.5\n167500.0\n913.0\n16.4\n50255.0\n25848.0\n499926.0\n\n\n3\nArkansas\n88268.4\n90332.0\n8800\n187400.4\n2988248.0\n50.9\n79.4\n15.7\n1.6\n...\n9.3\n60.1\n84.8\n21.1\n111400.0\n677.0\n17.2\n41371.0\n22798.0\n231959.0\n\n\n4\nCalifornia\n616390.5\n610005.0\n0\n1226395.5\n39250017.0\n50.3\n72.7\n6.5\n14.8\n...\n8.3\n63.2\n81.8\n31.4\n385500.0\n1255.0\n14.3\n61818.0\n30318.0\n3548449.0\n\n\n\n\n5 rows × 21 columns\n\n\n\n\nmerged_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50 entries, 0 to 49\nData columns (total 21 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   state                    50 non-null     object \n 1   handgun                  50 non-null     float64\n 2   long_gun                 50 non-null     float64\n 3   multiple                 50 non-null     int64  \n 4   total                    50 non-null     float64\n 5   pop_2016                 50 non-null     float64\n 6   female_pct               50 non-null     float64\n 7   white_pct                50 non-null     float64\n 8   black_pct                50 non-null     float64\n 9   asian_pct                50 non-null     float64\n 10  pe_hh                    50 non-null     float64\n 11  pe_whi                   50 non-null     float64\n 12  adults_below_65          50 non-null     float64\n 13  high_school_grad_higher  50 non-null     float64\n 14  bsc_higher               50 non-null     float64\n 15  median_hu                50 non-null     float64\n 16  median_gross_rent        50 non-null     float64\n 17  poverty_pct              50 non-null     float64\n 18  median_hh_income         50 non-null     float64\n 19  per_cap_inc              50 non-null     float64\n 20  firms                    50 non-null     float64\ndtypes: float64(19), int64(1), object(1)\nmemory usage: 8.3+ KB\n\n\n\n# plotting\nsns.set_style('whitegrid')\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,4))\n\n\nmerged_df.handgun.hist(bins=20, ax=ax1)\nmerged_df.long_gun.hist(bins=20, ax=ax2)\nmerged_df.multiple.hist(bins=20, ax=ax3)\nmerged_df.total.hist(bins=20, ax=ax4)\n\nax1.set(title='Distribution of Handgun Sales')\nax2.set(title='Distribution of Long-gun Sales')\nax3.set(title='Distribution of Multiple Sales')\nax4.set(title='Distribution of Total Sales')\n\nsns.despine(right=True)\nsns.despine(offset=0, trim=False)\n\n\n\n\n\n# summary statistics\nmerged_df[['handgun', 'long_gun', 'multiple', 'total']].describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nhandgun\n50.0\n177551.638\n185945.395696\n0.0\n51416.750\n132783.75\n211942.500\n728538.8\n\n\nlong_gun\n50.0\n131676.908\n118931.726762\n0.0\n49726.325\n99129.25\n175310.575\n610005.0\n\n\nmultiple\n50.0\n10294.720\n13047.318153\n0.0\n1813.500\n5490.00\n14688.500\n66204.0\n\n\ntotal\n50.0\n319523.266\n302625.048243\n0.0\n114836.400\n231037.30\n378888.975\n1297197.5\n\n\n\n\n\n\n\nAs expected with data regarding sales, the data is heavily right-skewed; with most of gun sales on the low end, evidenced by the mean being greater than the median.\n\n\nLet’s explore the next 5 columns, representing the population % by race\n\nsns.set_style('whitegrid')\nfig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(20,4))\n\n\nmerged_df.pop_2016.hist(bins=20, ax=ax1)\nmerged_df.female_pct.hist(bins=20, ax=ax2)\nmerged_df.black_pct.hist(bins=20, ax=ax3)\nmerged_df.white_pct.hist(bins=20, ax=ax4)\nmerged_df.asian_pct.hist(bins=20, ax=ax5)\n\nax1.set(title='Distribution of the Whole Population')\nax2.set(title='Distribution of the % Female Population')\nax3.set(title='Distribution of the % Black Population')\nax4.set(title='Distribution of the %  White Population')\nax5.set(title='Distribution of the % Asian Population')\n\nsns.despine(right=True)\nsns.despine(offset=0, trim=False)\n\n\n\n\n\n\nLet’s explore the next 5 columns, representing levels of education and persons per household and persons without health insurance\n\nsns.set_style('whitegrid')\nfig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(20,4))\n\n\nmerged_df. pe_hh.hist(bins=20, ax=ax1)\nmerged_df.pe_whi.hist(bins=20, ax=ax2)\nmerged_df.adults_below_65.hist(bins=20, ax=ax3)\nmerged_df.high_school_grad_higher.hist(bins=20, ax=ax4)\nmerged_df.bsc_higher.hist(bins=20, ax=ax5)\n\nax1.set(title='count of persons per household')\nax2.set(title='count of persons without health insurance')\nax3.set(title='adult population below 65')\nax4.set(title='High School Graduates or Higher')\nax5.set(title='Bachelor Degree Holders or Higher')\n\nsns.despine(right=True)\nsns.despine(offset=0, trim=False)\n\n\n\n\n\n\nLet’s explore the next 6 columns, representing economic data\n\nsns.set_style('whitegrid')\nfig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(1, 6, figsize=(20,4))\n\n\nmerged_df.median_hu.hist(bins=20, ax=ax1)\nmerged_df.median_gross_rent.hist(bins=20, ax=ax2)\nmerged_df.poverty_pct .hist(bins=20, ax=ax3)\nmerged_df.median_hh_income.hist(bins=20, ax=ax4)\nmerged_df.per_cap_inc.hist(bins=20, ax=ax5)\nmerged_df.firms.hist(bins=20, ax=ax6)\n\nax1.set(title='Housing Units Value')\nax2.set(title='Median Gross Rent')\nax3.set(title='% Persons in Poverty')\nax4.set(title='Median Household Income')\nax5.set(title='Per Capita Income')\nax6.set(title='All firms')\n\nsns.despine(right=True)\nsns.despine(offset=0, trim=False)"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#covariate-analysis",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#covariate-analysis",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Covariate Analysis",
    "text": "Covariate Analysis\nI want to create a new calculated column: gun_sales_per_capita, which would be derived from the division of total gun sales by state populations. This would provide a more appropriate baseline for the comparison of gun sales between states\n\nmerged_df['gun_sales_per_capita'] = merged_df['total'] / merged_df['pop_2016']\n\n\nmerged_df['gun_sales_per_capita'].describe()\n\ncount    50.000000\nmean      0.060184\nstd       0.028694\nmin       0.000000\n25%       0.038424\n50%       0.063000\n75%       0.081355\nmax       0.116523\nName: gun_sales_per_capita, dtype: float64\n\n\n\n# getting the correlations with gun per capita\n\nmerged_df.drop(columns=\"state\").corrwith(other=merged_df['gun_sales_per_capita'])\n\nhandgun                    0.029506\nlong_gun                  -0.056207\nmultiple                   0.122681\ntotal                      0.001330\npop_2016                  -0.335466\nfemale_pct                -0.384595\nwhite_pct                  0.382182\nblack_pct                 -0.223373\nasian_pct                 -0.492825\npe_hh                     -0.386078\npe_whi                     0.275326\nadults_below_65            0.103166\nhigh_school_grad_higher    0.147886\nbsc_higher                -0.356364\nmedian_hu                 -0.408704\nmedian_gross_rent         -0.462526\npoverty_pct                0.169764\nmedian_hh_income          -0.319988\nper_cap_inc               -0.260645\nfirms                     -0.337613\ngun_sales_per_capita       1.000000\ndtype: float64\n\n\n\n# visualizing the correlations\n\nfig, ax = plt.subplots(figsize=(15, 15))\ncorr = merged_df.drop(columns=\"state\").corr()\n\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, mask=mask, fmt=\".2f\", annot=True, ax=ax,\n            cmap=\"RdBu\", vmin=-1, vmax=1, square=True)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nPlotting the variables against gun_sales_per_capita to spot associations\n\nsns.pairplot(merged_df, x_vars=['pop_2016', 'female_pct', 'white_pct', 'black_pct', 'asian_pct'], y_vars=['gun_sales_per_capita'], kind='reg')\n\n\n\n\n\nsns.pairplot(merged_df, x_vars=['pe_hh', 'pe_whi','adults_below_65', 'high_school_grad_higher', 'bsc_higher'], y_vars=['gun_sales_per_capita'], kind='reg')\n\n\n\n\n\nsns.pairplot(merged_df, x_vars=['median_hu', 'median_gross_rent','poverty_pct', 'median_hh_income', 'per_cap_inc', 'firms'], y_vars=['gun_sales_per_capita'], kind='reg')\n\n\n\n\nWhich census data is most associated with high gun sales per capita\n\n\n\n\n\n\n\nVariable\nPositive/Negative Correlation\n\n\n\n\npersons per household (pe_hh)\nnegative\n\n\npersons without health insurance (pe_whi)\npositive\n\n\nbachelor degree holder or higher (bsc_higher)\nnegative\n\n\nmedian housing unit costs (median_hu)\nnegative\n\n\nmedian gross rent\nnegative"
  },
  {
    "objectID": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#conclusions",
    "href": "posts/2022-10-05_FBI-Gun-Data-EDA/index.html#conclusions",
    "title": "Project: Analyzing Trends in Gun Sales Data across the U.S.",
    "section": "Conclusions",
    "text": "Conclusions\nCaveats!\nThe findings of these analysis are summarised below. These are tentative and do indicate any causal relationship between variables.\n\n\nThe most populous states in the United States in 2016 are California, Texas, Florida, New York and Illinois.\nThe least populous states are Wyoming, Vermont, Alaska, North Dakota and South Dakota.\n\n\n\n\nStates with the highest median household income in 2016 are Maryland, Alaska, New Jersey, Connecticut and Hawaii.\nStates with the lowest median household income in 2016 are Mississippi, Arkankas, West Virginia, Alabama, and Kentucky.\n\n\n\n\nStates with the highest black population in 2016 are Mississippi, Louisiana, Georgia, Maryland and South Carolina.\nStates with the lowest black population in 2016 are Montana, Idaho, Vermont, Wyoming and Utah.\n\n\n\n2006\n\n\nLong guns were sold the most in 2006 with about 5,268,550 units sold.\n\n\n\n\nIn 2006, handgun sales varied between 200 thousand and 300 thousand units throughout the year. However, long-gun sales started strongly at around 400 thousand units in January, started sinking in March till a year-low of just below 300 thousand units before dramatically shooting up to about 800 thousand units at the end of the year\n\n\n\n\nThe top 5 states in terms of gun sales are Texas, Pennsylvania, California, Florida and Ohio.\n\n\n\n\nAmong these states, only in Florida were handgun sales greater than long-gun sales.\n\n\n\n\nAll these states experience a remarkable dip in sales in the middle of the year.\n\n\n\n\n2015\n\n9 years later in 2015, handguns became most bought gun types with a total of about 8 million units sold across all states in the U.S.\n\n\nThe same trend of a dip in sales around the middle of the year and a sharp rise in purchases towards the end of the year persists.\n\n\nTexas remains the top state in terms of gun purchases.\n\n\n\n2016\n\nA year later in 2016, total gun sales reached a little short of 16 million units. Handguns led the way at more than 8 million units.\n\n\nGun sales started relatively high compared to the previous years. I note the double dip in sales with second dip occuring around September. This double dip was mostly prominent in Florida. Another trend of note is that long-gun sales almost superceded handgun sales country-wise in November.\n\n\nTexas, California, Florida, Pennsylvania and Ohio remains the top states in terms of gun purchases\n\n\n\nGuam, Illinois, New Jersey, Connecticut and Massachusetts show the highest percentage increase in gun sales over the span of a decade (2006 - 2016). Hawaii, Mariana Islands, Iowa and Nebraska actually had gun sales reduced.\n\n\n\n\nGun sales per capita is weakly negatively correlated with persons per household; persons without health insurance (positive); bachelor degree holder or higher (negative); median housing unit costs (negative); median gross rent (negative)"
  },
  {
    "objectID": "posts/2023_03_16_pwc-call-centre-trends/index.html",
    "href": "posts/2023_03_16_pwc-call-centre-trends/index.html",
    "title": "Visualising call centre data trends",
    "section": "",
    "text": "Live Demo: here\n\n\n\n\n\n\n\n   \n\n\n\n\n\nI designed this dashboard as part of a PwC Switzerland Virtual Internship Program.\nMy design considerations included 5 KPIs I came up with from the dataset: Number of calls answered, Number of calls abandoned, Average speed taken to answer calls, Average cal duration and Average satisfaction rating. After defining the KPIs, I translated them to Tableau code and implemented them.\nI designed 3 filter element that allowed easy filtering by date, call topic and call agent.\nOne KPI of note is the Agent performance. The agent performance quadrant allows performance to be measured across two dimensions, average calls answered by the agent and the average call duration of their calls.\nThe visual can be viewed and interacted with at link"
  },
  {
    "objectID": "posts/2023_03_16_pwc-call-centre-trends/index.html#about-the-project",
    "href": "posts/2023_03_16_pwc-call-centre-trends/index.html#about-the-project",
    "title": "Visualising call centre data trends",
    "section": "",
    "text": "Live Demo: here\n\n\n\n\n\n\n\n   \n\n\n\n\n\nI designed this dashboard as part of a PwC Switzerland Virtual Internship Program.\nMy design considerations included 5 KPIs I came up with from the dataset: Number of calls answered, Number of calls abandoned, Average speed taken to answer calls, Average cal duration and Average satisfaction rating. After defining the KPIs, I translated them to Tableau code and implemented them.\nI designed 3 filter element that allowed easy filtering by date, call topic and call agent.\nOne KPI of note is the Agent performance. The agent performance quadrant allows performance to be measured across two dimensions, average calls answered by the agent and the average call duration of their calls.\nThe visual can be viewed and interacted with at link"
  },
  {
    "objectID": "posts/2023_03_16_pwc-call-centre-trends/index.html#contact",
    "href": "posts/2023_03_16_pwc-call-centre-trends/index.html#contact",
    "title": "Visualising call centre data trends",
    "section": "Contact",
    "text": "Contact\nEdun Joshua Olubori - connect on linkedin - joshuaolubori@gmail.com"
  },
  {
    "objectID": "posts/2024-01-20_Aqua-Vista/index.html",
    "href": "posts/2024-01-20_Aqua-Vista/index.html",
    "title": "Aqua Vista College Database",
    "section": "",
    "text": "Based on the case study (interview scripts and required reports) at the end of this brief, provide the following:\n\nConceptual ERD with justifications of design choices\nLogical Design utilising skeleton tables\n\nPrimary Keys are underlined, and Foreign Keys have an asterisk next to them. e.g. table_name(table_id, table_attribute, other_table_id*)\n\nIndication of report creation\n\nIndicate which tables from your design would be used, and what are the common values.\n\nOutline of data accuracy strategy\n\nWhat concrete measures would you take to ensure data can be analysed to aid prediction?\n\n\nTo achieve a distinction:\n\nThe Conceptual ERD fully covers the case study’s entities and relationships. There are justifications provided for the design choices based on the case study’s statements. The justifications are thought through, concrete and well-argued.\nThe Logical Design is in the required format, and it is directly derived from and related to the produced Conceptual ERD. Where design choices needed to be taken, they are clearly indicated and justified. Problematic relationships, such as many-to-many relationships have been resolved correctly.\nA clear indication which entities/tables had to be included to produce the required reports, and which matching fields were used.\nA mature and concrete strategy to ensure data accuracy is provided which is relevant for the given case study."
  },
  {
    "objectID": "posts/2024-01-20_Aqua-Vista/index.html#task-introduction",
    "href": "posts/2024-01-20_Aqua-Vista/index.html#task-introduction",
    "title": "Aqua Vista College Database",
    "section": "Task Introduction",
    "text": "Task Introduction\nBased on the case study (interview scripts and required reports) at the end of this brief, provide the following:\n\nConceptual ERD with justifications of design choices\nLogical Design utilising skeleton tables\n\nPrimary Keys are underlined, and Foreign Keys have an asterisk next to them. e.g. table_name(table_id, table_attribute, other_table_id*)\n\nIndication of report creation\n\nIndicate which tables from your design would be used, and what are the common values.\n\nOutline of data accuracy strategy\n\nWhat concrete measures would you take to ensure data can be analysed to aid prediction?\n\n\nTo achieve a distinction:\n\nThe Conceptual ERD fully covers the case study’s entities and relationships. There are justifications provided for the design choices based on the case study’s statements. The justifications are thought through, concrete and well-argued.\nThe Logical Design is in the required format, and it is directly derived from and related to the produced Conceptual ERD. Where design choices needed to be taken, they are clearly indicated and justified. Problematic relationships, such as many-to-many relationships have been resolved correctly.\nA clear indication which entities/tables had to be included to produce the required reports, and which matching fields were used.\nA mature and concrete strategy to ensure data accuracy is provided which is relevant for the given case study."
  },
  {
    "objectID": "posts/2024-01-20_Aqua-Vista/index.html#conceptual-erd",
    "href": "posts/2024-01-20_Aqua-Vista/index.html#conceptual-erd",
    "title": "Aqua Vista College Database",
    "section": "Conceptual ERD",
    "text": "Conceptual ERD\n\n\n\nAqua Vista College Database - Conceptual ERD. (Click to view full image online)"
  },
  {
    "objectID": "posts/2024-01-20_Aqua-Vista/index.html#logical-design",
    "href": "posts/2024-01-20_Aqua-Vista/index.html#logical-design",
    "title": "Aqua Vista College Database",
    "section": "Logical Design",
    "text": "Logical Design\n\n\n\nAqua Vista College - Logical Design. (Click to view full image online)"
  },
  {
    "objectID": "posts/2024-01-20_Aqua-Vista/index.html#design-justifications",
    "href": "posts/2024-01-20_Aqua-Vista/index.html#design-justifications",
    "title": "Aqua Vista College Database",
    "section": "Design Justifications",
    "text": "Design Justifications\n\nOffice Allocation:\nImplementing a dedicated “Office” entity and bridge table for academic staff assignments unlocks the following advantages: flexibility to track office occupancy dynamically with start and end dates, data efficiency by avoiding redundancy through centralized office details and shared/single occupancy flags, and clarity by decoupling physical locations from reporting structures and staff attributes, leading to a more interpretable model.\n\n\nLine Management\nRepresenting the Academic Staff reporting structure within the university is elegantly achieved through a self-referencing AcademicStaff entity. This leverages a natural one-to-one relationship referencing the line_manager_id attribute, allowing each staff member to have a designated manager while managers oversee multiple staff as needed. This avoids the redundancy of a dedicated line manager table, maintaining streamlined queries and ensuring consistent data. The self-referencing approach eliminates the need for additional joins, keeping the model clear and easily traversable.\n\n\nResolving the many-to-many relationship between AcademicStaff and Unit\nThe Head of Department interview revealed that: some academics teach several units, some none, and units can have multiple instructors. A dedicated bridge table elegantly untangles this many-to-many relationship.\nBeyond capturing the current teaching relationships, this design offer these benefits:\n\nDynamic tracking: Start and end dates record teaching history, combating the “slowly changing dimensions” issue and aiding analysis of past assignments.\nFuture-proof scalability: Easily extend the model to track different appointment types or add new analytical data.\nEfficient queries: The bridge table centralizes teaching data, making queries related to instructors or units much faster than sifting through individual records.\n\nThis robust design allows valuable insights for analysis, reporting, and career progression tracking, while laying a flexible foundation for future changes.\n\n\nModelling Academic Staff’s Research Interests\nFollowing the Head of Department’s emphasis - “all academic staff members are required to record their research interests” - research interests are modeled as a mandatory one-to-many relationship with academic staff. Every staff member must actively record their research interests, and this relationship is reflected by the Crow’s Foot notation in the relationship between the entities.\n\n\nSubjects and Units\nA many-to-many relationship exists between these two entities, as outlined in the requirements derived from the interview with the Head of Department. This relationship is effectively resolved by a bridge table, aligning with the fact that a Unit may encompass multiple Subjects, and conversely, a Subject can be associated with multiple Units.\nThe Industry Liaison Lead validates that subjects are open to proposals from companies. To address this requirement, a proposed_by_company foreign key field is incorporated, establishing a connection with the Industry Partner entity. This ensures that the relationship between subjects and proposing companies is appropriately represented in the database structure.\n\n\nCourse Enrollment\nThe interview underscores that students dedicate themselves to a single course at a time, establishing a one-to-one relationship between a student and their presently enrolled course. Although students eventually undertake multiple courses throughout their program, this is done sequentially, indicating that they are not concurrently enrolled in several courses.\nThis design facilitates the historical tracking of student enrollment. While the primary focus is on the current course, there arises a necessity to trace past courses undertaken by a student. Introducing a bridge table offers an efficient means of storing and retrieving historical enrollment data without cluttering the Student or Course entities.\nThe bridge table offers flexibility to manage potential scenarios such as course changes or leaves of absence, ensuring a precise record of all courses a student has enrolled in during their program. The course_completion_status attribute enables this record-keeping through a check constraint represented as an ENUM datatype with values like “Completed,” “Ongoing,” and “Withdrawn.”\nAdditional attributes may be incorporated into the bridge table for each enrollment record, including start and end dates, course completion status, grades, or even reasons for withdrawal if applicable. This supplementary information proves valuable for subsequent analysis and reporting purposes.\n\n\nUnit, Attendance and Session\nThis approach incorporates a bridge table between students and sessions to record attendance. Reflecting the Head of Department’s statement that “A session can be a lecture, seminar, or lab associated with a unit,” an ENUM constraint on the session_type attribute ensures adherence to these three specified values.\nMoreover, as per the emphasis on sessions potentially being part of extra-curricular events with external speakers, the design introduces a distinct entity, “extra-curricular_event,” to capture this information. The relationship is non-mandatory, and the Crow’s Foot notation appropriately signifies this flexibility.\nGiven that units encompass sessions and sessions relate to subjects, the design establishes a connection between the session_id attribute and the subject_id attribute through the unit_subject bridge table. This alignment is logical, considering each unit-subject combination functions as an independent entity, allowing for variations in their respective sessions.\nTo address the requirement that a subject may not yet be part of any unit or session, the presence of an approval_status ENUM attribute effectively manages this condition.\n\n\nIndustrial Partnership\nThe interview with the Industrial Liaison Lead sheds light on the interaction between the school and industry partners. Industrial partners are embodied in an Industry Partner entity, contributing through the delivery of Talks on specific subjects. This is depicted as a one-to-many optional relationship with the Talks entity, along with a one-to-one relationship between Talks and Subject.\nIn the discussion, the official affirms that companies can propose multiple subjects, and conversely, a subject can be proposed by multiple companies, establishing a resolved many-to-many relationship through the industry_partners_subject bridge table, linking Industry Partner to Subject. This bridge table serves to capture additional proposal details, including the proposal date and description.\nRecognizing that not all subjects are covered by industry partners, an optional relationship is indicated between subject and industry_partner_subject using Crow’s Foot notation. The subject_type has an ENUM type, offering values “traditional” and “proposed.”\nThe subject_type attribute distinguishes a subject’s origin, marking it as either proposed by an industry partner or part of Aqua Vista’s traditional curriculum.\n\n\nTalks and Activity\nTalks may encompass various activities, and it is affirmed that recording these activities is crucial. As a solution, an activity entity has been established, featuring a talk_id foreign key referencing the Talks entity. Each talk or activity is associated with a designated contact person, fulfilling this requirement through the inclusion of a contact_person_phone attribute that stores the contact information of the designated person. I could not confirm if the contact person is a staff of the school, otherwise, I would relate this attribute to the academic_staff entity.\nRecognizing that not all partners engage in delivering talks, the model accommodates this by incorporating an optional one-to-many relationship between industry partners and talks. This ensures flexibility in representing instances where some industry partners may not be involved in delivering talks."
  },
  {
    "objectID": "posts/2024-01-20_Aqua-Vista/index.html#indication-of-report-creation",
    "href": "posts/2024-01-20_Aqua-Vista/index.html#indication-of-report-creation",
    "title": "Aqua Vista College Database",
    "section": "Indication of report creation",
    "text": "Indication of report creation\n\nIdentifying the sessions most students attended out of all subjects delivered by an external company\nTables:\n\nAttendance: Tracks student participation, linking student_id to session_id.\nSession: Holds information about individual sessions, including the associated unit_subject_id.\nUnit_Subject: Bridges sessions and subjects, connecting session_id to unit_subject_id.\nSubject: Captures subject details, including a unique subject_id and a “subject_type” flag indicating external involvement.\n\nCommon Values:\n\nSessionID: Unique identifier for each session, linking Attendance and Session data.\nUnitSubjectID: Unique identifier for each Unit-Subject, acting as the bridge between Session and Subject tables.\nSubjectID: Unique identifier for each Subject, enabling filtering for those externally proposed in the Subject table.\n\nMethodology:\n\nFilter Sessions: Using the Subject table’s “subject_type” flag as a filter, I select only sessions linked to subjects tagged as “External”\nJoin and Aggregate: By joining Attendance, Session, Unit_Subject_Junction, and Subject tables on their respective common values, we can count the number of students attending each session.\nIdentify Maximum Attendance: Finally, we sort the aggregated data by student attendance in descending order and locate the session with the highest attendance count, pinpointing the most popular session delivered by an external company.\n\nThe following SQL query will deliver the report:\n\nSELECT s.SessionID, COUNT(a.StudentID) AS AttendanceCount\nFROM Attendance a JOIN Session s ON a.SessionID = s.SessionID\nJOIN Unit_Subject_Junction usj ON s.UnitSubjectID = usj.UnitSubjectID\nJOIN Subject sub ON usj.SubjectID = sub.SubjectID\nWHERE sub.subject_type = \"External\"GROUP BY s.SessionID ORDER BY AttendanceCount DESC LIMIT 1;\n\n\n\nIdentifying the unit with the most sessions delivered by a company on any subject\nTo reveal the unit hosting the most sessions delivered by companies across all subjects, the following entities are identified.\nTables:\n\nSession: Captures session details, including the associated unit_subject_id.\nUnit_Subject: Bridges sessions to units and subjects, connecting session_id to unit_subject_id and subject_id.\nUnit: Holds unit information, including the unique unit_id.\nSubject: Stores subject details, including the subject_type flag indicating company involvement.\n\nCommon Values:\n\nUnitSubjectID: Unique identifier for each unit-subject, linking Session and Unit_Subject tables.\nUnitID: Unique identifier for each Unit, connecting Unit_Subject and Unit tables.\nSubjectID: Unique identifier for each subject, enabling filtering for those marked as externally proposed in the Subject table.\n\nMethodology:\n\nFilter Company-Delivered Sessions: Employ the WHERE clause to select only sessions linked to subjects where the subject_type flag is “Exernal”. This ensures we focus on sessions originating from external partnerships.\nJoin and Aggregate: By joining Session, Unit_Subject, and Unit tables on their respective common values, we associate sessions with their corresponding units and count the total number of sessions per unit.\nIdentify Top Unit: We order the aggregated data by session count in descending order. The unit at the top boasts the highest number of sessions delivered by a company, making it the champion of external collaborations.\n\nThe following SQL query will deliver the report:\n\nSELECT u.unit_id, u.unit_name, COUNT(s.session_id) AS session_count\nFROM Session s JOIN Unit_Subject_Junction usj ON s.unit_subject_id = usj.unit_subject_id\nJOIN Unit u ON usj.unit_id = u.unit_id\nJOIN Subject sub ON usj.subject_id = sub.subject_id\nWHERE sub.subject_type = \"External\"\nGROUP BY u.unit_id, u.unit_name ORDER BY session_count DESC LIMIT 1;"
  },
  {
    "objectID": "posts/2024-01-20_Aqua-Vista/index.html#identifying-the-academic-staff-with-the-most-research-interests",
    "href": "posts/2024-01-20_Aqua-Vista/index.html#identifying-the-academic-staff-with-the-most-research-interests",
    "title": "Aqua Vista College Database",
    "section": "Identifying The Academic staff with the most Research Interests",
    "text": "Identifying The Academic staff with the most Research Interests\nTo uncover the academic staff member with the most diverse research interests within your Aqua Vista College, the database design offers the insights through two tables:\nTables:\n\nacademic_staff: This table holds core information about staff members, including their unique staff_id and first_name.\nresearch_interests: This table captures details about staff members’ research pursuits, linking them to individual staff through the staff_id and providing descriptive research_title information.\n\nThe Bridge: Shared Values:\nstaff_id. links these 2 tables together. This unique identifier, present in both tables, allows to seamlessly connect each staff member with their respective research interests.\nMethodology:\n\nConnecting the Dots: We’ll employ a JOIN operation to merge the academic_staff and research_interests tables on the staff_id column. This creates a unified view where each staff member is accompanied by their research areas.\nQuantifying Passion: For each staff member, we’ll utilize the COUNT() function to determine the total number of research interests listed in the research_interests table associated with their staff_id. This effectively quantifies their research diversity.\nCrowning the Champion: Finally, we’ll order the combined data by the count of research interests in descending order. The staff member at the top, boasting the highest count, emerges as the champion of diversified research pursuits within your institution.\n\nThe following SQL query will deliver the report:\n\nSELECT as.staff_id, as.first_name, COUNT(ri.research_id) AS research_interest_count\nFROM academic_staff as JOIN research_interests ri ON as.staff_id = ri.staff_id\nGROUP BY as.staff_id, as.first_name\nORDER BY research_interest_count DESC LIMIT 1;"
  },
  {
    "objectID": "posts/2024-01-20_Aqua-Vista/index.html#data-accuracy-strategy",
    "href": "posts/2024-01-20_Aqua-Vista/index.html#data-accuracy-strategy",
    "title": "Aqua Vista College Database",
    "section": "Data Accuracy Strategy",
    "text": "Data Accuracy Strategy\n\nData Accuracy Strategy for Institutional Research and Teaching Analysis:\nObjective: Establish the database reliably for analyzing research interests, teaching involvement, and other key academic elements.\nConcrete Measures:\nEnum Constraints for Research Interests: Apply ENUM constraints, such as for the “category” field in the academic_staff table, restricting values to a predefined set (“lecturers” or “demonstrators”). Integrate drop-down menus in the front-end to facilitate user selection, minimizing ambiguity and ensuring consistency.\nTeaching Units: Implement lookups or autocomplete features for existing units in the front-end, preventing typographical errors and eliminating duplicate entries during data entry.\nEnforce Temporal Validity in Office Allocation: Enact data integrity rules to prohibit overlapping occupancy periods or exceeding office capacity, enhancing accuracy in office allocation records.\nEnsure Acyclic Relationships: Incorporate checks to validate acyclic relationships within the line-management hierarchy, preventing the formation of circular dependencies and ensuring a transparent organizational structure.\nCascade Deletion: Employ cascading actions for the line_manager_id foreign key. By using “ON DELETE CASCADE” and “ON UPDATE CASCADE” in SQL, ensure that changes to staff members cascade appropriately, maintaining referential integrity.\nPeriodic Audits: Conduct routine data consistency checks to identify and rectify duplicates or incomplete entries, enhancing overall data accuracy.\nRandom Sampling: Implement manual verification through random sampling of research interests and teaching assignments against employee resumes or course schedules, providing an additional layer of accuracy validation.\nPeer Review: Foster departmental reviews of research areas and unit affiliations to encourage collective scrutiny and improvement of data accuracy.\n\n\nData Accuracy Strategy for Enhancing Student Attendance and Course Insights\nObjective: Establish a foundation of reliable data to facilitate accurate attendance tracking, course optimization, and predictive student success analysis.\nConcrete Measures:\nCourse and Unit Information: Use ENUM constraint to ensure that data entered into each field adheres to predefined standards. For instance, enforce constraints on fields like attendance_status to accept only valid values (e.g., “present,” “absent”).\nImplement validation rules to prevent duplicate entries and maintain course levels within defined ranges.\nAttendance Data: Use foreign key constraints in the physical model to restrict attendance entry to valid student IDs, session IDs, and dates, preventing errors and inconsistencies.\nSession Details: Ensure temporal validity (i.e. using the start_date and end_date attributes) by designing checks in the physical model to avoid conflicts and maintain data integrity."
  },
  {
    "objectID": "posts/2024-01-20_Aqua-Vista/index.html#sql-create-statements",
    "href": "posts/2024-01-20_Aqua-Vista/index.html#sql-create-statements",
    "title": "Aqua Vista College Database",
    "section": "SQL CREATE statements",
    "text": "SQL CREATE statements\n\n\nCREATE TABLE course (\n  course_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  course_title VARCHAR(255),\n  course_name VARCHAR(255),\n  course_level VARCHAR(10)\n);\nCREATE TABLE student (\n  student_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  first_name VARCHAR(255),\n  last_name VARCHAR(255),\n  date_of_birth DATE,\n  email VARCHAR(255),\n  phone_number INTEGER\n);\nCREATE TABLE course_enrolment (\n  student_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  course_id INTEGER,\n  course_completion_status ENUM(Completed, course_enrolment),\n  grade ENUM(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"),\n  phone_number INTEGER,\n  start_date DATE,\n  end_date DATE,\n  CONSTRAINT FK_course_enrolment.course_id FOREIGN KEY (course_id) \n  REFERENCES course(course_id) ON UPDATE CASCADE ON DELETE RESTRICT, -- Update updated course_ids but do not allow deletions in the parent table,\n  CONSTRAINT FK_course_enrolment.student_id FOREIGN KEY (student_id) \n  REFERENCES student(student_id) ON UPDATE CASCADE ON DELETE RESTRICT -- Update updated student_ids but do not allow deletions in the parent table\n);\nCREATE TABLE industry_partners (\n  company_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  company_name VARCHAR(50),\n);\nCREATE TABLE talks (\n  talk_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  subject_id INTEGER,\n  date DATE,\n  contact_person_phone INTEGER,\n  company_id INTEGER,\n  CONSTRAINT FK_talks.company_id FOREIGN KEY (company_id) \n  REFERENCES industry_partners(company_id) ON UPDATE CASCADE ON DELETE CASCADE, -- Cascade updates and deletions to the parent table,\n  CONSTRAINT FK_talks.subject_id FOREIGN KEY (subject_id) \n  REFERENCES industry_partners(subject_id) ON UPDATE CASCADE ON DELETE CASCADE -- Cascade updates and deletions to the parent table\n);\nCREATE TABLE activity (\n  activity_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  talk_id INTEGER,\n  activity_name VARCHAR(255),\n  CONSTRAINT FK_activity.talk_id FOREIGN KEY (talk_id) REFERENCES talks(talk_id)\n);\nCREATE TABLE research_interest (\n  research_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  staff_id INTEGER,\n  research_area VARCHAR(50),\n  research_title VARCHAR(255),\n  CONSTRAINT FK_research_interest.staff_id FOREIGN KEY (staff_id) REFERENCES academic_staff(staff_id)\n);\nCREATE TABLE subject (\n  subject_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  subject_name VARCHAR(50),\n  description TEXT,\n  approval_status ENUM(\"Approved\", \"Pending\", \" Rejected\") subject_type ENUM(Traditional, External) is_extra - curricular BOOLEAN\n);\nCREATE TABLE unit (\n  unit_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  unit_title VARCHAR(50)\n);\nCREATE TABLE unit_subject (\n  unit_subject_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  unit_id INTEGER,\n  subject_id INTEGER,\n  CONSTRAINT FK_unit_subject.unit_id FOREIGN KEY (unit_id) REFERENCES unit(unit_id),\n  CONSTRAINT FK_unit_subject.subject_id FOREIGN KEY (subject_id) REFERENCES subject(subject_id)\n);\nCREATE TABLE session (\n  session_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  unit_subject_id INTEGER,\n  date DATE,\n  session_type ENUM(\"Lecture\", \"Seminar\", \"Lab\"),\n  CONSTRAINT FK_session.unit_subject_id FOREIGN KEY (unit_subject_id) REFERENCES unit_subject(unit_subject_id)\n);\nCREATE TABLE office (\n  office_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  office_name VARCHAR(255),\n  location VARCHAR(255),\n  capacity NUMERIC\n);\nCREATE TABLE academic_staff (\n  staff_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  first_name VARCHAR(255),\n  last_name VARCHAR(255),\n  category ENUM(\"lecturers\", \"demonstrators\"),\n  line_manager_id INTEGER,\n  CONSTRAINT FK_academic_staff.staff_id FOREIGN KEY (line_manager_id) REFERENCES academic_staff(staff_id)\n);\nCREATE TABLE attendance (\n  attendance_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  student_id INTEGER,\n  session_id INTEGER,\n  date DATE,\n  attendance_status ENUM(\"Present\", \"Absent\", \"Excused\"),\n  CONSTRAINT FK_attendance.session_id FOREIGN KEY (session_id) REFERENCES session(session_id),\n  CONSTRAINT FK_attendance.student_id FOREIGN KEY (student_id) REFERENCES student(student_id)\n);\nCREATE TABLE office_allocation (\n  staff_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  office_id INTEGER,\n  start_date DATE,\n  end_date DATE,\n  CONSTRAINT FK_office_allocation.staff_id FOREIGN KEY (staff_id) REFERENCES academic_staff(staff_id),\n  CONSTRAINT FK_office_allocation.office_id FOREIGN KEY (office_id) REFERENCES office(office_id)\n);\nCREATE TABLE industry_partners_subject (\n  company_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n  subject_id INTEGER,\n  description TEXT,\n  date_of_proposal DATE,\n  CONSTRAINT FK_industry_partners_subject.subject_id FOREIGN KEY (subject_id) REFERENCES subject(subject_id),\n  CONSTRAINT FK_industry_partners_subject.company_id FOREIGN KEY (company_id) REFERENCES industry_partners(company_id)\n);\nCREATE TABLE staff_unit_assignments (\n  staff_id INTEGER,\n  unit_id INTEGER,\n  role ENUM(\"unit coordinator\", \"lecturer\",\"assistant\"),\n  start_date DATE,\n  end_date DATE,\n  CONSTRAINT FK_staff_unit_assignments.unit_id FOREIGN KEY (unit_id) REFERENCES unit(unit_id),\n  CONSTRAINT FK_staff_unit_assignments.staff_id FOREIGN KEY (staff_id) REFERENCES academic_staff(staff_id)\n);\nCREATE TABLE extra-curricular_events (\n  event_id INTEGER PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n,\n  session_id INTEGER,\n  date DATE,\n  external_speaker VARCHAR(255),\n  CONSTRAINT FK_extra-curricular_events.session_id FOREIGN KEY (session_id) REFERENCES session(session_id)\n);"
  },
  {
    "objectID": "posts/2024-01-20_Aqua-Vista/index.html#conclusion",
    "href": "posts/2024-01-20_Aqua-Vista/index.html#conclusion",
    "title": "Aqua Vista College Database",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, this project is a sophisticated and comprehensive approach to database design, incorporating various concepts for accuracy, flexibility, and efficiency. The use of ENUM constraints, foreign key relationships, and validation rules ensures consistent data across tables. Bridge tables resolve many-to-many relationships, enabling dynamic tracking, historical analysis, and scalability. Handling potential acyclic relationships, cascading actions, and periodic audits enhances data model integrity, creating a strong foundation for analytical insights. The project meets immediate requirements and establishes a groundwork for future adaptability, reflecting a nuanced understanding of Aqua Vista College’s operational intricacies. This solution ensures the database meets current needs and remains resilient to changing educational and analytical demands.\nBack to top"
  },
  {
    "objectID": "posts/2022-09-18_tweet-wrangling/index.html",
    "href": "posts/2022-09-18_tweet-wrangling/index.html",
    "title": "Tweet Data Analysis",
    "section": "",
    "text": "As part of the Udacity Data Analysis with Python Nanodegree, I was the first to complete this project in my cohort. Feel free to review and offer your thoughts.\n\n\n\nThis code is hosted on Kaggle. View this and more on my Kaggle page"
  },
  {
    "objectID": "posts/2022-09-18_tweet-wrangling/index.html#introduction",
    "href": "posts/2022-09-18_tweet-wrangling/index.html#introduction",
    "title": "Tweet Data Analysis",
    "section": "",
    "text": "As part of the Udacity Data Analysis with Python Nanodegree, I was the first to complete this project in my cohort. Feel free to review and offer your thoughts.\n\n\n\nThis code is hosted on Kaggle. View this and more on my Kaggle page"
  },
  {
    "objectID": "posts/2022-09-23_go-ford-bike-share-analysis/index.html",
    "href": "posts/2022-09-23_go-ford-bike-share-analysis/index.html",
    "title": "Deriving insights from bike traffic data",
    "section": "",
    "text": "I pushed python data wrangling and visualisation to the limit in this project. I hope you rate it highly.\n\nThis code is hosted on Kaggle. View this and more on my Kaggle page"
  },
  {
    "objectID": "posts/2022-09-23_go-ford-bike-share-analysis/index.html#introduction",
    "href": "posts/2022-09-23_go-ford-bike-share-analysis/index.html#introduction",
    "title": "Deriving insights from bike traffic data",
    "section": "",
    "text": "I pushed python data wrangling and visualisation to the limit in this project. I hope you rate it highly.\n\nThis code is hosted on Kaggle. View this and more on my Kaggle page"
  },
  {
    "objectID": "posts/2024-02-01_Onyx-January/index.html",
    "href": "posts/2024-02-01_Onyx-January/index.html",
    "title": "Pushing the Boundaries of Excel",
    "section": "",
    "text": "Recently, I took on the challenge presented by Onyx Data’s January visualization challenge. Despite the availability of popular BI tools like Tableau and Power BI, I decided to push the limits of Excel and create an aesthetic dashboard.\nAfter missing out on the December edition due to laptop issues, I was determined to participate in the January challenge. The dataset provided by Onyx Data included various fields such as course ID, title, URL, pricing, subscribers, reviews, lectures, level, duration, publication timestamp, and subject category.\nTo begin, I organized my data and created pivot tables on a separate sheet to keep them organized."
  },
  {
    "objectID": "posts/2024-02-01_Onyx-January/index.html#introduction",
    "href": "posts/2024-02-01_Onyx-January/index.html#introduction",
    "title": "Pushing the Boundaries of Excel",
    "section": "",
    "text": "Recently, I took on the challenge presented by Onyx Data’s January visualization challenge. Despite the availability of popular BI tools like Tableau and Power BI, I decided to push the limits of Excel and create an aesthetic dashboard.\nAfter missing out on the December edition due to laptop issues, I was determined to participate in the January challenge. The dataset provided by Onyx Data included various fields such as course ID, title, URL, pricing, subscribers, reviews, lectures, level, duration, publication timestamp, and subject category.\nTo begin, I organized my data and created pivot tables on a separate sheet to keep them organized."
  },
  {
    "objectID": "posts/2024-02-01_Onyx-January/index.html#data-summary",
    "href": "posts/2024-02-01_Onyx-January/index.html#data-summary",
    "title": "Pushing the Boundaries of Excel",
    "section": "Data summary",
    "text": "Data summary\ncourse_id: A unique identifier for each course. course_title: The title of the course. url: URL of the course on Udemy. is_paid: Indicates whether the course is paid or free. price: The price of the course (if it’s a paid course). num_subscribers: The number of subscribers for the course. num_reviews: The number of reviews the course has received. num_lectures: The number of lectures in the course. level: The level of the course (e.g., All Levels, Intermediate Level). content_duration: The duration of the course content in hours. published_timestamp: The date and time when the course was published. subject: The subject category of the course."
  },
  {
    "objectID": "posts/2024-02-01_Onyx-January/index.html#design-elemn",
    "href": "posts/2024-02-01_Onyx-January/index.html#design-elemn",
    "title": "Pushing the Boundaries of Excel",
    "section": "Design Elemn",
    "text": "Design Elemn\n\nPivot tables\nEvery chart and metric in the dashboard was derived from pivot tables, ensuring a structured approach to data visualization. \n\n\nCalculations sheet\nBecause for some reason, I could not pull single figures from the pivot tables directly into the dashboard, I had to link them from an intermediate calculations sheet. \n\n\nGradients\nInspired by Other Level’s on YouTube, I used gradients to add a 3D feel to the dashboard, enhancing its visual appeal. \n\n\nCustom graphics\nUsing Excel’s built-in shapes and applying creative styling, I added custom graphics to enhance the dashboard’s aesthetics. \n\n\nSlicers\nThere are two slicers in this dashboard. One general slicer that filter by month running vertically along the right side of the dashboard, and another bright orange slicer that slices by year and only affects the “Revenue and suscriber growth trends” and “Does a course’s price justifies its revenue?” charts. To style them, I had to create their own Slicer Styles \nYou can view the raw Excel file on Excel Online here\n\n\nConclusion\nCreating this dashboard was both challenging and rewarding. Despite the initial limitations of Excel, I was able to produce a visually appealing and functional dashboard that met the requirements of the challenge."
  },
  {
    "objectID": "posts/2024-02-01_Onyx-January/index.html#contact-me",
    "href": "posts/2024-02-01_Onyx-January/index.html#contact-me",
    "title": "Pushing the Boundaries of Excel",
    "section": "Contact me",
    "text": "Contact me\nFor more information or to connect, you can find me on LinkedIn here or email me at joshuaolubori@gmail.com"
  },
  {
    "objectID": "posts/2024-02-01_Onyx-January/index.html#design-elements",
    "href": "posts/2024-02-01_Onyx-January/index.html#design-elements",
    "title": "Pushing the Boundaries of Excel",
    "section": "Design Elements",
    "text": "Design Elements\n\nPivot tables\nEvery chart and metric in the dashboard was derived from pivot tables, ensuring a structured approach to data visualization. \n\n\nCalculations sheet\nBecause for some reason, I could not pull single figures from the pivot tables directly into the dashboard, I had to link them from an intermediate calculations sheet. \n\n\nGradients\nInspired by Other Level’s on YouTube, I used gradients to add a 3D feel to the dashboard, enhancing its visual appeal. \n\n\nCustom graphics\nUsing Excel’s built-in shapes and applying creative styling, I added custom graphics to enhance the dashboard’s aesthetics. \n\n\nSlicers\nThere are two slicers in this dashboard. One general slicer that filter by month running vertically along the right side of the dashboard, and another bright orange slicer that slices by year and only affects the “Revenue and suscriber growth trends” and “Does a course’s price justifies its revenue?” charts. To style them, I had to create their own Slicer Styles \nYou can view the raw Excel file on Excel Online here\n\n\nConclusion\nCreating this dashboard was both challenging and rewarding. Despite the initial limitations of Excel, I was able to produce a visually appealing and functional dashboard that met the requirements of the challenge."
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_2/index.html",
    "href": "posts/2024-03-05_yusuf_app_2/index.html",
    "title": "Developing a Modern Data Pipeline — Part 2",
    "section": "",
    "text": "In Part 1, we explored web scraping. Now, we switch to an API for the data retrieval. The API I used is API-Football at rapidapi.com. You’ll need a subscription to access this API. See Part 3 for the final implementation of the pipeline\nimport os\nimport csv\nimport requests\nfrom requests.exceptions import RequestException\nfrom urllib3.exceptions import NewConnectionError, ConnectTimeoutError\nfrom time import sleep\nfrom config import api_key, api_endpoint\nThis section imports necessary libraries:\nAdditionally, it imports the API key and endpoint from the config module.\nNext, the code defines two variables:\ntop_level_folder = \"data\"\nos.makedirs(top_level_folder, exist_ok=True)\nThis section creates a top-level folder named “data” to store the downloaded CSV files. The os.makedirs function ensures the folder is created even if it doesn’t exist, and the exist_ok=True argument prevents errors if the folder already exists.\nurl = api_endpoint\nheaders = {\n    \"X-RapidAPI-Key\": api_key,\n    \"X-RapidAPI-Host\": \"api-football-v1.p.rapidapi.com\"\n}\n\ntotal_calls = len(unique_league_ids)\ncurrent_call = 0\n\n\ndef fetch_data(chosen_season=\"2023\"):\n    global current_call\n    for league_id in unique_league_ids:\n        try:\n            querystring = {\"league\": str(league_id), \"season\": chosen_season}\n            response = requests.get(url, headers=headers, params=querystring)\n\n            # Check if the response is successful\n            response.raise_for_status()\n\n            data = response.json()['response']\n\n            # ... (data processing and CSV creation) ...\n\n            current_call += 1\n            print(f\"{csv_data[0]['country']} {csv_data[0]['league_name']} called: ({current_call}/{total_calls})\")\n            print(f\"\\nCSV file saved at: {csv_file_path}\")\n\n        except (NewConnectionError, ConnectTimeoutError) as e:\n            # ... (error handling) ...\n            return\n\n        except RequestException as e:\n            # ... (error handling) ...\n\n        except Exception as e:\n            # ... (error handling) ...\n\n        # Sleep for 3 seconds before the next API call\n        sleep(3)\nThe core functionality resides within the fetch_data function. Let’s explore its steps:\nIn the final part of this series, we’ll dockerise the application and integrate tools such as DuckDB, MinIO and Airflow for orchestration."
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_2/index.html#setting-up-the-stage-imports-and-options",
    "href": "posts/2024-03-05_yusuf_app_2/index.html#setting-up-the-stage-imports-and-options",
    "title": "Developing a Modern Data Pipeline — Part 2",
    "section": "1. Setting Up The Stage: Imports and Options",
    "text": "1. Setting Up The Stage: Imports and Options"
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_2/index.html#data-organization-folders-and-paths",
    "href": "posts/2024-03-05_yusuf_app_2/index.html#data-organization-folders-and-paths",
    "title": "Developing a Modern Data Pipeline — Part 2",
    "section": "2. Data Organization: Folders and Paths",
    "text": "2. Data Organization: Folders and Paths"
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_2/index.html#api-interaction-and-data-extraction",
    "href": "posts/2024-03-05_yusuf_app_2/index.html#api-interaction-and-data-extraction",
    "title": "Developing a Modern Data Pipeline — Part 2",
    "section": "3. API Interaction and Data Extraction",
    "text": "3. API Interaction and Data Extraction"
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_1/index.html",
    "href": "posts/2024-03-05_yusuf_app_1/index.html",
    "title": "Developing a Modern Data Pipeline — Part 1",
    "section": "",
    "text": "In January, a client approached me regarding an analytics project involving football data. This blog post chronicles the first iteration of my data gathering approach: a custom web scraper. I detail my code section by section, explaining its logic. See Part 3 for the final implementation of the pipeline.\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nThis section imports the necessary libraries for web scraping. Here’s a breakdown of their roles:\nNext, the code defines Chrome webdriver options:\n# Define the Chrome webdriver options\noptions = webdriver.ChromeOptions()\noptions.add_argument(\"--headless\")\n\noptions.page_load_strategy = \"none\"\nThese lines configure the Chrome webdriver:\nFinally, a webdriver instance is created and directed to the target URL:\ndriver = webdriver.Chrome(options=options)\ndriver.get(url)\nThis section utilizes Selenium to simulate user behavior and dynamically load additional content:\nhtml_content = \"\"\n\nwhile True:\n    try:\n        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n\n        button = driver.find_element(By.CLASS_NAME, \"event__more--static\")\n\n        driver.execute_script(\"arguments[0].click();\", button)\n\n        time.sleep(2)\n\n        html_content += driver.page_source\n\n    except NoSuchElementException:\n        break\nThis loop continuously performs the following actions:\nOnce all data is loaded, the loop terminates, and the script utilizes BeautifulSoup for parsing content:\nsoup = BeautifulSoup(html_content, 'html.parser')\n\nmatch_containers = soup.find_all('div', class_='event__match event__match--static event__match--twoLine')\n\ndates = []\nhome_teams = []\nhome_scores = []\naway_scores = []\naway_teams = []\n\nmatch_containers = soup.find_all('div', class_=['event__match event__match--static event__match--twoLine', 'event__match event__match--static event__match--last event__match--twoLine'])\n\nfor match in match_containers:\n    date_time_str = match.find('div', class_='event__time').text.strip()\n    date_time_str = f\"2023-{date_time_str.replace('.', '-')}\"\n\n    home_team = match.find('div', class_='event__participant--home').text.strip()\n    home_score = int(match.find('div', class_='event__score--home').text.strip())\n    away_score = int(match.find('div', class_='event__score--away').text.strip())\n    away_team = match.find('div', class_='event__participant--away').text.strip()\n\n    dates.append(date_time_str)\n    home_teams.append(home_team)\n    home_scores.append(home_score)\n    away_scores.append(away_score)\n    away_teams.append(away_team)\nThis section performs the following tasks:\nAfter scraping individual match details, the script organizes and saves the data:\ndata = {'Date': dates, 'HomeTeam': home_teams, 'HomeScore': home_scores, 'AwayScore': away_scores, 'AwayTeam': away_teams}\ndf = pd.DataFrame(data)\n\nleague_name = url.split('/')[-3]\n\noutput_file_name = f\"{league_name}_results.csv\"\ndf.to_csv(output_file_name, index=False)\n\nprint(\"DONE SCRAPING!\")\ndriver.quit()\nHaving explored the web scraping technique, we’ll shift to the API approach in the next part, as the client opted for a robust solution using an API (as previously suggested by me).\nUp next, Part 2! of the series."
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_1/index.html#setting-up-the-stage-imports-and-options",
    "href": "posts/2024-03-05_yusuf_app_1/index.html#setting-up-the-stage-imports-and-options",
    "title": "Developing a Modern Data Pipeline — Part 1",
    "section": "1. Setting Up The Stage: Imports and Options",
    "text": "1. Setting Up The Stage: Imports and Options"
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_1/index.html#infinite-scrolling-and-button-handling",
    "href": "posts/2024-03-05_yusuf_app_1/index.html#infinite-scrolling-and-button-handling",
    "title": "Developing a Modern Data Pipeline — Part 1",
    "section": "2. Infinite Scrolling and Button Handling",
    "text": "2. Infinite Scrolling and Button Handling"
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_1/index.html#data-extraction-with-beautifulsoup",
    "href": "posts/2024-03-05_yusuf_app_1/index.html#data-extraction-with-beautifulsoup",
    "title": "Developing a Modern Data Pipeline — Part 1",
    "section": "3. Data Extraction with BeautifulSoup",
    "text": "3. Data Extraction with BeautifulSoup"
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_1/index.html#data-organization-and-saving",
    "href": "posts/2024-03-05_yusuf_app_1/index.html#data-organization-and-saving",
    "title": "Developing a Modern Data Pipeline — Part 1",
    "section": "4. Data Organization and Saving",
    "text": "4. Data Organization and Saving"
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_3/index.html",
    "href": "posts/2024-03-05_yusuf_app_3/index.html",
    "title": "Developing a Modern Data Pipeline — Part 3",
    "section": "",
    "text": "This third and final part of the project focuses on implementing and orchestrating the data pipeline. Here, we’ll delve into the code behind the scenes, exploring how Airflow DAGs orchestrate the various data processing steps, ensuring reliable execution and well-defined dependencies. We’ll also examine the data storage and transformation strategies, including the utilization of Minio for object storage and the Astro SDK for efficient in-database transformations within DuckDB.\nFinally, we’ll explore the user-friendly Streamlit frontend that allows users to interact with and visualize the processed football fixture data, gaining valuable insights from the analysis. By bringing these elements together, this part finalizes the series, transforming the initial concept into a robust and functional data analysis system.\nClick here for Part 1 and Part 2 of the series.\nThis project was inspired by the Astronomer Quick Start Repo for getting started with Apache Airflow.\nHere’s a breakdown of the chosen technologies:\n2. LOAD\n3. TRANSFORM\n4. FRONTEND"
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_3/index.html#system-architecture",
    "href": "posts/2024-03-05_yusuf_app_3/index.html#system-architecture",
    "title": "Developing a Modern Data Pipeline — Part 3",
    "section": "1. System architecture",
    "text": "1. System architecture"
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_3/index.html#implementation-details",
    "href": "posts/2024-03-05_yusuf_app_3/index.html#implementation-details",
    "title": "Developing a Modern Data Pipeline — Part 3",
    "section": "2. Implementation details",
    "text": "2. Implementation details"
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_3/index.html#initiating-the-dag-start.py",
    "href": "posts/2024-03-05_yusuf_app_3/index.html#initiating-the-dag-start.py",
    "title": "Developing a Modern Data Pipeline — Part 3",
    "section": "Initiating the DAG: start.py",
    "text": "Initiating the DAG: start.py\n\nstart.py defines the initial Airflow DAG using the @dag decorator.\nIt sets the DAG to run only once manually and defines default arguments and descriptive metadata.\nThe code creates a dedicated “duckdb” pool with one worker slot using a BashOperator.\nThis pool ensures controlled execution of tasks querying DuckDB.\n\n\n\"\"\"DAG that kicks off the pipeline by producing to the start dataset.\"\"\"\n\n# --------------- #\n# Package imports #\n# --------------- #\n\nfrom airflow import Dataset\nfrom airflow.decorators import dag\nfrom airflow.operators.bash import BashOperator\nfrom pendulum import datetime\n\n# -------------------- #\n# Local module imports #\n# -------------------- #\n\nfrom include.global_variables import global_variables as gv\n\n# --- #\n# DAG #\n# --- #\n\n\n@dag(\n    start_date=datetime(2023, 1, 1),\n    # after being unpaused this DAG will run once, afterwards it can be run\n    # manually with the play button in the Airflow UI\n    schedule=\"@once\",\n    catchup=False,\n    default_args=gv.default_args,\n    description=\"Run this DAG to start the pipeline!\",\n    tags=[\"start\", \"setup\"],\n)\ndef a_start():\n\n    # this task uses the BashOperator to run a bash command creating an Airflow\n    # pool called 'duckdb' which contains one worker slot. All tasks running\n    # queries against DuckDB will be assigned to this pool, preventing parallel\n    # requests to DuckDB.\n    create_duckdb_pool = BashOperator(\n        task_id=\"bash_task\",\n        bash_command=\"airflow pools list | grep -q 'duckdb' || airflow pools set duckdb 1 'Pool for duckdb'\",\n        outlets=[gv.DS_START],\n    )\n\nstart_dag = a_start()"
  },
  {
    "objectID": "posts/2024-03-05_yusuf_app_3/index.html#data-acquisition-extract.py",
    "href": "posts/2024-03-05_yusuf_app_3/index.html#data-acquisition-extract.py",
    "title": "Developing a Modern Data Pipeline — Part 3",
    "section": "Data Acquisition: extract.py",
    "text": "Data Acquisition: extract.py\nThis DAG, defined in extract.py, extracts football fixture data from an API and combines them.\nfetch_data function:\n\nFetches data from the API using the fetch_data function, likely defined in include.logic.\nOptionally passes arguments like the chosen season (e.g., “2023”).\n\nconcatenate_csvs function:\n\nCombines multiple CSV files located in a specified directory (gv.FIXTURES_DATA_FOLDER) into a single file (all_fixtures_combined.csv)\nUses os.walk to iterate through the directory and pandas.concat to combine the dataframes.\n\nDAG Definition:\n\nSimilar to start.py, the DAG is defined using the @dag decorator with various parameters.\nThis DAG depends on the gv.DS_START output, meaning it only runs after the initial DAG (start.py) finishes.\n\n\n\"\"\"DAG that extracts fixtures data from an API, and concatenates them into a single file saved to the local disk.\"\"\"\n# --------------- #\n# PACKAGE IMPORTS #\n# --------------- #\nfrom pendulum import datetime\nimport io\nimport os\nimport pandas as pd\nfrom airflow.decorators import dag\nfrom airflow.operators.python import PythonOperator\n\n# -------------------- #\n# Local module imports #\n# -------------------- #\nfrom include.global_variables import global_variables as gv\nfrom include.logic import fetch_data\n\n\ndef concatenate_csvs(root_dir, output_file):\n    csv_files = []\n    for root, _, files in os.walk(root_dir):\n        for file in files:\n            if file.endswith(\".csv\"):\n                file_path = os.path.join(root, file)\n                csv_files.append(file_path)\n\n    if not csv_files:\n        gv.task_log.warning(\"No CSV files found in the specified directory.\")\n        return\n\n    combined_df = pd.concat([pd.read_csv(file) for file in csv_files])\n    combined_df.to_csv(os.path.join(root_dir, output_file), index=False)\n\n\n# --- #\n# DAG #\n# --- #\n@dag(\n    start_date=datetime(2023, 1, 1),\n    # this DAG runs as soon as the \"start\" Dataset has been produced to\n    schedule=[gv.DS_START],\n    catchup=False,\n    default_args=gv.default_args,\n    description=\"extract fixtures data from API.\",\n    tags=[\"extract\", \"API\"],\n    render_template_as_native_obj=True,\n)\ndef b_extraction():\n    api_fetcher = PythonOperator(\n            task_id = \"fetch_data\",\n            python_callable= fetch_data,\n            op_kwargs = {\n                \"chosen_season\": \"2023\"\n            }\n        )\n\n\n    concatenator = PythonOperator(\n            task_id = \"concat_csvs\",\n            python_callable= concatenate_csvs,\n            op_kwargs = {\n                \"root_dir\": gv.FIXTURES_DATA_FOLDER,\n                \"output_file\": \"all_fixtures_combined.csv\"\n\n            },\n            outlets=[gv.DS_INGEST]\n        )\n\n    api_fetcher &gt;&gt; concatenator\nb_extraction()"
  }
]