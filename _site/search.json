[
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Causal-Behavioral Framework for Data Analysis\n\n\nIntroductory part to a series on Florent Buisson’s book, Behavioral Data Analysis with R & Python\n\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPizza Runner\n\n\nSecond part of a series of SQL case studies where I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoodie Fi\n\n\nThird part of a series of SQL case studies\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Bank\n\n\nFourth part of a series of SQL case studies… more CTEs!\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDanny’s Diner\n\n\nIn this first part of a series of SQL case studies, I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships of Variables in Sleep Data\n\n\nIn this post, I use regression, with an emphasis on understanding the casual relationships between variables\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#introduction",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#introduction",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "title": "Foodie Fi",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#database-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#database-connection",
    "title": "Foodie Fi",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"foodie_fi\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#queries",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#queries",
    "title": "Foodie Fi",
    "section": "Queries",
    "text": "Queries\n\n1. How many customers has Foodie-Fi ever had?\n\n\nSELECT count(DISTINCT customer_id)\nFROM subscriptions;\n\n\n1 records\n\n\ncount\n\n\n\n\n1000\n\n\n\n\n\n\n\n3. What is the monthly distribution of trial plan start_date values for our dataset use the start of the month as the group by value?\n\nSELECT upper(to_char(start_date, 'month')) as start_month,\n    count(*) frequency\nfrom subscriptions\nwhere plan_id = 0\ngroup by 1\nORDER BY 2 desc;\n\n\nDisplaying records 1 - 10\n\n\nstart_month\nfrequency\n\n\n\n\nMARCH\n94\n\n\nJULY\n89\n\n\nAUGUST\n88\n\n\nMAY\n88\n\n\nJANUARY\n88\n\n\nSEPTEMBER\n87\n\n\nDECEMBER\n84\n\n\nAPRIL\n81\n\n\nJUNE\n79\n\n\nOCTOBER\n79\n\n\n\n\n\n\n\n3. What plan start_date values occur after the year 2020 for our dataset? Show the breakdown by count of events for each plan_name ?\n\n\nSELECT p.plan_name,\n    count(*) as count_of_events_after_2020\nfrom subscriptions s\n    natural join plans p\nwhere EXTRACT(\n        year\n        from start_date\n    ) &gt; 2020\ngroup by 1;\n\n\n4 records\n\n\nplan_name\ncount_of_events_after_2020\n\n\n\n\npro annual\n63\n\n\nchurn\n71\n\n\npro monthly\n60\n\n\nbasic monthly\n8\n\n\n\n\n\n\n\n4. What is the customer count and percentage of customers who have churned rounded to 1 decimal place?\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id = 4\n)\nSELECT cte1.whole as total_customers,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_churned\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ntotal_customers\npct_churned\n\n\n\n\n2650\n12\n\n\n\n\n\n\n\n5. How many customers have churned straight after their initial free trial? what percentage is this rounded to the nearest whole number?\n\nwith cte as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\ncte2 as (\n    -- getting rows whose values satisfy the condition in the question\n    select *\n    from cte\n    where plan_id = 0\n        and lead_plan_id = 4\n) -- solution\nselect count(*) as count_of_customers_who_churned_after_free_trial\nfrom cte2 \n\n\n1 records\n\n\ncount_of_customers_who_churned_after_free_trial\n\n\n\n\n92\n\n\n\n\n\n\n\n6. What is the number and percentage of customer plans after their initial free trial?\n\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id &lt;&gt; 0\n)\nSELECT cte2.part as customer_count_after_trial_plan,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_ccatp\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ncustomer_count_after_trial_plan\npct_ccatp\n\n\n\n\n1650\n62\n\n\n\n\n\n\n\n7. How many customers have upgraded to an annual plan in 2020?\n\n\nwith cte1 as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\n-- filtering to only annual plans\ncte2 as (\n    select *,\n        lead_plan_id - plan_id as diff\n    from cte1\n    where lead_plan_id = 3\n) -- excluding churned customers and unupgraded plans\nselect count(DISTINCT customer_id) as upgraded_customers_2020_count\nfrom cte2\nwhere (diff &gt; 0)\n    and (lead_plan_id &lt;&gt; 4)\n    and EXTRACT(\n        year\n        from start_date\n    ) = 2020;\n\n\n1 records\n\n\nupgraded_customers_2020_count\n\n\n\n\n253\n\n\n\n\n\n\n\n8. How many days on average does it take for a customer to upgrade to an annual plan from the – day they join Foodie-Fi?\n\nwith cte1 as (\n    select *,\n        max(plan_id) over (partition by customer_id) as highest_plan_suscribed,\n        max(start_date) over (partition by customer_id) as date_of_hps,\n        min(start_date) over (partition by customer_id) as date_of_lps,\n        row_number() over (partition by customer_id) as sn\n    from subscriptions\n    order by customer_id,\n        start_date,\n        plan_id\n),\ncte2 as(\n    select *,\n        date_of_hps - date_of_lps as diff_in_days\n    from cte1\n    where highest_plan_suscribed = 3\n        and sn = 1\n)\nselect round(avg(diff_in_days)::numeric, 2) as avg_days_to_upgrade_to_annual\nfrom cte2\n\n\n1 records\n\n\navg_days_to_upgrade_to_annual\n\n\n\n\n105.95\n\n\n\n\n\n\n\n9. How many customers downgraded from a pro monthly to a basic monthly plan in 2020?\n\nwith cte1 as(\n        select *,\n            lead(plan_id) over(partition by customer_id) as lead_plan_id\n        from subscriptions\n        order by customer_id,\n            start_date,\n            plan_id\n    ),\n    cte2 as (\n        select customer_id,\n            plan_id,\n            lead_plan_id,\n            start_date\n            from cte1\n        where plan_id = 2\n            and lead_plan_id = 1\n            and EXTRACT(\n                year\n                from start_date\n            ) = 2020\n    )\nselect count(*) as number_of_customers_downgrade_from_prom_basm\nfrom cte2\n\n\n1 records\n\n\nnumber_of_customers_downgrade_from_prom_basm\n\n\n\n\n0"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#closing-the-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-3/index.html#closing-the-connection",
    "title": "Foodie Fi",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html",
    "href": "posts/2023-11-18-sleep-disorders/index.html",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "",
    "text": "The goal is to build a casual model using linear regression that explains quality of sleep, and logistic regression to explain sleep disorders\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n# descriptives\nlibrary(datawizard)\n\nWarning: package 'datawizard' was built under R version 4.3.2\n\nlibrary(kableExtra)\nlibrary(skimr)\nlibrary(qqplotr)\n\nWarning: package 'qqplotr' was built under R version 4.3.2\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.2\n\n# os\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.3.2"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#data-cleaning",
    "href": "posts/2023-11-18-sleep-disorders/index.html#data-cleaning",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Data cleaning",
    "text": "Data cleaning\nI observed the following issues in the data\n\nOccupation variable having levels with very few counts. Those with few counts can be consolidated with similar categories.\nThe BMI category variable has both “Normal” and “Normal Weight” as levels. This is most likely as data entry error.\nThe heart rate variable is not in a suitable format to work with. I’ll extract the systolic blood pressure from the variable as an integer. I would not need the diastolic blood pressure as I observed strong multicollinearity between both diastolic and systolic blood pressure.\n\n\n# observe occupations with low frequency counts\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# observe data entry error in bmi_category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# regrouping occupation variable so that occupations with lower counts are consolidated with other similar occupations. Simultaneously correcting data entry error in bmi_category variable\nsleep &lt;- sleep %&gt;% mutate( \n  occupation=ifelse(sleep$occupation %in% c(\"Software Engineer\", \"Scientist\", \"Engineer\"),\"Technical\", \nifelse(sleep$occupation %in% c(\"Salesperson\", \"Sales Representative\", \"Manager\"), \"Sales\", ifelse(sleep$occupation %in% c(\"Doctor\", \"Nurse\"), \"Medical\",\n                                                          as.character(occupation)))),\nbmi_category = ifelse(sleep$bmi_category %in% c(\"Normal\", \"Normal Weight\"), \"Normal\", as.character(sleep$bmi_category)))\n\n\n# extracting systolic blood pressure from the blood_pressure variable as it such a numeric variable is easier to work with. Subsequently dropping hear_rate variable\n\nsleep &lt;- sleep %&gt;% mutate(\nbp_sys= parse_number(str_sub(sleep$blood_pressure, 1, 3)),\nperson_id = as.character(person_id)\n) %&gt;% select(!heart_rate)\n\n\n# confirming changes\nhead(sleep)  %&gt;% gt() %&gt;% opt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      person_id\n      gender\n      age\n      occupation\n      sleep_duration\n      quality_of_sleep\n      physical_activity_level\n      stress_level\n      bmi_category\n      blood_pressure\n      daily_steps\n      sleep_disorder\n      bp_sys\n    \n  \n  \n    1\nMale\n27\nTechnical\n6.1\n6\n42\n6\nOverweight\n126/83\n4200\nNone\n126\n    2\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    3\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    4\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    5\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    6\nMale\n28\nTechnical\n5.9\n4\n30\n8\nObese\n140/90\n3000\nInsomnia\n140\n  \n  \n  \n\n\n\n\n\n# confirming changes to BMI category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# confirming changes to occupation variable\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#associations",
    "href": "posts/2023-11-18-sleep-disorders/index.html#associations",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Associations",
    "text": "Associations\n\nResearch Question: “What strong associations exists between quality_of_sleep and other variables in the dataset?\n\n# | warning: false\n# | message: false\n# packages required for associations\nlibrary(correlation)\nlibrary(GGally)\n\n\n# plotting the grid plot of scatterplots of numeric variables\nggscatmat(sleep %&gt;% select_if(is.numeric)) + theme_light()\n\n\n\n\n\n# showing relationships with strong correlations (greater than |+-0.4|)\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation() %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs)) %&gt;% filter(abs(r) &gt; 0.4) %&gt;% arrange(r) \ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.899\n0.000\n    sleep_duration\nstress_level\n−0.811\n0.000\n    age\nstress_level\n−0.422\n0.000\n    age\nquality_of_sleep\n0.474\n0.000\n    age\nbp_sys\n0.606\n0.000\n    physical_activity_level\ndaily_steps\n0.773\n0.000\n    sleep_duration\nquality_of_sleep\n0.883\n0.000\n  \n  \n  \n\n\n\n\n\n# robust correlation analysis since the data are skewed. Results is filtered to include only relationships involving quality_of_sleep. Sorted ascendingly by p-value\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation(method=\"percentage\") %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs))  %&gt;% arrange(p)  %&gt;% filter( Parameter1 == \"quality_of_sleep\" | Parameter2 == \"quality_of_sleep\") %&gt;% arrange(p) %&gt;% filter(p &lt; 0.05)\ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.910\n0.000\n    sleep_duration\nquality_of_sleep\n0.892\n0.000\n    age\nquality_of_sleep\n0.432\n0.000\n    quality_of_sleep\nphysical_activity_level\n0.178\n0.005\n  \n  \n  \n\n\n\n\n\nResults\nThe most significant relationships (at alpha = .05) are the relationships between:\n\nquality of sleep and stress level, r(372) =-0.91, p &lt; 0.0001\nquality of sleep and sleep duration, r(372) = 0.89, p &lt; 0.0001\nquality of sleep and age, r(372) =0.43, p &lt; 0.0001\nquality of sleep and physical activity level, r(372) =0.18, p = 0.0054\n\nThese are the variables we would fit to our linear regression model."
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#research-questions",
    "href": "posts/2023-11-18-sleep-disorders/index.html#research-questions",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research questions:",
    "text": "Research questions:\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\nIs the relationship between stress level and quality of sleep moderated by occupation?\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\n\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\n\n# Visualizing the relationship of the three variables\nggplot(sleep, aes(x = sleep_duration, y = quality_of_sleep, color = sleep_disorder)) +\ngeom_jitter(size=1, alpha=0.8) +\nlabs(x = \"Sleep duration\", y = \"Quality of sleep\", color = \"Sleep disorder\") +\ngeom_smooth(method = \"lm\", se = FALSE, linewidth=0.9) + theme_bw()\n\n\n\n\nThe plot shows that there might be some moderation effects.\n\n# making sleep_disorder a factor variable and setting the base level to \"None\"\nsleep &lt;- sleep %&gt;% mutate(\n  sleep_disorder = as_factor(sleep_disorder) %&gt;% fct_relevel(\"None\")\n)\n\n\nqs_lm_03 &lt;- lm(quality_of_sleep ~ sleep_duration * sleep_disorder, data=sleep)\nqs_lm_03 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-0.5750673\n0.36703899\n-1.566774\n1.180269e-01\n    sleep_duration\n1.1144522\n0.04963586\n22.452560\n6.124844e-71\n    sleep_disorderSleep Apnea\n-3.4341228\n0.57712441\n-5.950403\n6.233752e-09\n    sleep_disorderInsomnia\n-3.2040380\n1.11193745\n-2.881491\n4.190005e-03\n    sleep_duration:sleep_disorderSleep Apnea\n0.4802913\n0.08000178\n6.003508\n4.632039e-09\n    sleep_duration:sleep_disorderInsomnia\n0.4503706\n0.16657925\n2.703642\n7.176641e-03\n  \n  \n  \n\n\n\n\n\nInterpretation\n\n\n\n\n\n\nTip\n\n\n\nThe effect of increasing sleep duration in someone with no sleep disorder on sleep quality is 1.11\nThe effect of sleep apnea on sleep quality compared to someone with no disorders is -3.43\nThe effect of Insomnia on sleep quality compared to someone with no disorders is -3.20\nThe effect of increasing sleep duration in someone with sleep anea on sleep quality compared to someone with no disorders is 0.48\nThe effect of increasing sleep duration in someone with insomnia on sleep quality compared to someone with no disorders is 0.45\n\n\n\ninteractions::sim_slopes(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of sleep_duration when sleep_disorder = Insomnia: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.56   0.15   1.26    1.86    10.26   0.00\n\nSlope of sleep_duration when sleep_disorder = Sleep Apnea: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.59   0.06   1.48    1.71    27.35   0.00\n\nSlope of sleep_duration when sleep_disorder = None: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.11   0.03   1.05    1.17    36.50   0.00\n\n\nThe slopes for the three regressions are not significantly different\n\ninteractions::interact_plot(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Sleep disorder\"\n  )\n\n\n\n\nLooking at the slope, we can that the moderation effect is not significant.\n\n\nConclusion\nSleep disorder is not a moderator of the relationship between sleep duration and quality of sleep\n\n\n\nIs the relationship between stress level and quality of sleep moderated by occupation?\n\nqs_lm_04 &lt;- lm(quality_of_sleep ~ stress_level * occupation, data=sleep)\nqs_lm_04 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n9.3661417\n0.22464236\n41.6935685\n2.847324e-140\n    stress_level\n-0.3208661\n0.04725953\n-6.7894487\n4.624546e-11\n    occupationLawyer\n3.3611310\n1.08304354\n3.1034126\n2.063488e-03\n    occupationMedical\n1.3637168\n0.24305144\n5.6108156\n4.008125e-08\n    occupationSales\n3.5481440\n1.02746628\n3.4532948\n6.193572e-04\n    occupationTeacher\n-0.2842770\n0.33263086\n-0.8546322\n3.933200e-01\n    occupationTechnical\n2.0623438\n0.25972133\n7.9406026\n2.559029e-14\n    stress_level:occupationLawyer\n-0.6336793\n0.21426067\n-2.9575158\n3.304916e-03\n    stress_level:occupationMedical\n-0.2851357\n0.04939422\n-5.7726532\n1.679151e-08\n    stress_level:occupationSales\n-0.6791339\n0.15058782\n-4.5098857\n8.775327e-06\n    stress_level:occupationTeacher\n-0.1447393\n0.07087144\n-2.0422799\n4.184795e-02\n    stress_level:occupationTechnical\n-0.4718021\n0.05572917\n-8.4659819\n6.415721e-16\n  \n  \n  \n\n\n\n\n\ninteractions::sim_slopes(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of stress_level when occupation = Lawyer: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.95   0.03   -1.02   -0.89   -29.71   0.00\n\nSlope of stress_level when occupation = Accountant: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.32   0.04   -0.40   -0.24    -7.92   0.00\n\nSlope of stress_level when occupation = Teacher: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.47   0.09   -0.65   -0.28    -5.01   0.00\n\nSlope of stress_level when occupation = Sales: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -1.00   1.04   -3.04    1.04    -0.97   0.33\n\nSlope of stress_level when occupation = Medical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.61   0.01   -0.62   -0.59   -81.54   0.00\n\nSlope of stress_level when occupation = Technical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.79   0.07   -0.93   -0.65   -10.97   0.00\n\n\n\ninteractions::interact_plot(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Occupation\"\n  )\n\n\n\n\nAgain, we do not observe a significant moderation effect of occupation on the relationship between stress level and quality of sleep\n\n\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\nTo test this, we are going to run 3 regressions:\n\nThe total effect of physical_activity_level on quality_of_sleep.\nThe effect of physical_activity_level on quality_of_sleep that is mediated by daily_steps, a.k.a. the indirect effect\nThe effect of physical_activity_level on quality_of_sleep that is not mediated by daily_steps, a.k.a. the direct effect\n\n\nTotal effect\nWe first determine the total effect by running a regression of physical_activity_level on quality_of_sleep (without including daily_steps):\n\nlm(quality_of_sleep ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n6.657\n0.183\n36.307\n0.000\n    physical_activity_level\n0.011\n0.003\n3.792\n0.000\n  \n  \n  \n\n\n\n\nThe total effect is approximately 0.011, meaning that an increase of one unit in physical activity increases sleep quality by 0.001 units on average, not holding daily steps constant.\n\n\nMediated/Indirect Effect\nThe effect of physical_activity_level on quality_of_sleep mediated by daily_steps can be obtained by multiplying together the effect of physical_activity_level on daily_steps and the effect of daily_steps on quality_of_sleep.\n\n1. physical_activity_level on daily_steps (a)\n\nlm(daily_steps ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n3265.57619\n160.321889\n20.36887\n1.718735e-62\n    physical_activity_level\n60.01692\n2.556092\n23.47995\n1.968256e-75\n  \n  \n  \n\n\n\n\nOne unit increase in physical_activity_level increases daily_steps by an average of 60 steps\n\n\n2. daily_steps on quality_of_sleep\n\nlm(quality_of_sleep ~ daily_steps + physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n7.4500815511\n2.609352e-01\n28.551459\n1.151887e-95\n    daily_steps\n-0.0002428667\n5.802065e-05\n-4.185867\n3.551189e-05\n    physical_activity_level\n0.0256601109\n4.506428e-03\n5.694113\n2.524969e-08\n  \n  \n  \n\n\n\n\n(a * b) = 60.07 * -0.00024 = -0.0144\n\n\n\nDirect Effect\nThis is simply the coefficient of physical_activity_level in the above regression = 0.0257\nAccording to Zhao et al., what we have here isCompetitive Mediation (Regularly Partial Mediation)\nIn the competitive partial mediation hypothesis, it is assumed that the intermediate variable (daily_steps) could sometimes increase and at times decrease the relationship between the independent and dependent variables. i.e an “inconsistent” model.\n\n\n\nDecision tree for determining mediation"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "href": "posts/2023-11-18-sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research question: are significant differences in the mean sleep quality of both male and female participants",
    "text": "Research question: are significant differences in the mean sleep quality of both male and female participants\n\n# summary statistics of quality_of_sleep by gender\n\nby(cbind(data=sleep$quality_of_sleep), sleep$gender, psych::describe)\n\nINDICES: Female\n     vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 185 7.66 1.28      8    7.76 1.48   4   9     5 -0.49    -0.83 0.09\n------------------------------------------------------------ \nINDICES: Male\n     vars   n mean sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 189 6.97  1      7    7.01 1.48   4   9     5 -0.35    -0.59 0.07\n\n\nThe mean of female participants is one point higher than males. But is this difference significant?\n\n# plotting a violin-errorbar plot to visualise the relationship\nggplot(sleep, aes(gender,quality_of_sleep))+\ngeom_violin() +\nstat_summary(fun.data=\"mean_cl_normal\") + \ntheme_minimal()\n\n\n\n\nSince the errorbars do not overlap, we can be fairly confident that the difference is significant. Lets confirm this hunch with a Welsh’s t-test.\n\n t.test(quality_of_sleep ~ gender,\n                    data = sleep,\n                    paired = FALSE,\n                    var.equal = FALSE,\n                    conf.level = 0.95,\n                    na.action = na.exclude)\n\n\n    Welch Two Sample t-test\n\ndata:  quality_of_sleep by gender\nt = 5.8593, df = 347.96, p-value = 1.078e-08\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n 0.4627786 0.9304432\nsample estimates:\nmean in group Female   mean in group Male \n            7.664865             6.968254 \n\n\nBecause the p-value = 1.078e-08 is less than our alpha of .05, we can conclude that the difference between both genders is significant. Also, since the confidence interval expressing the true difference in means does not cross zero, we can be confident that there exists a true difference, with a chance of us being wrong 5% of the time.\n\neffectsize::cohens_d(quality_of_sleep ~ gender, data = sleep) %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Cohens_d\n      CI\n      CI_low\n      CI_high\n    \n  \n  \n    0.608\n0.950\n0.400\n0.815\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOn average, female participants have better sleep quality (M = 7.66, SE = 0.09), than those not given a cloak (M = 6.97, SE = 0.07). Sleep quality is significantly different for both genders: the mean difference, M = 0.69, 95% CI [0.46, 0.93], was significantly different from 0, t(347.96) = 5.85, p = 01.078e-08. This effect was quite large, d= 0.61[0.4, 0.82]"
  },
  {
    "objectID": "posts/2023-11-18-sleep-disorders/index.html#conclusion-1",
    "href": "posts/2023-11-18-sleep-disorders/index.html#conclusion-1",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Conclusion",
    "text": "Conclusion\nI created this post as a way of solidifying my understanding of these concepts. I find that documenting the material helps it to stick iwth me better. I hope that you find this helpful. My deepest gratitude goes to Andy Field for his exceptional book on statistical analysis with R."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chamber of (data) secrets",
    "section": "",
    "text": "The Causal-Behavioral Framework for Data Analysis\n\n\n\ncausal regression modelling\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPizza Runner\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoodie Fi\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Bank\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDanny’s Diner\n\n\n\nsql\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships of Variables in Sleep Data\n\n\n\nstats\n\n\ncausal regression modelling\n\n\n\n\n\n\n\nEdun Joshua\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Musings of a data mage",
    "section": "",
    "text": "See all"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Edun Joshua is a data enthusiast and an environmental engineering student. Data science is a reprieve from the existential crises that haunt him."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nFederal University of Agriculture, Abeokuta | Alabata, Ogun State\nB.Eng. in Environmental Engineering | March 2017 - March 2023"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#introduction",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#introduction",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "title": "Danny’s Diner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram\n\n\nSales\nThe sales table captures all customer_id level purchases with an corresponding order_date and product_id information for when and what menu items were ordered.\n\n\nMenu\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\n\n\nMembers\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\nkey concepts: CTEs, window functions"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#database-connection",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#database-connection",
    "title": "Danny’s Diner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"danny_diners\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#queries",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#queries",
    "title": "Danny’s Diner",
    "section": "Queries",
    "text": "Queries\nNow let’s convert business questions into SQL queries!\n\n1. What is the total amount each customer spent at the restaurant?\n\n\nselect s.customer_id as customer,\n   sum(m.price) as total_amount\nfrom sales s\n   inner join menu m on s.product_id = m.product_id\ngroup by customer;\n\n\n3 records\n\n\ncustomer\ntotal_amount\n\n\n\n\nB\n74\n\n\nC\n36\n\n\nA\n76\n\n\n\n\n\n\n\n2. How many days has each customer visited the restaurant?\n\n\nselect customer_id,\n   count(\n      distinct extract(\n         day\n         from order_date\n      )\n   ) as no_of_days_visited\nfrom sales\ngroup by customer_id\norder by customer_id;\n\n\n3 records\n\n\ncustomer_id\nno_of_days_visited\n\n\n\n\nA\n4\n\n\nB\n5\n\n\nC\n2\n\n\n\n\n\nThis SQL query calculates the number of days each customer has visited a restaurant by grouping the sales transactions by customer ID, extracting the day from the order date, counting the distinct days, and ordering the results by customer ID.\n\n\n3. What was the first item from the menu purchased by each customer?\n\n\nwith cte as (\n   select customer_id,\n      order_date,\n      row_number() over (\n         partition by customer_id\n         order by order_date\n      ) as order_rank,\n      product_id\n   from sales\n)\nselect c.customer_id,\n   c.order_date,\n   c.order_rank,\n   m.product_name\nfrom cte c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\norder_date\norder_rank\nproduct_name\n\n\n\n\nA\n2021-01-01\n1\nsushi\n\n\nB\n2021-01-01\n1\ncurry\n\n\nC\n2021-01-01\n1\nramen\n\n\n\n\n\nWithin the CTE, the row_number() function is employed to assign a ranking to each order for each customer based on the order_date. The ranking starts from 1, indicating the first order for each customer. The main query then selects the customer ID, order date, order rank, and product name from the CTE, joining the menu table to retrieve the product name corresponding to the product ID. Finally, it filters the results to include only records with an order rank of 1, effectively selecting the first item purchased for each customer.\n\n\n4. What is the most purchased item on the menu and how many times was it purchased by all customers?\n\n\nwith cte as(\n   select product_id,\n      count(product_id)\n   from sales\n   group by product_id\n   order by count(product_id) desc\n   limit 1\n) -- Most purchased item on the menu is the product with the id 3 which is ramen, according to cte\nselect customer_id,\n   count(product_id) as count_of_most_purchased_product\nfrom sales\nwhere product_id in (\n      select product_id\n      from cte\n   )\ngroup by customer_id;\n\n\n3 records\n\n\ncustomer_id\ncount_of_most_purchased_product\n\n\n\n\nA\n3\n\n\nB\n2\n\n\nC\n3\n\n\n\n\n\nThe query utilizes a common table expression (CTE) named cte to generate a temporary result set. Within the CTE, the count() function is employed to count the number of times each product ID appears in the sales table. The results are then sorted in descending order based on the product count. The limit 1 clause restricts the output to the top row, effectively identifying the product ID with the highest count.\nThe main query then selects the customer ID and the count of the most purchased product for each customer. It filters the sales table to include only records where the product_id matches the one identified in the CTE, effectively focusing on the most purchased item. Finally, it groups the results by customer_id to determine how many times each customer purchased the most popular item.\n\n\n5. Which item was the most popular for each customer?\n\n\nwith cte_1 as (\n   select customer_id,\n      product_id,\n      count(product_id) as count_of_item\n   from sales\n   group by customer_id,\n      product_id\n),\ncte_2 as (\n   select *,\n      row_number() over (\n         partition by customer_id\n         order by count_of_item desc\n      ) as order_rank\n   from cte_1\n)\nselect c.customer_id,\n   m.product_name,\n   c.count_of_item\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\nproduct_name\ncount_of_item\n\n\n\n\nC\nramen\n3\n\n\nB\nramen\n2\n\n\nA\nramen\n3\n\n\n\n\n\nThis query aims to identify the most popular item for each customer. The query utilizes two common table expressions to process the data and generate the desired output. The first CTE, named cte_1, calculates the count of each product purchased by each customer. It groups the rows in the sales table by customer_id and product_id, and then counts the occurrences of each product ID for each customer. This step determines the frequency of each product purchase for each customer.\nThe second CTE, named cte_2, assigns a ranking to each product for each customer based on the purchase frequency calculated in cte_1. It uses the row_number() function and partitions the data by customer_id, sorting within each partition by the count_of_item in descending order. This step effectively identifies the product with the highest purchase frequency (i.e., the most popular item) for each customer.\nThe main query then selects the customer ID, product name, and purchase count for each customer’s most popular item. It joins the menu table to obtain the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the most popular item for each customer is selected.\n\n\n6. Which item was purchased first by the customer after they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date\n      ) as order_rank\n   from cte_1\n   where order_date &gt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nramen\n2021-01-10\n1\n\n\nB\nsushi\n2021-01-11\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date, ensuring a chronological order of transactions. The second CTE, named cte_2, focuses on purchases made after each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is later than the join_date (membership start date).\nAdditionally, it assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date. This identifies the first purchase (order_rank = 1) made by each customer after becoming a member. The main query then selects the customer ID, product name, order date, and order rank for each customer’s first purchase after becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the first purchase is selected.\n\n\n7. Which item was purchased just before the customer became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      join_date,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date desc\n      ) as order_rank\n   from cte_1\n   where order_date &lt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nsushi\n2021-01-01\n1\n\n\nB\nsushi\n2021-01-04\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date in ascending order, ensuring a chronological sequence of transactions.\nThe second CTE, cte_2, focuses on purchases made before each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date).\nIt assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date in descending order. This identifies the last purchase (order_rank = 1) made by each customer before becoming a member.\nThe main query then selects the customer ID, product name, order date, and order rank for each customer’s last purchase before becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the last purchase is selected.\n\n\n8. What is the total items and amount spent for each member before they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n      natural join menu\n   order by order_date\n)\nselect customer_id,\n   count(distinct product_id) as count_of_products,\n   sum(price) as total_amount_spent\nfrom cte_1\nwhere order_date &lt; join_date\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ncount_of_products\ntotal_amount_spent\n\n\n\n\nA\n2\n25\n\n\nB\n2\n40\n\n\n\n\n\nThe query utilizes a CTE named cte_1 to prepare the data and simplifies the aggregation in the main query.\nThe main query then aggregates the data for each customer based on their membership status. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date). For each customer, it counts the distinct product_id values to determine the total number of unique items purchased and calculates the sum of price values to determine the total amount spent. The results are grouped by customer_id to provide individual summaries for each member.\n\n\n9. If each $1 spent equates to 10 points and sushi has a 2x points multiplier - how many points would each customer have?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points\n   from members\n      natural join sales\n      natural join menu\n)\nselect customer_id,\n   sum(points) as total_points\nfrom cte\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n860\n\n\nB\n940\n\n\n\n\n\nThe CTE combines the members, sales, and menu tables, providing a comprehensive view of customer memberships, their purchases, and the corresponding product names.\nThe main query then summarizes the points earned for each customer. It groups the data from the CTE by customer_id and calculates the sum of points values for each group, effectively determining the total points earned by each customer.\n\n\n10. In the first week after a customer joins the program (including their join date) they earn 2x points on all items, – not just sushi - how many points do customer A and B have at the end of January?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points,\n      case\n         when order_date - join_date &lt;= 7 then 2\n         else 1\n      end multiplier\n   from members\n      natural join sales\n      natural join menu\n),\ncte_2 as (\n   select *,\n      points * multiplier as total_points\n   from cte\n)\nselect customer_id,\n   sum(total_points) as total_points\nfrom cte_2\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n1720\n\n\nB\n1760\n\n\n\n\n\nThe first CTE, named cte, joins the members, sales, and menu tables, and calculates the points earned for each purchase using a conditional CASE expression, similar to the previous query.\nAdditionally, it assigns a multiplier to each purchase based on whether it falls within the first week after the customer’s join date. For purchases within the first week, the multiplier is 2 (double points); for purchases outside the first week, the multiplier is 1 (standard points).\nThe second CTE, named cte_2, simplifies the calculation by multiplying the points and multiplier columns for each purchase, effectively determining the total points earned per transaction. The main query then summarizes the points earned for each customer, including the double points accrued during the first week. It groups the data from cte_2 by customer_id and calculates the sum of total_points values for each group, providing the total points earned by customer A and customer B at the end of January\n\n\n11. Recreate the following table output using the available data:\n\n\nselect s.customer_id,\n    s.order_date,\n    men.product_name,\n    men.price,\n    CASE\n        WHEN s.order_date &gt;= m.join_date THEN 'Y'\n        ELSE 'N'\n    END\nfrom sales s\n    LEFT JOIN menu men ON s.product_id = men.product_id\n    LEFT JOIN members m on m.customer_id = s.customer_id\nORDER BY s.customer_id,\n    s.order_date;\n\n\nDisplaying records 1 - 10\n\n\ncustomer_id\norder_date\nproduct_name\nprice\ncase\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY"
  },
  {
    "objectID": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#closing-the-connection",
    "href": "posts/2023-11-18-8weeks-sql-challenge-1/index.html#closing-the-connection",
    "title": "Danny’s Diner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#introduction",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#introduction",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "title": "Pizza Runner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#database-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#database-connection",
    "title": "Pizza Runner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"pizza_runner\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#queries",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#queries",
    "title": "Pizza Runner",
    "section": "Queries",
    "text": "Queries\n\nA. Pizza Metrics\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\nselect count(pizza_id)\nfrom customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n14\n\n\n\n\n\n\n\n2. How many unique customer orders were made?\n\n\nSELECT count(DISTINCT order_id) from customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n10\n\n\n\n\n\n\n\n3. How many successful orders were delivered by each runner?\n\n\nselect runner_id,\n    count(order_id) as count_of_successful_orders\nfrom runner_orders\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP BY runner_id;\n\n\n3 records\n\n\nrunner_id\ncount_of_successful_orders\n\n\n\n\n3\n1\n\n\n2\n3\n\n\n1\n4\n\n\n\n\n\n\n\n4. How many of each type of pizza was delivered?\n\n\nselect pn.pizza_name as pizza,\n    count(co.pizza_id) as count_of_pizza_delivered\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id\n    INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP by pn.pizza_name;\n\n\n2 records\n\n\npizza\ncount_of_pizza_delivered\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\n\n5. How many Vegetarian and Meatlovers were ordered by each customer?\n\n\nselect co.customer_id as customers, count(co.pizza_id) as count_of_pizza_ordered from customer_orders co INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id group by 1;\n\n\n5 records\n\n\ncustomers\ncount_of_pizza_ordered\n\n\n\n\n101\n3\n\n\n103\n4\n\n\n104\n3\n\n\n105\n1\n\n\n102\n3\n\n\n\n\n\n\n\n6. What was the maximum number of pizzas delivered in a single order?\n\n\nselect ro.order_id as order,\n    count(co.pizza_id) as number_of_pizzas\nfrom runner_orders ro\n    INNER JOIN customer_orders co ON ro.order_id = co.order_id\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 1;\n\n\n1 records\n\n\norder\nnumber_of_pizzas\n\n\n\n\n4\n3\n\n\n\n\n\n\n\n7. For each customer, how many delivered pizzas had at least 1 change and how many? and no changes?\n\n-- PART 1\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with at least 1 changes\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n        or (\n            exclusions not in ('', 'null')\n            or extras not in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n4 records\n\n\ncustomer_id\ndelivered_pizzas_with_changes\n\n\n\n\n102\n1\n\n\n105\n1\n\n\n104\n3\n\n\n103\n3\n\n\n\n\n\n\n-- PART 2: orders with no changes\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions = ''\n        and extras = ''\n        or (\n            exclusions in ('', 'null')\n            or extras in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_no_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n5 records\n\n\ncustomer_id\ndelivered_pizzas_with_no_changes\n\n\n\n\n101\n2\n\n\n102\n3\n\n\n103\n3\n\n\n104\n2\n\n\n105\n1\n\n\n\n\n\n\n\n8. How many pizzas were delivered that had both exclusions and extras?\n\n\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with both exclusions and extras\ncte2 as (\n    select *\n    from customer_orders\n    where (\n            exclusions &lt;&gt; 'null'\n            and extras &lt;&gt; 'null'\n        )\n        and exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n)\nselect *\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id;\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\norder_id..7\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10minutes\nnull\n10\n104\n1\n2, 6\n1, 4\n2020-01-11 18:34:49\n\n\n\n\n\n\n\n9. What was the total volume of pizzas ordered for each hour of the day?\n\n\nselect EXTRACT (\n        hour\n        from order_time\n    ) as hour_of_day,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n6 records\n\n\nhour_of_day\npizza_volume\n\n\n\n\n18\n3\n\n\n21\n3\n\n\n23\n3\n\n\n13\n3\n\n\n19\n1\n\n\n11\n1\n\n\n\n\n\n\n\n10. What was the volume of orders for each day of the week?\n\n-- select EXTRACT (\n--        dow\n--        from order_time\n--    ) as day_of_week,\n--    count(pizza_id) as pizza_volume\n-- from customer_orders\n-- GROUP BY 1;\n-- or\nselect to_char(order_time, 'Day') as day_of_week,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n4 records\n\n\nday_of_week\npizza_volume\n\n\n\n\nSaturday\n5\n\n\nThursday\n3\n\n\nFriday\n1\n\n\nWednesday\n5\n\n\n\n\n\n\n\n\nB. Runner and Customer Experience\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\n\nselect EXTRACT(\n        week\n        from registration_date\n    ),\n    count(runner_id)\nfrom runners\nGROUP BY 1;\n\n\n3 records\n\n\nextract\ncount\n\n\n\n\n1\n1\n\n\n53\n2\n\n\n2\n1\n\n\n\n\n\n\n\n2. What was the average time in minutes it took for each runner to arrive at the Pizza – Runner HQ to pickup the order?\n\nselect case\n        when ro.pickup_time = 'null' then null\n        else round(\n            EXTRACT(\n                epoch\n                from (\n                        ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                    )\n            ) / 60,\n            2\n        )\n    end as duration_till_pickup,\n    ro.pickup_time,\n    co.order_time\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id; \n\n\nDisplaying records 1 - 10\n\n\nduration_till_pickup\npickup_time\norder_time\n\n\n\n\n10.53\n2020-01-01 18:15:34\n2020-01-01 18:05:02\n\n\n10.03\n2020-01-01 19:10:54\n2020-01-01 19:00:52\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n10.47\n2020-01-08 21:10:57\n2020-01-08 21:00:29\n\n\nNA\nnull\n2020-01-08 21:03:13\n\n\n10.27\n2020-01-08 21:30:45\n2020-01-08 21:20:29\n\n\n\n\n\nThis query calculates the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pick up the order. It first joins the customer_orders and runner_orders tables to get the order information and the runner’s pickup time. Then, it checks if the pickup_time is null, and if so, it sets the corresponding duration_till_pickup to null. Otherwise, it calculates the time difference between the pickup_time and the order_time in seconds using the EXTRACT() and TIMESTAMP()functions. It then converts the time difference to minutes and rounds it to two decimal places. Finally, it selects the duration_till_pickup, pickup_time, and order_time for each order.\n\n\n3. What was the average time in minutes it took for each runner to deliver pizzas?\n\n\nselect runner_id,\n    round(\n        avg(\n            case\n                when left(duration, 2) ~ '^\\d+$' THEN cast(left(duration, 2) as integer)\n                else null\n            end\n        ),\n        2\n    ) as extracted_minutes\nfrom runner_orders\ngroup by runner_id;\n\n\n3 records\n\n\nrunner_id\nextracted_minutes\n\n\n\n\n3\n15.00\n\n\n2\n26.67\n\n\n1\n22.25\n\n\n\n\n\n\n\n5. What was the difference between the longest and shortest delivery times for all?\n\n\n-- If we define delivery times as the duration between ro.pickup_time - co.order_time + ro.duration\n-- then:\nwith cte1 as (\n    select case\n            when ro.pickup_time = 'null' then null\n            else round(\n                EXTRACT(\n                    epoch\n                    from (\n                            ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                        )\n                ) / 60,\n                2\n            )\n        end as duration_till_pickup,\n        ro.pickup_time,\n        co.order_time,\n        round(\n            case\n                when left(ro.duration, 2) ~ '^\\d+$' THEN cast(left(ro.duration, 2) as integer)\n                else null\n            end,\n            2\n        ) as cleaned_duration_minutes\n    from customer_orders co\n        INNER JOIN runner_orders ro on co.order_id = ro.order_id\n)\nselect max(duration_till_pickup + cleaned_duration_minutes) as longest_delivery_time,\n    min(duration_till_pickup + cleaned_duration_minutes) as shortest_delivery_time,\n    max(duration_till_pickup + cleaned_duration_minutes) - min(duration_till_pickup + cleaned_duration_minutes) as difference\nfrom cte1;\n\n\n1 records\n\n\nlongest_delivery_time\nshortest_delivery_time\ndifference\n\n\n\n\n69.28\n25.47\n43.81\n\n\n\n\n\nNow, this is a monstrous looking one 😅 .\nA (CTE) named cte1 is created, extracting relevant information such as duration_till_pickup, pickup_time, order_time, and cleaned_duration_minutes from the customer_orders (co) and runner_orders (ro) tables. The duration_till_pickup is calculated as the time between pickup and order in minutes. The cleaned_duration_minutes extracts the duration in minutes from the ro.duration field.\nThe main query then computes the maximum, minimum, and the difference between the sum of duration_till_pickup and cleaned_duration_minutes. These values represent the longest, shortest, and the time difference between delivery times for all orders.\n\n\n6. What was the average speed for each runner for each delivery and do you notice – any trend for these values?\n\n\nwith cte as (\n    select runner_id,\n        case\n            when distance ~ '.*' THEN cast(substring(distance, '[0-9\\-+\\.]+') as float)\n            else null\n        end as cleaned_distance_km,\n        case\n            when duration ~ '.*' THEN cast(substring(duration, '[0-9\\-+\\.]+') as float) / 60\n            else null\n        end as cleaned_duration_hr\n    from runner_orders\n)\nselect runner_id,\n    avg(cleaned_distance_km / cleaned_duration_hr) as speed_km_hr\nfrom cte\ngroup by 1;\n\n\n3 records\n\n\nrunner_id\nspeed_km_hr\n\n\n\n\n3\n40.00000\n\n\n2\n62.90000\n\n\n1\n45.53611\n\n\n\n\n\nThis psql query calculates the average speed for each runner for each delivery. It uses a CTE named cte to clean and extract relevant information such as cleaned_distance_km and cleaned_duration_hr from the runner_orders table. The main query then computes the average speed (cleaned_distance_km / cleaned_duration_hr) for each runner and presents the results grouped by runner_id. This allows you to observe trends in the average speed of each runner across their deliveries.\n\n\n7. What is the successful delivery percentage for each runner?\n\nwith part as (\n    select cte.runner_id,\n        count(*) as part_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    where cancel is null\n    group by runner_id\n),\nwhole as (\n    select runner_id,\n        count(*) as whole_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    group by runner_id\n)\nselect p.runner_id,\n    case\n        when w.whole_cancel = 0 then null\n        else round(\n            (p.part_cancel::numeric / w.whole_cancel) * 100,\n            2\n        )\n    end as percent\nfrom part p\n    inner join whole w on p.runner_id = w.runner_id;\n\n\n3 records\n\n\nrunner_id\npercent\n\n\n\n\n3\n50\n\n\n2\n75\n\n\n1\n100\n\n\n\n\n\n\n\n\nC. Ingredient Optimisation\n\n1. What are the standard ingredients for each pizza?\n\n\nselect pn.pizza_name, pt.topping_name\nfrom pizza_names pn inner join new_pizza_recipes np\non pn.pizza_id = np.pizza_id\ninner join pizza_toppings pt on pt.topping_id::text = ANY (np.toppings)\n\n\nDisplaying records 1 - 10\n\n\npizza_name\ntopping_name\n\n\n\n\nMeatlovers\nBacon\n\n\nMeatlovers\nBBQ Sauce\n\n\nMeatlovers\nBeef\n\n\nMeatlovers\nCheese\n\n\nMeatlovers\nChicken\n\n\nMeatlovers\nMushrooms\n\n\nMeatlovers\nPepperoni\n\n\nMeatlovers\nSalami\n\n\nVegetarian\nCheese\n\n\nVegetarian\nMushrooms\n\n\n\n\n\n\n\n\nD. Pricing and Ratings\n\n1. If a Meat Lovers pizza costs $12 and Vegetarian costs $10 and there were no charges for changes, how much money has Pizza Runner made so far if there are no delivery fees?\n\n-- Creating a view of cleaned data\nCREATE OR REPLACE VIEW clean_runner_orders AS\nselect order_id,\n    runner_id,\n    CASE\n        WHEN pickup_time = 'null' THEN NULL\n        ELSE pickup_time::TIMESTAMP\n    END,\n    cast(substring(distance, '[0-9\\-+\\.]+') as float) as distance,\n    cast(substring(duration, '[0-9\\-+\\.]+') as float) as duration,\n    nullif(cancellation, '') || nullif(cancellation, 'null') as cancellation\nfrom runner_orders;\n--\n\n\nwith cte1 as (\n    select co.pizza_id,\n        count(co.pizza_id) as quantity_sold\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n    GROUP BY 1\n)\nselect pizza_id,\n    quantity_sold * price as revenue\nfrom (\n        select *,\n            CASE\n                WHEN cte1.pizza_id = 1 THEN 12\n                WHEN cte1.pizza_id = 2 THEN 10\n            END AS price\n        from cte1\n    ) sq\n\n\n2 records\n\n\npizza_id\nrevenue\n\n\n\n\n1\n108\n\n\n2\n30\n\n\n\n\n\n\n-- Creating a random integer generator\ncreate or replace function random_between(low int, high int) returns int as $$ begin return floor(random() * (high - low + 1) + low);\nend;\n$$ language 'plpgsql' STRICT;\n\n\n\n2. If a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre travelled, how much money does Pizza Runner have left over after these deliveries?\n\nwith cte as (\n    select co.pizza_id,\n        ro.distance * 0.3 as runner_cost,\n        CASE\n            WHEN co.pizza_id = 1 THEN 12\n            WHEN co.pizza_id = 2 THEN 10\n        END AS price\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n)\nselect sum(price) revenue,\n    round(sum(runner_cost)::numeric, 2) cost,\n    round(sum(price) - sum(runner_cost)::numeric, 2) profit\nfrom cte\n\n\n1 records\n\n\nrevenue\ncost\nprofit\n\n\n\n\n138\n64.62\n73.38"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#closing-the-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-2/index.html#closing-the-connection",
    "title": "Pizza Runner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html",
    "title": "Data Bank",
    "section": "",
    "text": "Data Bank runs just like any other digital bank - but with a distributed data storage platform. Customers are allocated cloud data storage limits which are directly linked to how much money they have in their accounts. The management team at Data Bank want to increase their total customer base - but also need some help tracking just how much data storage their customers will need. This case study is all about calculating metrics, growth and helping the business analyse their data in a smart way to better forecast and plan for their future developments!\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"data_bank\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "title": "Data Bank",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#database-connection",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#database-connection",
    "title": "Data Bank",
    "section": "Database Connection",
    "text": "Database Connection"
  },
  {
    "objectID": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#queries",
    "href": "posts/2023-11-19-8weeks-sql-challenge-4/index.html#queries",
    "title": "Data Bank",
    "section": "Queries",
    "text": "Queries"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html",
    "href": "posts/2023-11-21-BDA-1/index.html",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "",
    "text": "#|message: false\n#|wwarning: false\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(boot)\n\nWarning: package 'boot' was built under R version 4.3.2\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.2\n\nlibrary(broom)\n# library(boot) #Required for Bootstrap simulations\n# library(rstudioapi)"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#variable-description",
    "href": "posts/2023-11-21-BDA-1/index.html#variable-description",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Variable description",
    "text": "Variable description"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "href": "posts/2023-11-21-BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Why Correlation Is Not Causation: A Confounder in Action",
    "text": "Why Correlation Is Not Causation: A Confounder in Action\n\n#Reading the data\nstand_tib &lt;- read_csv(\"chap1-stand_data.csv\")\n\nRows: 2400 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): icecream_sales, malt_sales, summer_months, temps\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(stand_tib) %&gt;% gt() %&gt;% opt_stylize(style = 6, color=\"cyan\")\n\n\n\n\n\n  \n    \n    \n      icecream_sales\n      malt_sales\n      summer_months\n      temps\n    \n  \n  \n    25649.78\n28592.18\n0\n28.59314\n    32694.93\n37152.84\n0\n37.13064\n    26467.80\n24074.19\n0\n24.11190\n    43438.79\n49169.47\n0\n49.21807\n    52452.63\n47249.40\n0\n47.25766\n    34130.66\n34228.50\n0\n34.25746\n  \n  \n  \n\n\n\n\n\n#Running linear regressions \n#Biased model (coeff should be 1,000)\nlm(icecream_sales ~ temps, data = stand_tib) %&gt;% \n  tidy() %&gt;% \n  gt() %&gt;% opt_stylize(style = 6, color =\"cyan\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-6169.844\n531.5062\n-11.60823\n2.385706e-30\n    temps\n1171.335\n9.0268\n129.76197\n0.000000e+00\n  \n  \n  \n\n\n\n\n\n#correct model for icecream (coeffs should be 1,000 and 20,000)\nsummary(lm(icecream_sales ~ temps + summer_months, data=stand_tib)) %&gt;% \n  tidy() %&gt;% \n  gt() %&gt;% opt_stylize(style = 6, color =\"cyan\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-7.068995\n374.401378\n-0.01888079\n0.9849378\n    temps\n1001.481846\n6.822242\n146.79657789\n0.0000000\n    summer_months\n19556.774716\n361.393093\n54.11496531\n0.0000000\n  \n  \n  \n\n\n\n\n\n#Model biased by extra controlling\n#note how the standard error of the estimate of temp increases a lot\n#note how the p-values for \nsummary(lm(icecream_sales ~ malt_sales + temps + summer_months, \n             data = stand_tib)) %&gt;% \n  tidy() %&gt;% \n  gt() %&gt;% opt_stylize(style = 6, color =\"cyan\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-15.827140\n374.580924\n-0.04225293\n0.9663006\n    malt_sales\n-1.701081\n2.082874\n-0.81669912\n0.4141816\n    temps\n2702.788460\n2083.160984\n1.29744580\n0.1946027\n    summer_months\n19548.167641\n361.571822\n54.06441117\n0.0000000\n  \n  \n  \n\n\n\n\n\nggplot(stand_tib, aes(x=icecream_sales, y=malt_sales)) + \n    geom_point() + labs(x='Malt sales', y='Iced coffee sales') + \n    theme_classic()"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#second-example",
    "href": "posts/2023-11-21-BDA-1/index.html#second-example",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Second Example",
    "text": "Second Example\n\n#Reading the data\nsurvey_tib &lt;- read_csv(\"chap1-survey_data.csv\")\n\nRows: 10000 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): vanilla, chocolate, shopped\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Reformatting shopped variable to binary\nsurvey_tib &lt;- survey_tib %&gt;%\n  mutate(shopped = as.factor(shopped) %&gt;% relevel(ref=\"0\"))\n\n\n## Figure 1-5. (a) Tastes for vanilla and chocolate are uncorrelated in the \n## overall population. (b) Taste for vanilla is higher for people who shop at \n## the ice-cream stand than for people who don’t. (c) Same thing with the taste for chocolate.\n\n#Scatterplot of chocolate versus vanilla taste \na &lt;- ggplot(survey_tib, aes(x=vanilla, y=chocolate)) + geom_point() +\n  xlim(c(0,28)) + ylim(c(0,28)) + geom_smooth(formula = 'y~x', method = lm, se = FALSE) +\n  labs(x='Taste for vanilla', y='Taste for chocolate') + theme_minimal()\n\n#Boxplot of vanilla taste against shopping behavior\nb &lt;- ggplot(survey_tib, aes(shopped, vanilla)) + geom_boxplot() +\n  labs(x='Shopped (Y/N)', y='Taste for vanilla') + ylim(c(0,30)) + theme_minimal()\n\n#Boxplot of chocolate taste against shopping behavior\nc &lt;- ggplot(survey_tib, aes(shopped, chocolate)) + geom_boxplot() +\n  labs(x='Shopped (Y/N)', y='Taste for chocolate') + ylim(c(0,30)) + theme_minimal()\n\nggpubr::ggarrange(a,b,c, ncol = 3)\n\n\n\n\n\n\n\n### Plotting same scatterplot but for shoppers only\n\n#Figure 1-6 Taste for vanilla and chocolate among shoppers. \nd &lt;- ggplot(survey_tib %&gt;% filter(shopped=='1'), aes(x=vanilla, y=chocolate)) + geom_point() + \n  labs(x='Taste for vanilla', y='Taste for chocolate') + geom_smooth(formula='y~x', method = lm,se = FALSE) + theme_minimal()\nd\n\n\n\n\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\n\nWarning: package 'RPostgres' was built under R version 4.3.2\n\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"pizza_runner\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#queries",
    "href": "posts/2023-11-21-BDA-1/index.html#queries",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Queries",
    "text": "Queries\n\nA. Pizza Metrics\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\nselect count(pizza_id)\nfrom customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n14\n\n\n\n\n\n\n\n2. How many unique customer orders were made?\n\n\nSELECT count(DISTINCT order_id) from customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n10\n\n\n\n\n\n\n\n3. How many successful orders were delivered by each runner?\n\n\nselect runner_id,\n    count(order_id) as count_of_successful_orders\nfrom runner_orders\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP BY runner_id;\n\n\n3 records\n\n\nrunner_id\ncount_of_successful_orders\n\n\n\n\n3\n1\n\n\n2\n3\n\n\n1\n4\n\n\n\n\n\n\n\n4. How many of each type of pizza was delivered?\n\n\nselect pn.pizza_name as pizza,\n    count(co.pizza_id) as count_of_pizza_delivered\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id\n    INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP by pn.pizza_name;\n\n\n2 records\n\n\npizza\ncount_of_pizza_delivered\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\n\n5. How many Vegetarian and Meatlovers were ordered by each customer?\n\n\nselect co.customer_id as customers, count(co.pizza_id) as count_of_pizza_ordered from customer_orders co INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id group by 1;\n\n\n5 records\n\n\ncustomers\ncount_of_pizza_ordered\n\n\n\n\n101\n3\n\n\n103\n4\n\n\n104\n3\n\n\n105\n1\n\n\n102\n3\n\n\n\n\n\n\n\n6. What was the maximum number of pizzas delivered in a single order?\n\n\nselect ro.order_id as order,\n    count(co.pizza_id) as number_of_pizzas\nfrom runner_orders ro\n    INNER JOIN customer_orders co ON ro.order_id = co.order_id\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 1;\n\n\n1 records\n\n\norder\nnumber_of_pizzas\n\n\n\n\n4\n3\n\n\n\n\n\n\n\n7. For each customer, how many delivered pizzas had at least 1 change and how many? and no changes?\n\n-- PART 1\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with at least 1 changes\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n        or (\n            exclusions not in ('', 'null')\n            or extras not in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n4 records\n\n\ncustomer_id\ndelivered_pizzas_with_changes\n\n\n\n\n102\n1\n\n\n105\n1\n\n\n104\n3\n\n\n103\n3\n\n\n\n\n\n\n-- PART 2: orders with no changes\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions = ''\n        and extras = ''\n        or (\n            exclusions in ('', 'null')\n            or extras in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_no_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n5 records\n\n\ncustomer_id\ndelivered_pizzas_with_no_changes\n\n\n\n\n101\n2\n\n\n102\n3\n\n\n103\n3\n\n\n104\n2\n\n\n105\n1\n\n\n\n\n\n\n\n8. How many pizzas were delivered that had both exclusions and extras?\n\n\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with both exclusions and extras\ncte2 as (\n    select *\n    from customer_orders\n    where (\n            exclusions &lt;&gt; 'null'\n            and extras &lt;&gt; 'null'\n        )\n        and exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n)\nselect *\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id;\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\norder_id..7\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10minutes\nnull\n10\n104\n1\n2, 6\n1, 4\n2020-01-11 18:34:49\n\n\n\n\n\n\n\n9. What was the total volume of pizzas ordered for each hour of the day?\n\n\nselect EXTRACT (\n        hour\n        from order_time\n    ) as hour_of_day,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n6 records\n\n\nhour_of_day\npizza_volume\n\n\n\n\n18\n3\n\n\n21\n3\n\n\n23\n3\n\n\n13\n3\n\n\n19\n1\n\n\n11\n1\n\n\n\n\n\n\n\n10. What was the volume of orders for each day of the week?\n\n-- select EXTRACT (\n--        dow\n--        from order_time\n--    ) as day_of_week,\n--    count(pizza_id) as pizza_volume\n-- from customer_orders\n-- GROUP BY 1;\n-- or\nselect to_char(order_time, 'Day') as day_of_week,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n4 records\n\n\nday_of_week\npizza_volume\n\n\n\n\nSaturday\n5\n\n\nThursday\n3\n\n\nFriday\n1\n\n\nWednesday\n5\n\n\n\n\n\n\n\n\nB. Runner and Customer Experience\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\n\nselect EXTRACT(\n        week\n        from registration_date\n    ),\n    count(runner_id)\nfrom runners\nGROUP BY 1;\n\n\n3 records\n\n\nextract\ncount\n\n\n\n\n1\n1\n\n\n53\n2\n\n\n2\n1\n\n\n\n\n\n\n\n2. What was the average time in minutes it took for each runner to arrive at the Pizza – Runner HQ to pickup the order?\n\nselect case\n        when ro.pickup_time = 'null' then null\n        else round(\n            EXTRACT(\n                epoch\n                from (\n                        ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                    )\n            ) / 60,\n            2\n        )\n    end as duration_till_pickup,\n    ro.pickup_time,\n    co.order_time\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id; \n\n\nDisplaying records 1 - 10\n\n\nduration_till_pickup\npickup_time\norder_time\n\n\n\n\n10.53\n2020-01-01 18:15:34\n2020-01-01 18:05:02\n\n\n10.03\n2020-01-01 19:10:54\n2020-01-01 19:00:52\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n10.47\n2020-01-08 21:10:57\n2020-01-08 21:00:29\n\n\nNA\nnull\n2020-01-08 21:03:13\n\n\n10.27\n2020-01-08 21:30:45\n2020-01-08 21:20:29\n\n\n\n\n\nThis query calculates the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pick up the order. It first joins the customer_orders and runner_orders tables to get the order information and the runner’s pickup time. Then, it checks if the pickup_time is null, and if so, it sets the corresponding duration_till_pickup to null. Otherwise, it calculates the time difference between the pickup_time and the order_time in seconds using the EXTRACT() and TIMESTAMP()functions. It then converts the time difference to minutes and rounds it to two decimal places. Finally, it selects the duration_till_pickup, pickup_time, and order_time for each order.\n\n\n3. What was the average time in minutes it took for each runner to deliver pizzas?\n\n\nselect runner_id,\n    round(\n        avg(\n            case\n                when left(duration, 2) ~ '^\\d+$' THEN cast(left(duration, 2) as integer)\n                else null\n            end\n        ),\n        2\n    ) as extracted_minutes\nfrom runner_orders\ngroup by runner_id;\n\n\n3 records\n\n\nrunner_id\nextracted_minutes\n\n\n\n\n3\n15.00\n\n\n2\n26.67\n\n\n1\n22.25\n\n\n\n\n\n\n\n5. What was the difference between the longest and shortest delivery times for all?\n\n\n-- If we define delivery times as the duration between ro.pickup_time - co.order_time + ro.duration\n-- then:\nwith cte1 as (\n    select case\n            when ro.pickup_time = 'null' then null\n            else round(\n                EXTRACT(\n                    epoch\n                    from (\n                            ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                        )\n                ) / 60,\n                2\n            )\n        end as duration_till_pickup,\n        ro.pickup_time,\n        co.order_time,\n        round(\n            case\n                when left(ro.duration, 2) ~ '^\\d+$' THEN cast(left(ro.duration, 2) as integer)\n                else null\n            end,\n            2\n        ) as cleaned_duration_minutes\n    from customer_orders co\n        INNER JOIN runner_orders ro on co.order_id = ro.order_id\n)\nselect max(duration_till_pickup + cleaned_duration_minutes) as longest_delivery_time,\n    min(duration_till_pickup + cleaned_duration_minutes) as shortest_delivery_time,\n    max(duration_till_pickup + cleaned_duration_minutes) - min(duration_till_pickup + cleaned_duration_minutes) as difference\nfrom cte1;\n\n\n1 records\n\n\nlongest_delivery_time\nshortest_delivery_time\ndifference\n\n\n\n\n69.28\n25.47\n43.81\n\n\n\n\n\nNow, this is a monstrous looking one 😅 .\nA (CTE) named cte1 is created, extracting relevant information such as duration_till_pickup, pickup_time, order_time, and cleaned_duration_minutes from the customer_orders (co) and runner_orders (ro) tables. The duration_till_pickup is calculated as the time between pickup and order in minutes. The cleaned_duration_minutes extracts the duration in minutes from the ro.duration field.\nThe main query then computes the maximum, minimum, and the difference between the sum of duration_till_pickup and cleaned_duration_minutes. These values represent the longest, shortest, and the time difference between delivery times for all orders.\n\n\n6. What was the average speed for each runner for each delivery and do you notice – any trend for these values?\n\n\nwith cte as (\n    select runner_id,\n        case\n            when distance ~ '.*' THEN cast(substring(distance, '[0-9\\-+\\.]+') as float)\n            else null\n        end as cleaned_distance_km,\n        case\n            when duration ~ '.*' THEN cast(substring(duration, '[0-9\\-+\\.]+') as float) / 60\n            else null\n        end as cleaned_duration_hr\n    from runner_orders\n)\nselect runner_id,\n    avg(cleaned_distance_km / cleaned_duration_hr) as speed_km_hr\nfrom cte\ngroup by 1;\n\n\n3 records\n\n\nrunner_id\nspeed_km_hr\n\n\n\n\n3\n40.00000\n\n\n2\n62.90000\n\n\n1\n45.53611\n\n\n\n\n\nThis psql query calculates the average speed for each runner for each delivery. It uses a CTE named cte to clean and extract relevant information such as cleaned_distance_km and cleaned_duration_hr from the runner_orders table. The main query then computes the average speed (cleaned_distance_km / cleaned_duration_hr) for each runner and presents the results grouped by runner_id. This allows you to observe trends in the average speed of each runner across their deliveries.\n\n\n7. What is the successful delivery percentage for each runner?\n\nwith part as (\n    select cte.runner_id,\n        count(*) as part_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    where cancel is null\n    group by runner_id\n),\nwhole as (\n    select runner_id,\n        count(*) as whole_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    group by runner_id\n)\nselect p.runner_id,\n    case\n        when w.whole_cancel = 0 then null\n        else round(\n            (p.part_cancel::numeric / w.whole_cancel) * 100,\n            2\n        )\n    end as percent\nfrom part p\n    inner join whole w on p.runner_id = w.runner_id;\n\n\n3 records\n\n\nrunner_id\npercent\n\n\n\n\n3\n50\n\n\n2\n75\n\n\n1\n100\n\n\n\n\n\n\n\n\nC. Ingredient Optimisation\n\n1. What are the standard ingredients for each pizza?\n\n\nselect pn.pizza_name, pt.topping_name\nfrom pizza_names pn inner join new_pizza_recipes np\non pn.pizza_id = np.pizza_id\ninner join pizza_toppings pt on pt.topping_id::text = ANY (np.toppings)\n\n\nDisplaying records 1 - 10\n\n\npizza_name\ntopping_name\n\n\n\n\nMeatlovers\nBacon\n\n\nMeatlovers\nBBQ Sauce\n\n\nMeatlovers\nBeef\n\n\nMeatlovers\nCheese\n\n\nMeatlovers\nChicken\n\n\nMeatlovers\nMushrooms\n\n\nMeatlovers\nPepperoni\n\n\nMeatlovers\nSalami\n\n\nVegetarian\nCheese\n\n\nVegetarian\nMushrooms\n\n\n\n\n\n\n\n\nD. Pricing and Ratings\n\n1. If a Meat Lovers pizza costs $12 and Vegetarian costs $10 and there were no charges for changes, how much money has Pizza Runner made so far if there are no delivery fees?\n\n-- Creating a view of cleaned data\nCREATE OR REPLACE VIEW clean_runner_orders AS\nselect order_id,\n    runner_id,\n    CASE\n        WHEN pickup_time = 'null' THEN NULL\n        ELSE pickup_time::TIMESTAMP\n    END,\n    cast(substring(distance, '[0-9\\-+\\.]+') as float) as distance,\n    cast(substring(duration, '[0-9\\-+\\.]+') as float) as duration,\n    nullif(cancellation, '') || nullif(cancellation, 'null') as cancellation\nfrom runner_orders;\n--\n\n\nwith cte1 as (\n    select co.pizza_id,\n        count(co.pizza_id) as quantity_sold\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n    GROUP BY 1\n)\nselect pizza_id,\n    quantity_sold * price as revenue\nfrom (\n        select *,\n            CASE\n                WHEN cte1.pizza_id = 1 THEN 12\n                WHEN cte1.pizza_id = 2 THEN 10\n            END AS price\n        from cte1\n    ) sq\n\n\n2 records\n\n\npizza_id\nrevenue\n\n\n\n\n1\n108\n\n\n2\n30\n\n\n\n\n\n\n-- Creating a random integer generator\ncreate or replace function random_between(low int, high int) returns int as $$ begin return floor(random() * (high - low + 1) + low);\nend;\n$$ language 'plpgsql' STRICT;\n\n\n\n2. If a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre travelled, how much money does Pizza Runner have left over after these deliveries?\n\nwith cte as (\n    select co.pizza_id,\n        ro.distance * 0.3 as runner_cost,\n        CASE\n            WHEN co.pizza_id = 1 THEN 12\n            WHEN co.pizza_id = 2 THEN 10\n        END AS price\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n)\nselect sum(price) revenue,\n    round(sum(runner_cost)::numeric, 2) cost,\n    round(sum(price) - sum(runner_cost)::numeric, 2) profit\nfrom cte\n\n\n1 records\n\n\nrevenue\ncost\nprofit\n\n\n\n\n138\n64.62\n73.38"
  },
  {
    "objectID": "posts/2023-11-21-BDA-1/index.html#closing-the-connection",
    "href": "posts/2023-11-21-BDA-1/index.html#closing-the-connection",
    "title": "Chapter 1: The Causal-Behavioral Framework for Data Analysis",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html",
    "title": "Data Bank",
    "section": "",
    "text": "Data Bank runs just like any other digital bank - but with a distributed data storage platform. Customers are allocated cloud data storage limits which are directly linked to how much money they have in their accounts. The management team at Data Bank want to increase their total customer base - but also need some help tracking just how much data storage their customers will need. This case study is all about calculating metrics, growth and helping the business analyse their data in a smart way to better forecast and plan for their future developments!\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"data_bank\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#entity-relationship-diagram",
    "title": "Data Bank",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#database-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#database-connection",
    "title": "Data Bank",
    "section": "Database Connection",
    "text": "Database Connection"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#queries",
    "href": "posts/2023-11-19_8weeks-sql-challenge-4/index.html#queries",
    "title": "Data Bank",
    "section": "Queries",
    "text": "Queries"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#introduction",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#introduction",
    "title": "Pizza Runner",
    "section": "",
    "text": "Monitoring KPIs of a pizza delivery business"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#entity-relationship-diagram",
    "title": "Pizza Runner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#database-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#database-connection",
    "title": "Pizza Runner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"pizza_runner\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#queries",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#queries",
    "title": "Pizza Runner",
    "section": "Queries",
    "text": "Queries\n\nA. Pizza Metrics\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\nselect count(pizza_id)\nfrom customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n14\n\n\n\n\n\n\n\n2. How many unique customer orders were made?\n\n\nSELECT count(DISTINCT order_id) from customer_orders;\n\n\n1 records\n\n\ncount\n\n\n\n\n10\n\n\n\n\n\n\n\n3. How many successful orders were delivered by each runner?\n\n\nselect runner_id,\n    count(order_id) as count_of_successful_orders\nfrom runner_orders\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP BY runner_id;\n\n\n3 records\n\n\nrunner_id\ncount_of_successful_orders\n\n\n\n\n3\n1\n\n\n2\n3\n\n\n1\n4\n\n\n\n\n\n\n\n4. How many of each type of pizza was delivered?\n\n\nselect pn.pizza_name as pizza,\n    count(co.pizza_id) as count_of_pizza_delivered\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id\n    INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id\nWHERE cancellation is NULL\n    or cancellation not IN (\n        'Restaurant Cancellation',\n        'Customer Cancellation'\n    )\nGROUP by pn.pizza_name;\n\n\n2 records\n\n\npizza\ncount_of_pizza_delivered\n\n\n\n\nMeatlovers\n9\n\n\nVegetarian\n3\n\n\n\n\n\n\n\n5. How many Vegetarian and Meatlovers were ordered by each customer?\n\n\nselect co.customer_id as customers, count(co.pizza_id) as count_of_pizza_ordered from customer_orders co INNER JOIN pizza_names pn on pn.pizza_id = co.pizza_id group by 1;\n\n\n5 records\n\n\ncustomers\ncount_of_pizza_ordered\n\n\n\n\n101\n3\n\n\n103\n4\n\n\n104\n3\n\n\n105\n1\n\n\n102\n3\n\n\n\n\n\n\n\n6. What was the maximum number of pizzas delivered in a single order?\n\n\nselect ro.order_id as order,\n    count(co.pizza_id) as number_of_pizzas\nfrom runner_orders ro\n    INNER JOIN customer_orders co ON ro.order_id = co.order_id\nGROUP BY 1\nORDER BY 2 DESC\nLIMIT 1;\n\n\n1 records\n\n\norder\nnumber_of_pizzas\n\n\n\n\n4\n3\n\n\n\n\n\n\n\n7. For each customer, how many delivered pizzas had at least 1 change and how many? and no changes?\n\n-- PART 1\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with at least 1 changes\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n        or (\n            exclusions not in ('', 'null')\n            or extras not in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n4 records\n\n\ncustomer_id\ndelivered_pizzas_with_changes\n\n\n\n\n102\n1\n\n\n105\n1\n\n\n104\n3\n\n\n103\n3\n\n\n\n\n\n\n-- PART 2: orders with no changes\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\ncte2 as (\n    select *\n    from customer_orders\n    where exclusions = ''\n        and extras = ''\n        or (\n            exclusions in ('', 'null')\n            or extras in ('', 'null', null)\n        )\n)\nselect cte2.customer_id,\n    count(pizza_id) delivered_pizzas_with_no_changes\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id\nGROUP BY 1;\n\n\n5 records\n\n\ncustomer_id\ndelivered_pizzas_with_no_changes\n\n\n\n\n101\n2\n\n\n102\n3\n\n\n103\n3\n\n\n104\n2\n\n\n105\n1\n\n\n\n\n\n\n\n8. How many pizzas were delivered that had both exclusions and extras?\n\n\n-- delivered orders\nwith cte1 as (\n    select *\n    from runner_orders\n    where cancellation is null\n        or cancellation in ('null', '')\n),\n-- orders with both exclusions and extras\ncte2 as (\n    select *\n    from customer_orders\n    where (\n            exclusions &lt;&gt; 'null'\n            and extras &lt;&gt; 'null'\n        )\n        and exclusions &lt;&gt; ''\n        and extras &lt;&gt; ''\n)\nselect *\nfrom cte1\n    inner join cte2 on cte1.order_id = cte2.order_id;\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder_id\nrunner_id\npickup_time\ndistance\nduration\ncancellation\norder_id..7\ncustomer_id\npizza_id\nexclusions\nextras\norder_time\n\n\n\n\n10\n1\n2020-01-11 18:50:20\n10km\n10minutes\nnull\n10\n104\n1\n2, 6\n1, 4\n2020-01-11 18:34:49\n\n\n\n\n\n\n\n9. What was the total volume of pizzas ordered for each hour of the day?\n\n\nselect EXTRACT (\n        hour\n        from order_time\n    ) as hour_of_day,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n6 records\n\n\nhour_of_day\npizza_volume\n\n\n\n\n18\n3\n\n\n21\n3\n\n\n23\n3\n\n\n13\n3\n\n\n19\n1\n\n\n11\n1\n\n\n\n\n\n\n\n10. What was the volume of orders for each day of the week?\n\n-- select EXTRACT (\n--        dow\n--        from order_time\n--    ) as day_of_week,\n--    count(pizza_id) as pizza_volume\n-- from customer_orders\n-- GROUP BY 1;\n-- or\nselect to_char(order_time, 'Day') as day_of_week,\n    count(pizza_id) as pizza_volume\nfrom customer_orders\nGROUP BY 1;\n\n\n4 records\n\n\nday_of_week\npizza_volume\n\n\n\n\nSaturday\n5\n\n\nThursday\n3\n\n\nFriday\n1\n\n\nWednesday\n5\n\n\n\n\n\n\n\n\nB. Runner and Customer Experience\n\n1. How many runners signed up for each 1 week period? (i.e. week starts 2021-01-01)\n\n\nselect EXTRACT(\n        week\n        from registration_date\n    ),\n    count(runner_id)\nfrom runners\nGROUP BY 1;\n\n\n3 records\n\n\nextract\ncount\n\n\n\n\n1\n1\n\n\n53\n2\n\n\n2\n1\n\n\n\n\n\n\n\n2. What was the average time in minutes it took for each runner to arrive at the Pizza – Runner HQ to pickup the order?\n\nselect case\n        when ro.pickup_time = 'null' then null\n        else round(\n            EXTRACT(\n                epoch\n                from (\n                        ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                    )\n            ) / 60,\n            2\n        )\n    end as duration_till_pickup,\n    ro.pickup_time,\n    co.order_time\nfrom customer_orders co\n    INNER JOIN runner_orders ro on co.order_id = ro.order_id; \n\n\nDisplaying records 1 - 10\n\n\nduration_till_pickup\npickup_time\norder_time\n\n\n\n\n10.53\n2020-01-01 18:15:34\n2020-01-01 18:05:02\n\n\n10.03\n2020-01-01 19:10:54\n2020-01-01 19:00:52\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n21.23\n2020-01-03 00:12:37\n2020-01-02 23:51:23\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n29.28\n2020-01-04 13:53:03\n2020-01-04 13:23:46\n\n\n10.47\n2020-01-08 21:10:57\n2020-01-08 21:00:29\n\n\nNA\nnull\n2020-01-08 21:03:13\n\n\n10.27\n2020-01-08 21:30:45\n2020-01-08 21:20:29\n\n\n\n\n\nThis query calculates the average time in minutes it took for each runner to arrive at the Pizza Runner HQ to pick up the order. It first joins the customer_orders and runner_orders tables to get the order information and the runner’s pickup time. Then, it checks if the pickup_time is null, and if so, it sets the corresponding duration_till_pickup to null. Otherwise, it calculates the time difference between the pickup_time and the order_time in seconds using the EXTRACT() and TIMESTAMP()functions. It then converts the time difference to minutes and rounds it to two decimal places. Finally, it selects the duration_till_pickup, pickup_time, and order_time for each order.\n\n\n3. What was the average time in minutes it took for each runner to deliver pizzas?\n\n\nselect runner_id,\n    round(\n        avg(\n            case\n                when left(duration, 2) ~ '^\\d+$' THEN cast(left(duration, 2) as integer)\n                else null\n            end\n        ),\n        2\n    ) as extracted_minutes\nfrom runner_orders\ngroup by runner_id;\n\n\n3 records\n\n\nrunner_id\nextracted_minutes\n\n\n\n\n3\n15.00\n\n\n2\n26.67\n\n\n1\n22.25\n\n\n\n\n\n\n\n5. What was the difference between the longest and shortest delivery times for all?\n\n\n-- If we define delivery times as the duration between ro.pickup_time - co.order_time + ro.duration\n-- then:\nwith cte1 as (\n    select case\n            when ro.pickup_time = 'null' then null\n            else round(\n                EXTRACT(\n                    epoch\n                    from (\n                            ro.pickup_time::TIMESTAMP - co.order_time::TIMESTAMP\n                        )\n                ) / 60,\n                2\n            )\n        end as duration_till_pickup,\n        ro.pickup_time,\n        co.order_time,\n        round(\n            case\n                when left(ro.duration, 2) ~ '^\\d+$' THEN cast(left(ro.duration, 2) as integer)\n                else null\n            end,\n            2\n        ) as cleaned_duration_minutes\n    from customer_orders co\n        INNER JOIN runner_orders ro on co.order_id = ro.order_id\n)\nselect max(duration_till_pickup + cleaned_duration_minutes) as longest_delivery_time,\n    min(duration_till_pickup + cleaned_duration_minutes) as shortest_delivery_time,\n    max(duration_till_pickup + cleaned_duration_minutes) - min(duration_till_pickup + cleaned_duration_minutes) as difference\nfrom cte1;\n\n\n1 records\n\n\nlongest_delivery_time\nshortest_delivery_time\ndifference\n\n\n\n\n69.28\n25.47\n43.81\n\n\n\n\n\nNow, this is a monstrous looking one 😅 .\nA (CTE) named cte1 is created, extracting relevant information such as duration_till_pickup, pickup_time, order_time, and cleaned_duration_minutes from the customer_orders (co) and runner_orders (ro) tables. The duration_till_pickup is calculated as the time between pickup and order in minutes. The cleaned_duration_minutes extracts the duration in minutes from the ro.duration field.\nThe main query then computes the maximum, minimum, and the difference between the sum of duration_till_pickup and cleaned_duration_minutes. These values represent the longest, shortest, and the time difference between delivery times for all orders.\n\n\n6. What was the average speed for each runner for each delivery and do you notice – any trend for these values?\n\n\nwith cte as (\n    select runner_id,\n        case\n            when distance ~ '.*' THEN cast(substring(distance, '[0-9\\-+\\.]+') as float)\n            else null\n        end as cleaned_distance_km,\n        case\n            when duration ~ '.*' THEN cast(substring(duration, '[0-9\\-+\\.]+') as float) / 60\n            else null\n        end as cleaned_duration_hr\n    from runner_orders\n)\nselect runner_id,\n    avg(cleaned_distance_km / cleaned_duration_hr) as speed_km_hr\nfrom cte\ngroup by 1;\n\n\n3 records\n\n\nrunner_id\nspeed_km_hr\n\n\n\n\n3\n40.00000\n\n\n2\n62.90000\n\n\n1\n45.53611\n\n\n\n\n\nThis psql query calculates the average speed for each runner for each delivery. It uses a CTE named cte to clean and extract relevant information such as cleaned_distance_km and cleaned_duration_hr from the runner_orders table. The main query then computes the average speed (cleaned_distance_km / cleaned_duration_hr) for each runner and presents the results grouped by runner_id. This allows you to observe trends in the average speed of each runner across their deliveries.\n\n\n7. What is the successful delivery percentage for each runner?\n\nwith part as (\n    select cte.runner_id,\n        count(*) as part_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    where cancel is null\n    group by runner_id\n),\nwhole as (\n    select runner_id,\n        count(*) as whole_cancel\n    from (\n            select runner_id,\n                nullif(cancellation, '') || nullif(cancellation, 'null') as cancel\n            from runner_orders\n        ) cte\n    group by runner_id\n)\nselect p.runner_id,\n    case\n        when w.whole_cancel = 0 then null\n        else round(\n            (p.part_cancel::numeric / w.whole_cancel) * 100,\n            2\n        )\n    end as percent\nfrom part p\n    inner join whole w on p.runner_id = w.runner_id;\n\n\n3 records\n\n\nrunner_id\npercent\n\n\n\n\n3\n50\n\n\n2\n75\n\n\n1\n100\n\n\n\n\n\n\n\n\nC. Ingredient Optimisation\n\n1. What are the standard ingredients for each pizza?\n\n\nselect pn.pizza_name, pt.topping_name\nfrom pizza_names pn inner join new_pizza_recipes np\non pn.pizza_id = np.pizza_id\ninner join pizza_toppings pt on pt.topping_id::text = ANY (np.toppings)\n\n\nDisplaying records 1 - 10\n\n\npizza_name\ntopping_name\n\n\n\n\nMeatlovers\nBacon\n\n\nMeatlovers\nBBQ Sauce\n\n\nMeatlovers\nBeef\n\n\nMeatlovers\nCheese\n\n\nMeatlovers\nChicken\n\n\nMeatlovers\nMushrooms\n\n\nMeatlovers\nPepperoni\n\n\nMeatlovers\nSalami\n\n\nVegetarian\nCheese\n\n\nVegetarian\nMushrooms\n\n\n\n\n\n\n\n\nD. Pricing and Ratings\n\n1. If a Meat Lovers pizza costs $12 and Vegetarian costs $10 and there were no charges for changes, how much money has Pizza Runner made so far if there are no delivery fees?\n\n-- Creating a view of cleaned data\nCREATE OR REPLACE VIEW clean_runner_orders AS\nselect order_id,\n    runner_id,\n    CASE\n        WHEN pickup_time = 'null' THEN NULL\n        ELSE pickup_time::TIMESTAMP\n    END,\n    cast(substring(distance, '[0-9\\-+\\.]+') as float) as distance,\n    cast(substring(duration, '[0-9\\-+\\.]+') as float) as duration,\n    nullif(cancellation, '') || nullif(cancellation, 'null') as cancellation\nfrom runner_orders;\n--\n\n\nwith cte1 as (\n    select co.pizza_id,\n        count(co.pizza_id) as quantity_sold\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n    GROUP BY 1\n)\nselect pizza_id,\n    quantity_sold * price as revenue\nfrom (\n        select *,\n            CASE\n                WHEN cte1.pizza_id = 1 THEN 12\n                WHEN cte1.pizza_id = 2 THEN 10\n            END AS price\n        from cte1\n    ) sq\n\n\n2 records\n\n\npizza_id\nrevenue\n\n\n\n\n1\n108\n\n\n2\n30\n\n\n\n\n\n\n-- Creating a random integer generator\ncreate or replace function random_between(low int, high int) returns int as $$ begin return floor(random() * (high - low + 1) + low);\nend;\n$$ language 'plpgsql' STRICT;\n\n\n\n2. If a Meat Lovers pizza was $12 and Vegetarian $10 fixed prices with no cost for extras and each runner is paid $0.30 per kilometre travelled, how much money does Pizza Runner have left over after these deliveries?\n\nwith cte as (\n    select co.pizza_id,\n        ro.distance * 0.3 as runner_cost,\n        CASE\n            WHEN co.pizza_id = 1 THEN 12\n            WHEN co.pizza_id = 2 THEN 10\n        END AS price\n    from clean_runner_orders ro\n        inner join customer_orders co on co.order_id = ro.order_id\n    where cancellation is null\n)\nselect sum(price) revenue,\n    round(sum(runner_cost)::numeric, 2) cost,\n    round(sum(price) - sum(runner_cost)::numeric, 2) profit\nfrom cte\n\n\n1 records\n\n\nrevenue\ncost\nprofit\n\n\n\n\n138\n64.62\n73.38"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#closing-the-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-2/index.html#closing-the-connection",
    "title": "Pizza Runner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#introduction",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#introduction",
    "title": "Danny’s Diner",
    "section": "",
    "text": "A restaurant, Danny’s Diner, sells 3 foods: sushi, curry and ramen. Danny’s Diner is in need of your assistance to help the restaurant stay afloat - the restaurant has captured some very basic data from their few months of operation but have no idea how to use their data to help them run the business.\nYou have 3 key datasets for this case study:\n\nsales\nmenu\nmembers"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#entity-relationship-diagram",
    "title": "Danny’s Diner",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram\n\n\nSales\nThe sales table captures all customer_id level purchases with an corresponding order_date and product_id information for when and what menu items were ordered.\n\n\nMenu\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\n\n\nMembers\nThe final members table captures the join_date when a customer_id joined the beta version of the Danny’s Diner loyalty program.\nkey concepts: CTEs, window functions"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#database-connection",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#database-connection",
    "title": "Danny’s Diner",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"danny_diners\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#queries",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#queries",
    "title": "Danny’s Diner",
    "section": "Queries",
    "text": "Queries\nNow let’s convert business questions into SQL queries!\n\n1. What is the total amount each customer spent at the restaurant?\n\n\nselect s.customer_id as customer,\n   sum(m.price) as total_amount\nfrom sales s\n   inner join menu m on s.product_id = m.product_id\ngroup by customer;\n\n\n3 records\n\n\ncustomer\ntotal_amount\n\n\n\n\nB\n74\n\n\nC\n36\n\n\nA\n76\n\n\n\n\n\n\n\n2. How many days has each customer visited the restaurant?\n\n\nselect customer_id,\n   count(\n      distinct extract(\n         day\n         from order_date\n      )\n   ) as no_of_days_visited\nfrom sales\ngroup by customer_id\norder by customer_id;\n\n\n3 records\n\n\ncustomer_id\nno_of_days_visited\n\n\n\n\nA\n4\n\n\nB\n5\n\n\nC\n2\n\n\n\n\n\nThis SQL query calculates the number of days each customer has visited a restaurant by grouping the sales transactions by customer ID, extracting the day from the order date, counting the distinct days, and ordering the results by customer ID.\n\n\n3. What was the first item from the menu purchased by each customer?\n\n\nwith cte as (\n   select customer_id,\n      order_date,\n      row_number() over (\n         partition by customer_id\n         order by order_date\n      ) as order_rank,\n      product_id\n   from sales\n)\nselect c.customer_id,\n   c.order_date,\n   c.order_rank,\n   m.product_name\nfrom cte c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\norder_date\norder_rank\nproduct_name\n\n\n\n\nA\n2021-01-01\n1\nsushi\n\n\nB\n2021-01-01\n1\ncurry\n\n\nC\n2021-01-01\n1\nramen\n\n\n\n\n\nWithin the CTE, the row_number() function is employed to assign a ranking to each order for each customer based on the order_date. The ranking starts from 1, indicating the first order for each customer. The main query then selects the customer ID, order date, order rank, and product name from the CTE, joining the menu table to retrieve the product name corresponding to the product ID. Finally, it filters the results to include only records with an order rank of 1, effectively selecting the first item purchased for each customer.\n\n\n4. What is the most purchased item on the menu and how many times was it purchased by all customers?\n\n\nwith cte as(\n   select product_id,\n      count(product_id)\n   from sales\n   group by product_id\n   order by count(product_id) desc\n   limit 1\n) -- Most purchased item on the menu is the product with the id 3 which is ramen, according to cte\nselect customer_id,\n   count(product_id) as count_of_most_purchased_product\nfrom sales\nwhere product_id in (\n      select product_id\n      from cte\n   )\ngroup by customer_id;\n\n\n3 records\n\n\ncustomer_id\ncount_of_most_purchased_product\n\n\n\n\nA\n3\n\n\nB\n2\n\n\nC\n3\n\n\n\n\n\nThe query utilizes a common table expression (CTE) named cte to generate a temporary result set. Within the CTE, the count() function is employed to count the number of times each product ID appears in the sales table. The results are then sorted in descending order based on the product count. The limit 1 clause restricts the output to the top row, effectively identifying the product ID with the highest count.\nThe main query then selects the customer ID and the count of the most purchased product for each customer. It filters the sales table to include only records where the product_id matches the one identified in the CTE, effectively focusing on the most purchased item. Finally, it groups the results by customer_id to determine how many times each customer purchased the most popular item.\n\n\n5. Which item was the most popular for each customer?\n\n\nwith cte_1 as (\n   select customer_id,\n      product_id,\n      count(product_id) as count_of_item\n   from sales\n   group by customer_id,\n      product_id\n),\ncte_2 as (\n   select *,\n      row_number() over (\n         partition by customer_id\n         order by count_of_item desc\n      ) as order_rank\n   from cte_1\n)\nselect c.customer_id,\n   m.product_name,\n   c.count_of_item\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n3 records\n\n\ncustomer_id\nproduct_name\ncount_of_item\n\n\n\n\nC\nramen\n3\n\n\nB\nramen\n2\n\n\nA\nramen\n3\n\n\n\n\n\nThis query aims to identify the most popular item for each customer. The query utilizes two common table expressions to process the data and generate the desired output. The first CTE, named cte_1, calculates the count of each product purchased by each customer. It groups the rows in the sales table by customer_id and product_id, and then counts the occurrences of each product ID for each customer. This step determines the frequency of each product purchase for each customer.\nThe second CTE, named cte_2, assigns a ranking to each product for each customer based on the purchase frequency calculated in cte_1. It uses the row_number() function and partitions the data by customer_id, sorting within each partition by the count_of_item in descending order. This step effectively identifies the product with the highest purchase frequency (i.e., the most popular item) for each customer.\nThe main query then selects the customer ID, product name, and purchase count for each customer’s most popular item. It joins the menu table to obtain the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the most popular item for each customer is selected.\n\n\n6. Which item was purchased first by the customer after they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date\n      ) as order_rank\n   from cte_1\n   where order_date &gt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nramen\n2021-01-10\n1\n\n\nB\nsushi\n2021-01-11\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date, ensuring a chronological order of transactions. The second CTE, named cte_2, focuses on purchases made after each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is later than the join_date (membership start date).\nAdditionally, it assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date. This identifies the first purchase (order_rank = 1) made by each customer after becoming a member. The main query then selects the customer ID, product name, order date, and order rank for each customer’s first purchase after becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the first purchase is selected.\n\n\n7. Which item was purchased just before the customer became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n   order by order_date\n),\ncte_2 as (\n   select customer_id,\n      product_id,\n      join_date,\n      order_date,\n      row_number() over(\n         partition by customer_id\n         order by order_date desc\n      ) as order_rank\n   from cte_1\n   where order_date &lt; join_date\n)\nselect c.customer_id,\n   m.product_name,\n   c.order_date,\n   c.order_rank\nfrom cte_2 c\n   natural join menu m\nwhere order_rank = 1;\n\n\n2 records\n\n\ncustomer_id\nproduct_name\norder_date\norder_rank\n\n\n\n\nA\nsushi\n2021-01-01\n1\n\n\nB\nsushi\n2021-01-04\n1\n\n\n\n\n\nThe first CTE, named cte_1, combines the members and sales tables, and sorts the combined data by order_date in ascending order, ensuring a chronological sequence of transactions.\nThe second CTE, cte_2, focuses on purchases made before each customer’s membership start date. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date).\nIt assigns an order_rank to each purchase for each customer using the row_number() function. The ranking is partitioned by customer_id and sorted within each partition by order_date in descending order. This identifies the last purchase (order_rank = 1) made by each customer before becoming a member.\nThe main query then selects the customer ID, product name, order date, and order rank for each customer’s last purchase before becoming a member. It joins the menu table to retrieve the corresponding product names and filters the results to include only records with an order_rank of 1, ensuring that only the last purchase is selected.\n\n\n8. What is the total items and amount spent for each member before they became a member?\n\n\nwith cte_1 as (\n   select *\n   from members\n      natural join sales\n      natural join menu\n   order by order_date\n)\nselect customer_id,\n   count(distinct product_id) as count_of_products,\n   sum(price) as total_amount_spent\nfrom cte_1\nwhere order_date &lt; join_date\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ncount_of_products\ntotal_amount_spent\n\n\n\n\nA\n2\n25\n\n\nB\n2\n40\n\n\n\n\n\nThe query utilizes a CTE named cte_1 to prepare the data and simplifies the aggregation in the main query.\nThe main query then aggregates the data for each customer based on their membership status. It filters the cte_1 data to include only records where the order_date is earlier than the join_date (membership start date). For each customer, it counts the distinct product_id values to determine the total number of unique items purchased and calculates the sum of price values to determine the total amount spent. The results are grouped by customer_id to provide individual summaries for each member.\n\n\n9. If each $1 spent equates to 10 points and sushi has a 2x points multiplier - how many points would each customer have?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points\n   from members\n      natural join sales\n      natural join menu\n)\nselect customer_id,\n   sum(points) as total_points\nfrom cte\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n860\n\n\nB\n940\n\n\n\n\n\nThe CTE combines the members, sales, and menu tables, providing a comprehensive view of customer memberships, their purchases, and the corresponding product names.\nThe main query then summarizes the points earned for each customer. It groups the data from the CTE by customer_id and calculates the sum of points values for each group, effectively determining the total points earned by each customer.\n\n\n10. In the first week after a customer joins the program (including their join date) they earn 2x points on all items, – not just sushi - how many points do customer A and B have at the end of January?\n\n\nwith cte as (\n   select *,\n      case\n         when product_name = 'sushi' then price * 10 * 2\n         else price * 10\n      end points,\n      case\n         when order_date - join_date &lt;= 7 then 2\n         else 1\n      end multiplier\n   from members\n      natural join sales\n      natural join menu\n),\ncte_2 as (\n   select *,\n      points * multiplier as total_points\n   from cte\n)\nselect customer_id,\n   sum(total_points) as total_points\nfrom cte_2\ngroup by customer_id;\n\n\n2 records\n\n\ncustomer_id\ntotal_points\n\n\n\n\nA\n1720\n\n\nB\n1760\n\n\n\n\n\nThe first CTE, named cte, joins the members, sales, and menu tables, and calculates the points earned for each purchase using a conditional CASE expression, similar to the previous query.\nAdditionally, it assigns a multiplier to each purchase based on whether it falls within the first week after the customer’s join date. For purchases within the first week, the multiplier is 2 (double points); for purchases outside the first week, the multiplier is 1 (standard points).\nThe second CTE, named cte_2, simplifies the calculation by multiplying the points and multiplier columns for each purchase, effectively determining the total points earned per transaction. The main query then summarizes the points earned for each customer, including the double points accrued during the first week. It groups the data from cte_2 by customer_id and calculates the sum of total_points values for each group, providing the total points earned by customer A and customer B at the end of January\n\n\n11. Recreate the following table output using the available data:\n\n\nselect s.customer_id,\n    s.order_date,\n    men.product_name,\n    men.price,\n    CASE\n        WHEN s.order_date &gt;= m.join_date THEN 'Y'\n        ELSE 'N'\n    END\nfrom sales s\n    LEFT JOIN menu men ON s.product_id = men.product_id\n    LEFT JOIN members m on m.customer_id = s.customer_id\nORDER BY s.customer_id,\n    s.order_date;\n\n\nDisplaying records 1 - 10\n\n\ncustomer_id\norder_date\nproduct_name\nprice\ncase\n\n\n\n\nA\n2021-01-01\nsushi\n10\nN\n\n\nA\n2021-01-01\ncurry\n15\nN\n\n\nA\n2021-01-07\ncurry\n15\nY\n\n\nA\n2021-01-10\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nA\n2021-01-11\nramen\n12\nY\n\n\nB\n2021-01-01\ncurry\n15\nN\n\n\nB\n2021-01-02\ncurry\n15\nN\n\n\nB\n2021-01-04\nsushi\n10\nN\n\n\nB\n2021-01-11\nsushi\n10\nY"
  },
  {
    "objectID": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#closing-the-connection",
    "href": "posts/2023-11-18_8weeks-sql-challenge-1/index.html#closing-the-connection",
    "title": "Danny’s Diner",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html",
    "href": "posts/2023-11-18_sleep-disorders/index.html",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "",
    "text": "The goal is to build a casual model using linear regression that explains quality of sleep, and logistic regression to explain sleep disorders\n\nlibrary(tidyverse)\n\n# descriptives\nlibrary(datawizard)\nlibrary(kableExtra)\nlibrary(skimr)\nlibrary(qqplotr)\nlibrary(gt)\n\n# os\nlibrary(here)"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#data-cleaning",
    "href": "posts/2023-11-18_sleep-disorders/index.html#data-cleaning",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Data cleaning",
    "text": "Data cleaning\nI observed the following issues in the data\n\nOccupation variable having levels with very few counts. Those with few counts can be consolidated with similar categories.\nThe BMI category variable has both “Normal” and “Normal Weight” as levels. This is most likely as data entry error.\nThe heart rate variable is not in a suitable format to work with. I’ll extract the systolic blood pressure from the variable as an integer. I would not need the diastolic blood pressure as I observed strong multicollinearity between both diastolic and systolic blood pressure.\n\n\n# observe occupations with low frequency counts\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# observe data entry error in bmi_category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# regrouping occupation variable so that occupations with lower counts are consolidated with other similar occupations. Simultaneously correcting data entry error in bmi_category variable\nsleep &lt;- sleep %&gt;% mutate( \n  occupation=ifelse(sleep$occupation %in% c(\"Software Engineer\", \"Scientist\", \"Engineer\"),\"Technical\", \nifelse(sleep$occupation %in% c(\"Salesperson\", \"Sales Representative\", \"Manager\"), \"Sales\", ifelse(sleep$occupation %in% c(\"Doctor\", \"Nurse\"), \"Medical\",\n                                                          as.character(occupation)))),\nbmi_category = ifelse(sleep$bmi_category %in% c(\"Normal\", \"Normal Weight\"), \"Normal\", as.character(sleep$bmi_category)))\n\n\n# extracting systolic blood pressure from the blood_pressure variable as it such a numeric variable is easier to work with. Subsequently dropping hear_rate variable\n\nsleep &lt;- sleep %&gt;% mutate(\nbp_sys= parse_number(str_sub(sleep$blood_pressure, 1, 3)),\nperson_id = as.character(person_id)\n) %&gt;% select(!heart_rate)\n\n\n# confirming changes\nhead(sleep)  %&gt;% gt() %&gt;% opt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      person_id\n      gender\n      age\n      occupation\n      sleep_duration\n      quality_of_sleep\n      physical_activity_level\n      stress_level\n      bmi_category\n      blood_pressure\n      daily_steps\n      sleep_disorder\n      bp_sys\n    \n  \n  \n    1\nMale\n27\nTechnical\n6.1\n6\n42\n6\nOverweight\n126/83\n4200\nNone\n126\n    2\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    3\nMale\n28\nMedical\n6.2\n6\n60\n8\nNormal\n125/80\n10000\nNone\n125\n    4\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    5\nMale\n28\nSales\n5.9\n4\n30\n8\nObese\n140/90\n3000\nSleep Apnea\n140\n    6\nMale\n28\nTechnical\n5.9\n4\n30\n8\nObese\n140/90\n3000\nInsomnia\n140\n  \n  \n  \n\n\n\n\n\n# confirming changes to BMI category variable\nggplot(sleep, aes(bmi_category))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()\n\n\n\n\n\n# confirming changes to occupation variable\nggplot(sleep, aes(occupation))+geom_bar(fill=\"#25d366\", color=\"black\") + coord_flip()"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#associations",
    "href": "posts/2023-11-18_sleep-disorders/index.html#associations",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Associations",
    "text": "Associations\n\nResearch Question: “What strong associations exists between quality_of_sleep and other variables in the dataset?\n\n# | warning: false\n# | message: false\n# packages required for associations\nlibrary(correlation)\nlibrary(GGally)\n\n\n# plotting the grid plot of scatterplots of numeric variables\nggscatmat(sleep %&gt;% select_if(is.numeric)) + theme_light()\n\n\n\n\n\n# showing relationships with strong correlations (greater than |+-0.4|)\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation() %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs)) %&gt;% filter(abs(r) &gt; 0.4) %&gt;% arrange(r) \ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.899\n0.000\n    sleep_duration\nstress_level\n−0.811\n0.000\n    age\nstress_level\n−0.422\n0.000\n    age\nquality_of_sleep\n0.474\n0.000\n    age\nbp_sys\n0.606\n0.000\n    physical_activity_level\ndaily_steps\n0.773\n0.000\n    sleep_duration\nquality_of_sleep\n0.883\n0.000\n  \n  \n  \n\n\n\n\n\n# robust correlation analysis since the data are skewed. Results is filtered to include only relationships involving quality_of_sleep. Sorted ascendingly by p-value\ncorrelates_tib &lt;- sleep |&gt; \n  select(where(is.numeric)) |&gt; \n  correlation(method=\"percentage\") %&gt;% as.tibble() %&gt;% select(!c(CI:df_error,Method,n_Obs))  %&gt;% arrange(p)  %&gt;% filter( Parameter1 == \"quality_of_sleep\" | Parameter2 == \"quality_of_sleep\") %&gt;% arrange(p) %&gt;% filter(p &lt; 0.05)\ncorrelates_tib %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Parameter1\n      Parameter2\n      r\n      p\n    \n  \n  \n    quality_of_sleep\nstress_level\n−0.910\n0.000\n    sleep_duration\nquality_of_sleep\n0.892\n0.000\n    age\nquality_of_sleep\n0.432\n0.000\n    quality_of_sleep\nphysical_activity_level\n0.178\n0.005\n  \n  \n  \n\n\n\n\n\nResults\nThe most significant relationships (at alpha = .05) are the relationships between:\n\nquality of sleep and stress level, r(372) =-0.91, p &lt; 0.0001\nquality of sleep and sleep duration, r(372) = 0.89, p &lt; 0.0001\nquality of sleep and age, r(372) =0.43, p &lt; 0.0001\nquality of sleep and physical activity level, r(372) =0.18, p = 0.0054\n\nThese are the variables we would fit to our linear regression model."
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#research-questions",
    "href": "posts/2023-11-18_sleep-disorders/index.html#research-questions",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research questions:",
    "text": "Research questions:\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\nIs the relationship between stress level and quality of sleep moderated by occupation?\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\n\n\nIs the relationship between sleep duration and quality of sleep moderated by sleep disorder?\n\n# Visualizing the relationship of the three variables\nggplot(sleep, aes(x = sleep_duration, y = quality_of_sleep, color = sleep_disorder)) +\ngeom_jitter(size=1, alpha=0.8) +\nlabs(x = \"Sleep duration\", y = \"Quality of sleep\", color = \"Sleep disorder\") +\ngeom_smooth(method = \"lm\", se = FALSE, linewidth=0.9) + theme_bw()\n\n\n\n\nThe plot shows that there might be some moderation effects.\n\n# making sleep_disorder a factor variable and setting the base level to \"None\"\nsleep &lt;- sleep %&gt;% mutate(\n  sleep_disorder = as_factor(sleep_disorder) %&gt;% fct_relevel(\"None\")\n)\n\n\nqs_lm_03 &lt;- lm(quality_of_sleep ~ sleep_duration * sleep_disorder, data=sleep)\nqs_lm_03 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n-0.5750673\n0.36703899\n-1.566774\n1.180269e-01\n    sleep_duration\n1.1144522\n0.04963586\n22.452560\n6.124844e-71\n    sleep_disorderSleep Apnea\n-3.4341228\n0.57712441\n-5.950403\n6.233752e-09\n    sleep_disorderInsomnia\n-3.2040380\n1.11193745\n-2.881491\n4.190005e-03\n    sleep_duration:sleep_disorderSleep Apnea\n0.4802913\n0.08000178\n6.003508\n4.632039e-09\n    sleep_duration:sleep_disorderInsomnia\n0.4503706\n0.16657925\n2.703642\n7.176641e-03\n  \n  \n  \n\n\n\n\n\nInterpretation\n\n\n\n\n\n\nTip\n\n\n\nThe effect of increasing sleep duration in someone with no sleep disorder on sleep quality is 1.11\nThe effect of sleep apnea on sleep quality compared to someone with no disorders is -3.43\nThe effect of Insomnia on sleep quality compared to someone with no disorders is -3.20\nThe effect of increasing sleep duration in someone with sleep anea on sleep quality compared to someone with no disorders is 0.48\nThe effect of increasing sleep duration in someone with insomnia on sleep quality compared to someone with no disorders is 0.45\n\n\n\ninteractions::sim_slopes(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of sleep_duration when sleep_disorder = Insomnia: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.56   0.15   1.26    1.86    10.26   0.00\n\nSlope of sleep_duration when sleep_disorder = Sleep Apnea: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.59   0.06   1.48    1.71    27.35   0.00\n\nSlope of sleep_duration when sleep_disorder = None: \n\n  Est.   S.E.   2.5%   97.5%   t val.      p\n------ ------ ------ ------- -------- ------\n  1.11   0.03   1.05    1.17    36.50   0.00\n\n\nThe slopes for the three regressions are not significantly different\n\ninteractions::interact_plot(\n  qs_lm_03,\n  pred = sleep_duration,\n  modx = sleep_disorder,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Sleep disorder\"\n  )\n\n\n\n\nLooking at the slope, we can that the moderation effect is not significant.\n\n\nConclusion\nSleep disorder is not a moderator of the relationship between sleep duration and quality of sleep\n\n\n\nIs the relationship between stress level and quality of sleep moderated by occupation?\n\nqs_lm_04 &lt;- lm(quality_of_sleep ~ stress_level * occupation, data=sleep)\nqs_lm_04 %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n9.3661417\n0.22464236\n41.6935685\n2.847324e-140\n    stress_level\n-0.3208661\n0.04725953\n-6.7894487\n4.624546e-11\n    occupationLawyer\n3.3611310\n1.08304354\n3.1034126\n2.063488e-03\n    occupationMedical\n1.3637168\n0.24305144\n5.6108156\n4.008125e-08\n    occupationSales\n3.5481440\n1.02746628\n3.4532948\n6.193572e-04\n    occupationTeacher\n-0.2842770\n0.33263086\n-0.8546322\n3.933200e-01\n    occupationTechnical\n2.0623438\n0.25972133\n7.9406026\n2.559029e-14\n    stress_level:occupationLawyer\n-0.6336793\n0.21426067\n-2.9575158\n3.304916e-03\n    stress_level:occupationMedical\n-0.2851357\n0.04939422\n-5.7726532\n1.679151e-08\n    stress_level:occupationSales\n-0.6791339\n0.15058782\n-4.5098857\n8.775327e-06\n    stress_level:occupationTeacher\n-0.1447393\n0.07087144\n-2.0422799\n4.184795e-02\n    stress_level:occupationTechnical\n-0.4718021\n0.05572917\n-8.4659819\n6.415721e-16\n  \n  \n  \n\n\n\n\n\ninteractions::sim_slopes(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  jnplot = TRUE,\n  robust = TRUE,\n  confint = TRUE\n  )\n\nSIMPLE SLOPES ANALYSIS \n\nSlope of stress_level when occupation = Lawyer: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.95   0.03   -1.02   -0.89   -29.71   0.00\n\nSlope of stress_level when occupation = Accountant: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.32   0.04   -0.40   -0.24    -7.92   0.00\n\nSlope of stress_level when occupation = Teacher: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.47   0.09   -0.65   -0.28    -5.01   0.00\n\nSlope of stress_level when occupation = Sales: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -1.00   1.04   -3.04    1.04    -0.97   0.33\n\nSlope of stress_level when occupation = Medical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.61   0.01   -0.62   -0.59   -81.54   0.00\n\nSlope of stress_level when occupation = Technical: \n\n   Est.   S.E.    2.5%   97.5%   t val.      p\n------- ------ ------- ------- -------- ------\n  -0.79   0.07   -0.93   -0.65   -10.97   0.00\n\n\n\ninteractions::interact_plot(\n  qs_lm_04,\n  pred = stress_level,\n  modx = occupation,\n  interval = TRUE,\n  robust = TRUE,\n  legend.main = \"Occupation\"\n  )\n\n\n\n\nAgain, we do not observe a significant moderation effect of occupation on the relationship between stress level and quality of sleep\n\n\nIs the relationship between physical activity and quality of sleep mediated by daily steps?\nTo test this, we are going to run 3 regressions:\n\nThe total effect of physical_activity_level on quality_of_sleep.\nThe effect of physical_activity_level on quality_of_sleep that is mediated by daily_steps, a.k.a. the indirect effect\nThe effect of physical_activity_level on quality_of_sleep that is not mediated by daily_steps, a.k.a. the direct effect\n\n\nTotal effect\nWe first determine the total effect by running a regression of physical_activity_level on quality_of_sleep (without including daily_steps):\n\nlm(quality_of_sleep ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n6.657\n0.183\n36.307\n0.000\n    physical_activity_level\n0.011\n0.003\n3.792\n0.000\n  \n  \n  \n\n\n\n\nThe total effect is approximately 0.011, meaning that an increase of one unit in physical activity increases sleep quality by 0.001 units on average, not holding daily steps constant.\n\n\nMediated/Indirect Effect\nThe effect of physical_activity_level on quality_of_sleep mediated by daily_steps can be obtained by multiplying together the effect of physical_activity_level on daily_steps and the effect of daily_steps on quality_of_sleep.\n\n1. physical_activity_level on daily_steps (a)\n\nlm(daily_steps ~ physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n3265.57619\n160.321889\n20.36887\n1.718735e-62\n    physical_activity_level\n60.01692\n2.556092\n23.47995\n1.968256e-75\n  \n  \n  \n\n\n\n\nOne unit increase in physical_activity_level increases daily_steps by an average of 60 steps\n\n\n2. daily_steps on quality_of_sleep\n\nlm(quality_of_sleep ~ daily_steps + physical_activity_level, sleep) %&gt;% tidy() %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n7.4500815511\n2.609352e-01\n28.551459\n1.151887e-95\n    daily_steps\n-0.0002428667\n5.802065e-05\n-4.185867\n3.551189e-05\n    physical_activity_level\n0.0256601109\n4.506428e-03\n5.694113\n2.524969e-08\n  \n  \n  \n\n\n\n\n(a * b) = 60.07 * -0.00024 = -0.0144\n\n\n\nDirect Effect\nThis is simply the coefficient of physical_activity_level in the above regression = 0.0257\nAccording to Zhao et al., what we have here isCompetitive Mediation (Regularly Partial Mediation)\nIn the competitive partial mediation hypothesis, it is assumed that the intermediate variable (daily_steps) could sometimes increase and at times decrease the relationship between the independent and dependent variables. i.e an “inconsistent” model.\n\n\n\nDecision tree for determining mediation"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "href": "posts/2023-11-18_sleep-disorders/index.html#research-question-are-significant-differences-in-the-mean-sleep-quality-of-both-male-and-female-participants",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Research question: are significant differences in the mean sleep quality of both male and female participants",
    "text": "Research question: are significant differences in the mean sleep quality of both male and female participants\n\n# summary statistics of quality_of_sleep by gender\n\nby(cbind(data=sleep$quality_of_sleep), sleep$gender, psych::describe)\n\nINDICES: Female\n     vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 185 7.66 1.28      8    7.76 1.48   4   9     5 -0.49    -0.83 0.09\n------------------------------------------------------------ \nINDICES: Male\n     vars   n mean sd median trimmed  mad min max range  skew kurtosis   se\ndata    1 189 6.97  1      7    7.01 1.48   4   9     5 -0.35    -0.59 0.07\n\n\nThe mean of female participants is one point higher than males. But is this difference significant?\n\n# plotting a violin-errorbar plot to visualise the relationship\nggplot(sleep, aes(gender,quality_of_sleep))+\ngeom_violin() +\nstat_summary(fun.data=\"mean_cl_normal\") + \ntheme_minimal()\n\n\n\n\nSince the errorbars do not overlap, we can be fairly confident that the difference is significant. Lets confirm this hunch with a Welsh’s t-test.\n\n t.test(quality_of_sleep ~ gender,\n                    data = sleep,\n                    paired = FALSE,\n                    var.equal = FALSE,\n                    conf.level = 0.95,\n                    na.action = na.exclude)\n\n\n    Welch Two Sample t-test\n\ndata:  quality_of_sleep by gender\nt = 5.8593, df = 347.96, p-value = 1.078e-08\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n 0.4627786 0.9304432\nsample estimates:\nmean in group Female   mean in group Male \n            7.664865             6.968254 \n\n\nBecause the p-value = 1.078e-08 is less than our alpha of .05, we can conclude that the difference between both genders is significant. Also, since the confidence interval expressing the true difference in means does not cross zero, we can be confident that there exists a true difference, with a chance of us being wrong 5% of the time.\n\neffectsize::cohens_d(quality_of_sleep ~ gender, data = sleep) %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\") %&gt;% fmt_number(decimals=3)\n\n\n\n\n\n  \n    \n    \n      Cohens_d\n      CI\n      CI_low\n      CI_high\n    \n  \n  \n    0.608\n0.950\n0.400\n0.815\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOn average, female participants have better sleep quality (M = 7.66, SE = 0.09), than those not given a cloak (M = 6.97, SE = 0.07). Sleep quality is significantly different for both genders: the mean difference, M = 0.69, 95% CI [0.46, 0.93], was significantly different from 0, t(347.96) = 5.85, p = 01.078e-08. This effect was quite large, d= 0.61[0.4, 0.82]"
  },
  {
    "objectID": "posts/2023-11-18_sleep-disorders/index.html#conclusion-1",
    "href": "posts/2023-11-18_sleep-disorders/index.html#conclusion-1",
    "title": "Exploring Relationships of Variables in Sleep Data",
    "section": "Conclusion",
    "text": "Conclusion\nI created this post as a way of solidifying my understanding of these concepts. I find that documenting the material helps it to stick iwth me better. I hope that you find this helpful. My deepest gratitude goes to Andy Field for his exceptional book on statistical analysis with R."
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#introduction",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#introduction",
    "title": "Foodie Fi",
    "section": "",
    "text": "Using subscription style digital data to answer important business questions."
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#entity-relationship-diagram",
    "title": "Foodie Fi",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#database-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#database-connection",
    "title": "Foodie Fi",
    "section": "Database Connection",
    "text": "Database Connection\nFirst, I’ll create a connection to my local postgres database thanks to the RPostgres package.\n\n# | warning: false\n# Creating a connection to my local postgres database\nlibrary(RPostgres)\ncon &lt;-\n  dbConnect(Postgres(),\n            dbname = \"foodie_fi\",\n            user = \"postgres\",\n            password = my_password)"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#queries",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#queries",
    "title": "Foodie Fi",
    "section": "Queries",
    "text": "Queries\n\n1. How many customers has Foodie-Fi ever had?\n\n\nSELECT count(DISTINCT customer_id)\nFROM subscriptions;\n\n\n1 records\n\n\ncount\n\n\n\n\n1000\n\n\n\n\n\n\n\n3. What is the monthly distribution of trial plan start_date values for our dataset use the start of the month as the group by value?\n\nSELECT upper(to_char(start_date, 'month')) as start_month,\n    count(*) frequency\nfrom subscriptions\nwhere plan_id = 0\ngroup by 1\nORDER BY 2 desc;\n\n\nDisplaying records 1 - 10\n\n\nstart_month\nfrequency\n\n\n\n\nMARCH\n94\n\n\nJULY\n89\n\n\nAUGUST\n88\n\n\nMAY\n88\n\n\nJANUARY\n88\n\n\nSEPTEMBER\n87\n\n\nDECEMBER\n84\n\n\nAPRIL\n81\n\n\nJUNE\n79\n\n\nOCTOBER\n79\n\n\n\n\n\n\n\n3. What plan start_date values occur after the year 2020 for our dataset? Show the breakdown by count of events for each plan_name ?\n\n\nSELECT p.plan_name,\n    count(*) as count_of_events_after_2020\nfrom subscriptions s\n    natural join plans p\nwhere EXTRACT(\n        year\n        from start_date\n    ) &gt; 2020\ngroup by 1;\n\n\n4 records\n\n\nplan_name\ncount_of_events_after_2020\n\n\n\n\npro annual\n63\n\n\nchurn\n71\n\n\npro monthly\n60\n\n\nbasic monthly\n8\n\n\n\n\n\n\n\n4. What is the customer count and percentage of customers who have churned rounded to 1 decimal place?\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id = 4\n)\nSELECT cte1.whole as total_customers,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_churned\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ntotal_customers\npct_churned\n\n\n\n\n2650\n12\n\n\n\n\n\n\n\n5. How many customers have churned straight after their initial free trial? what percentage is this rounded to the nearest whole number?\n\nwith cte as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\ncte2 as (\n    -- getting rows whose values satisfy the condition in the question\n    select *\n    from cte\n    where plan_id = 0\n        and lead_plan_id = 4\n) -- solution\nselect count(*) as count_of_customers_who_churned_after_free_trial\nfrom cte2 \n\n\n1 records\n\n\ncount_of_customers_who_churned_after_free_trial\n\n\n\n\n92\n\n\n\n\n\n\n\n6. What is the number and percentage of customer plans after their initial free trial?\n\n\nwith cte1 as (\n    select 1 as id,\n        count(customer_id)::numeric as whole\n    from subscriptions\n),\ncte2 as (\n    select 1 as id,\n        count(customer_id)::numeric as part\n    from subscriptions\n    where plan_id &lt;&gt; 0\n)\nSELECT cte2.part as customer_count_after_trial_plan,\n    round(cte2.part / cte1.whole, 2) * 100 as pct_ccatp\nfrom cte1\n    natural join cte2;\n\n\n1 records\n\n\ncustomer_count_after_trial_plan\npct_ccatp\n\n\n\n\n1650\n62\n\n\n\n\n\n\n\n7. How many customers have upgraded to an annual plan in 2020?\n\n\nwith cte1 as (\n    -- using the lead window function to find the\n    -- preceding row to a particular row\n    select *,\n        lead(plan_id) over(partition by customer_id) as lead_plan_id\n    from subscriptions\n    order by customer_id,\n        plan_id\n),\n-- filtering to only annual plans\ncte2 as (\n    select *,\n        lead_plan_id - plan_id as diff\n    from cte1\n    where lead_plan_id = 3\n) -- excluding churned customers and unupgraded plans\nselect count(DISTINCT customer_id) as upgraded_customers_2020_count\nfrom cte2\nwhere (diff &gt; 0)\n    and (lead_plan_id &lt;&gt; 4)\n    and EXTRACT(\n        year\n        from start_date\n    ) = 2020;\n\n\n1 records\n\n\nupgraded_customers_2020_count\n\n\n\n\n253\n\n\n\n\n\n\n\n8. How many days on average does it take for a customer to upgrade to an annual plan from the – day they join Foodie-Fi?\n\nwith cte1 as (\n    select *,\n        max(plan_id) over (partition by customer_id) as highest_plan_suscribed,\n        max(start_date) over (partition by customer_id) as date_of_hps,\n        min(start_date) over (partition by customer_id) as date_of_lps,\n        row_number() over (partition by customer_id) as sn\n    from subscriptions\n    order by customer_id,\n        start_date,\n        plan_id\n),\ncte2 as(\n    select *,\n        date_of_hps - date_of_lps as diff_in_days\n    from cte1\n    where highest_plan_suscribed = 3\n        and sn = 1\n)\nselect round(avg(diff_in_days)::numeric, 2) as avg_days_to_upgrade_to_annual\nfrom cte2\n\n\n1 records\n\n\navg_days_to_upgrade_to_annual\n\n\n\n\n105.95\n\n\n\n\n\n\n\n9. How many customers downgraded from a pro monthly to a basic monthly plan in 2020?\n\nwith cte1 as(\n        select *,\n            lead(plan_id) over(partition by customer_id) as lead_plan_id\n        from subscriptions\n        order by customer_id,\n            start_date,\n            plan_id\n    ),\n    cte2 as (\n        select customer_id,\n            plan_id,\n            lead_plan_id,\n            start_date\n            from cte1\n        where plan_id = 2\n            and lead_plan_id = 1\n            and EXTRACT(\n                year\n                from start_date\n            ) = 2020\n    )\nselect count(*) as number_of_customers_downgrade_from_prom_basm\nfrom cte2\n\n\n1 records\n\n\nnumber_of_customers_downgrade_from_prom_basm\n\n\n\n\n0"
  },
  {
    "objectID": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#closing-the-connection",
    "href": "posts/2023-11-19_8weeks-sql-challenge-3/index.html#closing-the-connection",
    "title": "Foodie Fi",
    "section": "Closing the connection",
    "text": "Closing the connection\n\ndbDisconnect(con)"
  },
  {
    "objectID": "posts_index.html",
    "href": "posts_index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nThe Causal-Behavioral Framework for Data Analysis\n\n\nIntroductory part to a series on Florent Buisson’s book, Behavioral Data Analysis with R & Python\n\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPizza Runner\n\n\nSecond part of a series of SQL case studies where I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoodie Fi\n\n\nThird part of a series of SQL case studies\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Bank\n\n\nFourth part of a series of SQL case studies… more CTEs!\n\n\n\nNov 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDanny’s Diner\n\n\nIn this first part of a series of SQL case studies, I use Postgres SQL to answer a bunch of business questions\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Relationships of Variables in Sleep Data\n\n\nIn this post, I use regression, with an emphasis on understanding the casual relationships between variables\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts_index.html#recent-posts-posts-recentes",
    "href": "posts_index.html#recent-posts-posts-recentes",
    "title": "Chamber of (data) secrets",
    "section": "",
    "text": "1 min\n\n\n\n\n\n\n  \n\n\n\n\n\nSecond part of a series of SQL case studies where I use Postgres SQL to answer a bunch of business questions\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 19, 2023\n\n\nEdun Joshua\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\n\nThird part of a series of SQL case studies\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 19, 2023\n\n\nEdun Joshua\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\n\nFourth part of a series of SQL case studies… more CTEs!\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 19, 2023\n\n\nEdun Joshua\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\n\nIn this first part of a series of SQL case studies, I use Postgres SQL to answer a bunch of business questions\n\n\n\n\nsql\n\n\n \n\n\n\n\nNov 18, 2023\n\n\nEdun Joshua\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\n\nIn this post, I use regression, with an emphasis on understanding the casual relationships between variables\n\n\n\n\nstats\n\n\ncausal regression modelling\n\n\n \n\n\n\n\nNov 18, 2023\n\n\nEdun Joshua\n\n\n16 min\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all/Ver todos"
  },
  {
    "objectID": "posts_index.html#recent-posts",
    "href": "posts_index.html#recent-posts",
    "title": "Chamber of (data) secrets",
    "section": "",
    "text": "See all"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html",
    "href": "posts/2023-11-30_BDA-1/index.html",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "",
    "text": "Please Note: The ideas presented in this article are not my own original thoughts. This material is merely a condensed version of my study notes based on Florent Buisson’s excellent book. Due to the need for brevity, certain nuances of these concepts have been omitted. For a more comprehensive understanding, please refer to the original source.\nPredictive analytics is like having a super cool fortune teller 🔮who can predict what’s going to happen next. But causal analytics is like being a detective 🕵️‍♀️🕵️‍♂️, trying to figure out why things happen in the first place 🤔. In predictive analytics, we just want to find a bunch of clues 🕵️‍♀️🕵️‍♂️ that can help us guess what’s going to happen next, even if those clues aren’t actually causing it to happen. But in causal analytics, we’re not just interested in guessing 🔮, we want to understand the real reasons behind things 🧠.\nImagine you want to know if eating more fruits and veggies 🍎🥦 will make you healthier 💪. Predictive analytics might tell you that people who eat more fruits and veggies tend to be healthier, but that doesn’t mean the fruits and veggies are actually causing the better health. It could be that healthier people just tend to make healthier choices overall 🏃‍♀️🚴🥗.\nSo, how do we really know if eating more fruits and veggies is making us healthier? That’s where causal analytics comes in 🕵️‍♀️🕵️‍♂️. We need to carefully choose the clues we use to solve the mystery, making sure they’re not just guessing 🔮but actually helping us understand the real reasons behind things 🧠.\ndf1 &lt;- read_csv(\"chap1-stand_data.csv\")\nhead(df1) %&gt;% gt() %&gt;%\nopt_stylize(style=6, color=\"blue\")\n\n\n\n\n\n  \n    \n    \n      icecream_sales\n      iced_coffee_sales\n      summer_months\n      temps\n    \n  \n  \n    25649.78\n28592.18\n0\n28.59314\n    32694.93\n37152.84\n0\n37.13064\n    26467.80\n24074.19\n0\n24.11190\n    43438.79\n49169.47\n0\n49.21807\n    52452.63\n47249.40\n0\n47.25766\n    34130.66\n34228.50\n0\n34.25746\nImagine you’re an ice cream 🍦 seller and you want to know if hotter days ☀️mean more ice cream sales 🤑. You track the temperature and your sales for a while and find that for every degree the temperature rises, your sales go up by $1,171!\nsummary(lm(\"icecream_sales ~ temps\",data=df1))\n\n\nCall:\nlm(formula = \"icecream_sales ~ temps\", data = df1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-30272  -5197   -494   3787  37430 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -6169.844    531.506  -11.61   &lt;2e-16 ***\ntemps        1171.335      9.027  129.76   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8730 on 2398 degrees of freedom\nMultiple R-squared:  0.8753,    Adjusted R-squared:  0.8753 \nF-statistic: 1.684e+04 on 1 and 2398 DF,  p-value: &lt; 2.2e-16\nYou’re excited and think you’ve found the perfect formula for success!🤑 But then something strange happens. October comes around and it’s a scorcher 🥵, but your sales actually drop! 😨 What gives?\nAfter some sleuthing, you realize that the culprit is school 🏫. In the summer, kids are out of school and have more time to enjoy ice cream 🍦. But in October, they’re back in class and have less time for frozen treats.\nSo, even though the temperature was high, your sales dropped because of the school year. This is called a “confounder” 😵. A confounder is something that can make it look like something else is causing a (third) something when it’s actually not.\nIn this case, the school year made it look like the temperature was causing sales to drop, when it was really the school year all along 🏫. So, next time you’re trying to figure out what’s causing something, be sure to watch out for confounders! 👀👀\nRemember that time when we added a bunch of random ingredients to our recipe and ended up with a weird-tasting soup? It’s the same with causal modelling.\nTo be fair, if your goal is only to predict a variable, you have a model that is carefully designed to generalize adequately beyond your testing data, and you don’t care about why the predicted variable is taking a certain value, then that’s a perfectly valid stance.\nLet’s demonstrate this with our example by adding a variable that we might be inclined to include but will bias our regression. The variable IcedCoffeeSales is correlated with Temperature but not with SummerMonth. Let’s look at what happens to our regression if we add this variable in addition to Temperature and SummerMonth (a binary 1/0 variable that indicates if the month was July or August (1) or any other month (0)):\nsummary(lm(icecream_sales ~ iced_coffee_sales + temps + summer_months, data=df1))\n\n\nCall:\nlm(formula = icecream_sales ~ iced_coffee_sales + temps + summer_months, \n    data = df1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25763.1  -3364.4     78.6   3317.5  27512.0 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         -15.827    374.581  -0.042    0.966    \niced_coffee_sales    -1.701      2.083  -0.817    0.414    \ntemps              2702.788   2083.161   1.297    0.195    \nsummer_months     19548.168    361.572  54.064   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5859 on 2396 degrees of freedom\nMultiple R-squared:  0.9439,    Adjusted R-squared:  0.9438 \nF-statistic: 1.344e+04 on 3 and 2396 DF,  p-value: &lt; 2.2e-16\nOnly summer_months has a statistically significant influence on icecream_sales . The other variable are not good predictors of the target variable and only confound the model.\nImagine you’re running an ice cream stand 🍦🍨 and you want to know if people who like chocolate ice cream 🍫are more likely to buy from your shop than people who like vanilla ice. You ask a bunch of people how much they like chocolate and vanilla ice cream (on a scale of 1 to 10, of course), and whether they’ve ever bought from your shop. You find out that some people like one but not the other, some like both equally, and some like one more than the other.\nChecking the association between our variables we discover 2 things:\nAll good so far! 👍 If we want to understand how both vanilla and chocolate ice cream preferences affect whether someone is a customer or not, then it makes sense to include both variables in our logistic regression model. 📈 But what if we want to know something different? What if we want to know how much the people who like vanilla ice cream also like chocolate ice cream? 🍦🍫 In other words, we’re interested in the relationship between how much someone likes vanilla ice cream and how much they like chocolate ice cream, but only for people who have actually bought from the ice cream stand. 🛍️ To do this, we can plot a graph of how much customers who have bought ice cream like chocolate ice cream, based on how much they like vanilla ice cream.\nHmm, something’s not right! 🤨 When we looked at the relationship between how much vanilla ice cream customers like and how much chocolate ice cream they like, we found a weird pattern: people who like one flavor tend to dislike the other. 🤯 But don’t worry, vanilla lovers aren’t suddenly becoming chocolate haters, and vice versa! 👎 This strange correlation was actually caused by the way we looked at the data.\nWe only looked at people who had bought ice cream from the stand, and this left out a lot of people who might have liked both flavours but didn’t buy any ice cream. If someone has a weak taste for both vanilla and chocolate ice creams, they are most likely not shoppers. if someone has a strong taste for vanilla, they might shop at your stand even if they don’t have a strong taste for chocolate. However, since you excluded non-shoppers, you removed a bunch of data points that should exist in the lower left quadrant of the scatterplot. If they were included, a positive relationship would be observed.\nSo, next time you’re looking at data, be sure to consider all of the data points, not just the ones that fit your expectations.😉\nUp next, chapter 2!"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#entity-relationship-diagram",
    "href": "posts/2023-11-30_BDA-1/index.html#entity-relationship-diagram",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Entity Relationship Diagram",
    "text": "Entity Relationship Diagram"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#causal-analytics-unraveling-the-mystery-behind-why-things-happen",
    "href": "posts/2023-11-30_BDA-1/index.html#causal-analytics-unraveling-the-mystery-behind-why-things-happen",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Causal Analytics: 🕵️‍♂️Unraveling the Mystery Behind Why Things Happen",
    "text": "Causal Analytics: 🕵️‍♂️Unraveling the Mystery Behind Why Things Happen"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#data",
    "href": "posts/2023-11-30_BDA-1/index.html#data",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "href": "posts/2023-11-30_BDA-1/index.html#why-correlation-is-not-causation-a-confounder-in-action",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Why Correlation Is Not Causation: A Confounder in Action",
    "text": "Why Correlation Is Not Causation: A Confounder in Action"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#too-many-variables-can-spoil-the-broth",
    "href": "posts/2023-11-30_BDA-1/index.html#too-many-variables-can-spoil-the-broth",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Too Many Variables Can Spoil the Broth",
    "text": "Too Many Variables Can Spoil the Broth"
  },
  {
    "objectID": "posts/2023-11-30_BDA-1/index.html#another-example",
    "href": "posts/2023-11-30_BDA-1/index.html#another-example",
    "title": "The Causal-Behavioral Framework for Data Analysis",
    "section": "Another example",
    "text": "Another example"
  }
]