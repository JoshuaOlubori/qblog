{
  "hash": "d0872d24188477c5624c73d8388c849f",
  "result": {
    "markdown": "---\ntitle: \"Developing a Modern Data Pipeline — Part 2\"\nsubtitle: \"A 3-part series featuring web-scraping, data pipelines, orchestration and containerisation\"\nauthor: \"Edun Joshua\"\ndate: \"2024-03-05\"\ncategories: [data engineering, api]\nimage: \"image.jpg\"\neval: false\n---\n\n::: {style=\"color: #25D366;\"}\n## Context\n:::\n\nIn[ Part 1](https://joshuaolubori.onrender.com/posts/2024-03-05_yusuf_app_1), we explored web scraping. Now, we switch to an API for the data retrieval. The API I used is [API-Football](https://rapidapi.com/api-sports/api/api-football/details) at rapidapi.com. You’ll need a subscription to access this API. See [Part 3](https://joshuaolubori.onrender.com/posts/2024-03-05_yusuf_app_3) for the final implementation of the pipeline\n\n::: {style=\"color: #25D366;\"}\n## 1. Setting Up The Stage: Imports and Options\n:::\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport csv\nimport requests\nfrom requests.exceptions import RequestException\nfrom urllib3.exceptions import NewConnectionError, ConnectTimeoutError\nfrom time import sleep\nfrom config import api_key, api_endpoint\n```\n:::\n\n\nThis section imports necessary libraries:\n\n-   **os**: Provides functionalities for interacting with the operating system (creating folders).\n\n-   **csv**: Enables working with CSV files (writing data).\n\n-   **requests**: Facilitates making HTTP requests to the API.\n\n-   **time**: Used for introducing a delay between API calls.\n\nAdditionally, it imports the API key and endpoint from the `config` module.\n\nNext, the code defines two variables:\n\n-   league_ids: Contains a list of League IDs representing various football leagues the script will process.\n\n-   unique_league_ids: Employs set() to remove duplicates from the league_ids list, ensuring each league is processed only once.\n\n::: {style=\"color: #25D366;\"}\n## 2. Data Organization: Folders and Paths\n:::\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ntop_level_folder = \"data\"\nos.makedirs(top_level_folder, exist_ok=True)\n```\n:::\n\n\nThis section creates a top-level folder named “data” to store the downloaded CSV files. The `os.makedirs` function ensures the folder is created even if it doesn't exist, and the `exist_ok=True` argument prevents errors if the folder already exists.\n\n::: {style=\"color: #25D366;\"}\n## 3. API Interaction and Data Extraction\n:::\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nurl = api_endpoint\nheaders = {\n    \"X-RapidAPI-Key\": api_key,\n    \"X-RapidAPI-Host\": \"api-football-v1.p.rapidapi.com\"\n}\n\ntotal_calls = len(unique_league_ids)\ncurrent_call = 0\n\n\ndef fetch_data(chosen_season=\"2023\"):\n    global current_call\n    for league_id in unique_league_ids:\n        try:\n            querystring = {\"league\": str(league_id), \"season\": chosen_season}\n            response = requests.get(url, headers=headers, params=querystring)\n\n            # Check if the response is successful\n            response.raise_for_status()\n\n            data = response.json()['response']\n\n            # ... (data processing and CSV creation) ...\n\n            current_call += 1\n            print(f\"{csv_data[0]['country']} {csv_data[0]['league_name']} called: ({current_call}/{total_calls})\")\n            print(f\"\\nCSV file saved at: {csv_file_path}\")\n\n        except (NewConnectionError, ConnectTimeoutError) as e:\n            # ... (error handling) ...\n            return\n\n        except RequestException as e:\n            # ... (error handling) ...\n\n        except Exception as e:\n            # ... (error handling) ...\n\n        # Sleep for 3 seconds before the next API call\n        sleep(3)\n```\n:::\n\n\nThe core functionality resides within the fetch_data function. Let's explore its steps:\n\n1.  **Looping through League IDs**: The `for` loop iterates through each league ID in the `unique_league_ids` set.\n\n2.  **API Request**: It constructs the API request URL with headers containing the API key and sends a GET request using `requests.get`.\n\n3.  **Error Handling**: The code utilizes a `try...except` block to handle potential exceptions:\n\n    -   Connection errors (`NewConnectionError`, `ConnectTimeoutError`): Indicate issues with the internet connection. The script prints an error message and exits the program.\n\n    -   `RequestException`: Catches other request-related errors, printing an error message.\n\n    -   `Exception`: Handles any unexpected errors, printing an error message.\n\n4.  **Successful Response**: If the response is successful (`response.raise_for_status()`), the script proceeds:\n\n    -   Extracting Data: It parses the JSON response and extracts the relevant data (`data = response.json()['response']`).\n\n5.  **CSV Creation**: The script constructs the CSV file:\n   \n    -   Folder Creation: It checks for a subfolder within “data” named after the league and country, creating it if necessary using `os.makedirs`.\n\n    -   File Path: The CSV file path is constructed with the league name, season, and “.csv” extension.\n\n    -   Writing Data: The script opens the CSV file in write mode and uses `csv.DictWriter` to write the header row and data rows from the processed data (`csv_data`).\n\n\n6.  **Progress Tracking and Information**:\n\n    -   The script increments a counter (`current_call`) to track the API call progress.\n\n    -   It prints a message indicating the successful API call with league details and the total progress.\n\n    -   Finally, it prints the location where the CSV file is saved.\n\nIn the [final part](https://medium.com/@joshuaolubori/developing-a-modern-data-pipeline-part-3-d40dda115e8c) of this series, we’ll dockerise the application and integrate tools such as DuckDB, MinIO and Airflow for orchestration.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}