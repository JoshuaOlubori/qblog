{
  "hash": "ee52c9ee609e3b810efb32ef642f5817",
  "result": {
    "markdown": "---\ntitle: \"Developing a Modern Data Pipeline — Part 3\"\nsubtitle: \"A 3-part series featuring web-scraping, data pipelines, orchestration and containerisation\"\nauthor: \"Edun Joshua\"\ndate: \"2024-03-05\"\ncategories: [data engineering, api]\nimage: \"image.png\"\neval: false\n---\n\n::: {style=\"color: #25D366;\"}\n## Context\n:::\n\nThis third and final part of the project focuses on implementing and orchestrating the data pipeline. Here, we’ll delve into the code behind the scenes, exploring how Airflow DAGs orchestrate the various data processing steps, ensuring reliable execution and well-defined dependencies. We’ll also examine the data storage and transformation strategies, including the utilization of Minio for object storage and the Astro SDK for efficient in-database transformations within DuckDB.\n\nFinally, we’ll explore the user-friendly Streamlit frontend that allows users to interact with and visualize the processed football fixture data, gaining valuable insights from the analysis. By bringing these elements together, this part finalizes the series, transforming the initial concept into a robust and functional data analysis system.\n\nClick here for [Part 1](https://joshuaolubori.onrender/posts/2024-03-05_yusuf_app_1/) and [Part 2](https://joshuaolubori.onrender/posts/2024-03-05_yusuf_app_1/) of the series.\n\nThis project was inspired by the [Astronomer Quick Start Repo](https://github.com/astronomer/airflow-quickstart) for getting started with Apache Airflow.\n\nHere’s a breakdown of the chosen technologies:\n\n-   **Airflow:** Orchestrates and schedules the data pipeline tasks seamlessly.\n\n-   **Docker:** Encapsulates the application and its dependencies within lightweight containers, ensuring consistent execution across different environments.\n\n-   [**Astronomer**](https://www.astronomer.io/)**:** Simplifies Airflow management and deployment.\n\n-   [**MinIO**](https://min.io/)**:** Provides scalable and reliable object storage for data persistence.\n\n-   [**DuckDB**](https://duckdb.org/)**:** Acts as the embedded database for storing and querying the transformed data.\n\n-   **Streamlit:** Enables the creation of a user-friendly web application for data visualization and exploration.\n\n<div>\n\n[https://github.com/JoshuaOlubori/football-data-analysis](https://github.com/JoshuaOlubori/football-data-analysis?source=post_page-----d40dda115e8c--------------------------------)\n\n</div>\n\n::: {style=\"color: #25D366;\"}\n## 1. **System architecture**\n:::\n\n1.  **EXTRACT**\n\n-   **Football API Integration:** Similar to the previous iteration, we utilize Python to retrieve football fixture data from a designated API. The extracted data is stored locally in CSV format.\n\n**2. LOAD**\n\n-   **MinIO Object Storage:** MinIO, a highly scalable and reliable object storage solution similar to Amazon S3, acts as the central repository for the extracted data. The local CSV file is uploaded to MinIO for persistent storage.\n\n**3. TRANSFORM**\n\n-   **DuckDB Integration:** DuckDB, a powerful embedded database, serves as the in-memory storage for the data during the transformation process. The data is downloaded from MinIO and loaded into DuckDB for efficient filtering and manipulation.\n\n-   **Data Transformation:** Leveraging the Astro Python SDK, the data within DuckDB undergoes transformation according to specific criteria detailed in [part 2 of this series.](https://medium.com/@joshuaolubori/developing-a-modern-data-pipeline-part-2-52b73fbff70b)\n\n-   **Transformed Data Storage:** The transformed data is ultimately stored within DuckDB as a separate table, readily accessible for further analysis.\n\n**4. FRONTEND**\n\n-   **Streamlit Application:** A dedicated Streamlit container hosts a user-friendly web application. This application interacts with DuckDB, retrieving and visualizing the transformed data, providing valuable insights into the football fixtures.\n\n------------------------------------------------------------------------\n\n::: {style=\"color: #25D366;\"}\n## 2. **Implementation details**\n:::\n\n## **Initiating the DAG: `start.py`**\n\n-   `start.py` defines the initial Airflow DAG using the `@dag` decorator.\n\n-   It sets the DAG to run only once manually and defines default arguments and descriptive metadata.\n\n-   The code creates a dedicated “duckdb” pool with one worker slot using a `BashOperator`.\n\n-   This pool ensures controlled execution of tasks querying DuckDB.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n\"\"\"DAG that kicks off the pipeline by producing to the start dataset.\"\"\"\n\n# --------------- #\n# Package imports #\n# --------------- #\n\nfrom airflow import Dataset\nfrom airflow.decorators import dag\nfrom airflow.operators.bash import BashOperator\nfrom pendulum import datetime\n\n# -------------------- #\n# Local module imports #\n# -------------------- #\n\nfrom include.global_variables import global_variables as gv\n\n# --- #\n# DAG #\n# --- #\n\n\n@dag(\n    start_date=datetime(2023, 1, 1),\n    # after being unpaused this DAG will run once, afterwards it can be run\n    # manually with the play button in the Airflow UI\n    schedule=\"@once\",\n    catchup=False,\n    default_args=gv.default_args,\n    description=\"Run this DAG to start the pipeline!\",\n    tags=[\"start\", \"setup\"],\n)\ndef a_start():\n\n    # this task uses the BashOperator to run a bash command creating an Airflow\n    # pool called 'duckdb' which contains one worker slot. All tasks running\n    # queries against DuckDB will be assigned to this pool, preventing parallel\n    # requests to DuckDB.\n    create_duckdb_pool = BashOperator(\n        task_id=\"bash_task\",\n        bash_command=\"airflow pools list | grep -q 'duckdb' || airflow pools set duckdb 1 'Pool for duckdb'\",\n        outlets=[gv.DS_START],\n    )\n\nstart_dag = a_start()\n```\n:::\n\n\n## **Data Acquisition: `extract.py`**\n\nThis DAG, defined in `extract.py`, extracts football fixture data from an API and combines them.\n\n**`fetch_data` function:**\n\n-   Fetches data from the API using the `fetch_data` function, likely defined in `include.logic`.\n\n-   Optionally passes arguments like the chosen season (e.g., “2023”).\n\n**`concatenate_csvs` function:**\n\n-   Combines multiple CSV files located in a specified directory (`gv.FIXTURES_DATA_FOLDER`) into a single file (`all_fixtures_combined.csv`)\n\n-   Uses `os.walk` to iterate through the directory and `pandas.concat` to combine the dataframes.\n\n**DAG Definition:**\n\n-   Similar to `start.py`, the DAG is defined using the `@dag` decorator with various parameters.\n\n-   This DAG depends on the `gv.DS_START` output, meaning it only runs after the initial DAG (`start.py`) finishes.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n\"\"\"DAG that extracts fixtures data from an API, and concatenates them into a single file saved to the local disk.\"\"\"\n# --------------- #\n# PACKAGE IMPORTS #\n# --------------- #\nfrom pendulum import datetime\nimport io\nimport os\nimport pandas as pd\nfrom airflow.decorators import dag\nfrom airflow.operators.python import PythonOperator\n\n# -------------------- #\n# Local module imports #\n# -------------------- #\nfrom include.global_variables import global_variables as gv\nfrom include.logic import fetch_data\n\n\ndef concatenate_csvs(root_dir, output_file):\n    csv_files = []\n    for root, _, files in os.walk(root_dir):\n        for file in files:\n            if file.endswith(\".csv\"):\n                file_path = os.path.join(root, file)\n                csv_files.append(file_path)\n\n    if not csv_files:\n        gv.task_log.warning(\"No CSV files found in the specified directory.\")\n        return\n\n    combined_df = pd.concat([pd.read_csv(file) for file in csv_files])\n    combined_df.to_csv(os.path.join(root_dir, output_file), index=False)\n\n\n# --- #\n# DAG #\n# --- #\n@dag(\n    start_date=datetime(2023, 1, 1),\n    # this DAG runs as soon as the \"start\" Dataset has been produced to\n    schedule=[gv.DS_START],\n    catchup=False,\n    default_args=gv.default_args,\n    description=\"extract fixtures data from API.\",\n    tags=[\"extract\", \"API\"],\n    render_template_as_native_obj=True,\n)\ndef b_extraction():\n    api_fetcher = PythonOperator(\n            task_id = \"fetch_data\",\n            python_callable= fetch_data,\n            op_kwargs = {\n                \"chosen_season\": \"2023\"\n            }\n        )\n\n\n    concatenator = PythonOperator(\n            task_id = \"concat_csvs\",\n            python_callable= concatenate_csvs,\n            op_kwargs = {\n                \"root_dir\": gv.FIXTURES_DATA_FOLDER,\n                \"output_file\": \"all_fixtures_combined.csv\"\n\n            },\n            outlets=[gv.DS_INGEST]\n        )\n\n    api_fetcher >> concatenator\nb_extraction()\n```\n:::\n\n\n------------------------------------------------------------------------\n\n# **Data Storage and Transfer: MinIO and Airflow Operators**\n\nThis section covers how the data pipeline utilizes MinIO for object storage and interacts with it using custom Airflow operators.\n\n**Minio as Object Storage:**\n\n-   Minio serves as a central repository for storing the extracted and transformed football fixture data.\n\n-   It offers a **scalable and reliable** alternative to traditional file systems.\n\n-   Its **S3 compatibility** allows seamless integration with Airflow operators designed for S3.\n\n**Airflow Operators for MinIO Interactions:**\n\n**`CreateBucket`:** (Defined in `create_bucket.py`)\n\n-   Ensures a bucket named `gv.FIXTURES_BUCKET_NAME` exists in MinIO before uploading data.\n\n**`LocalFilesystemToMinIOOperator`:** (Defined in `custom_operators.minio.py`)\n\n-   Uploads local CSV files from `gv.FIXTURES_DATA_PATH` to the MinIO bucket.\n\n-   It utilizes dynamic task expansion, allowing you to add new files to the folder without modifying the DAG code.\n\n**`MinIOListOperator`:** (Defined in `custom_operators.minio.py`)\n\n-   Lists all objects within the `gv.FIXTURES_BUCKET_NAME` bucket.\n\n**Custom `load_fixtures_data` task:** (Defined in `load_minio_to_duckdb.py`)\n\n-   Iterates through the listed objects in Minio.\n\n-   Downloads each object as a temporary CSV file.\n\n-   Uses `duckdb.connect` to establish a connection to the DuckDB database.\n\n-   Creates a new table in DuckDB using `CREATE TABLE` with the downloaded data loaded from the temporary CSV file.\n\n-   Deletes the temporary CSV file after loading the data.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n\"\"\"DAG that loads fixtures ingests from local csv files into MinIO.\"\"\"\n\n# --------------- #\n# PACKAGE IMPORTS #\n# --------------- #\n\nfrom airflow.decorators import dag\nfrom pendulum import datetime\nimport io\n\n# -------------------- #\n# Local module imports #\n# -------------------- #\n\nfrom include.global_variables import global_variables as gv\nfrom include.custom_task_groups.create_bucket import CreateBucket\nfrom include.custom_operators.minio import LocalFilesystemToMinIOOperator\n\n# --- #\n# DAG #\n# --- #\n\n\n@dag(\n    start_date=datetime(2023, 1, 1),\n    schedule=[gv.DS_INGEST],\n    catchup=False,\n    default_args=gv.default_args,\n    description=\"Ingests fixtures data from provided csv files to MinIO.\",\n    tags=[\"ingestion\", \"minio\"],\n    render_template_as_native_obj=True,\n)\ndef c_in_fixtures_data():\n\n    create_bucket_tg = CreateBucket(\n        task_id=\"create_fixtures_bucket\", bucket_name=gv.FIXTURES_BUCKET_NAME\n    )\n\n    ingest_fixtures_data = LocalFilesystemToMinIOOperator.partial(\n        task_id=\"ingest_fixtures_data\",\n        bucket_name=gv.FIXTURES_BUCKET_NAME,\n        outlets=[gv.DS_FIXTURES_DATA_MINIO],\n    ).expand_kwargs(\n        [\n            {\n                \"local_file_path\": gv.FIXTURES_DATA_PATH,\n                \"object_name\": gv.FIXTURES_DATA_PATH.split(\"/\")[-1],\n            },\n        ]\n    )\n\n    # set dependencies\n    create_bucket_tg >> ingest_fixtures_data\n\n\nc_in_fixtures_data()\n\"\"\"DAG that loads fixtures from MinIO to DuckDB.\"\"\"\n\n# --------------- #\n# PACKAGE IMPORTS #\n# --------------- #\n\nfrom airflow.decorators import dag, task\nfrom pendulum import datetime, parse\nimport duckdb\nimport os\nimport json\n\n# -------------------- #\n# Local module imports #\n# -------------------- #\n\nfrom include.global_variables import global_variables as gv\nfrom include.custom_operators.minio import (\n    MinIOListOperator,\n    MinIODeleteObjectsOperator,\n)\n\n# --- #\n# DAG #\n# --- #\n\n\n@dag(\n    start_date=datetime(2023, 1, 1),\n    schedule=[gv.DS_FIXTURES_DATA_MINIO],\n    catchup=False,\n    default_args=gv.default_args,\n    description=\"Loads fixtures data from MinIO to DuckDB.\",\n    tags=[\"load\", \"minio\", \"duckdb\"],\n    render_template_as_native_obj=True,\n)\ndef d_load_data():\n\n\n    list_files_fixtures_bucket = MinIOListOperator(\n        task_id=\"list_files_fixtures_bucket\", bucket_name=gv.FIXTURES_BUCKET_NAME\n    )\n\n    @task(outlets=[gv.DS_DUCKDB_IN_FIXTURES], pool=\"duckdb\")\n    def load_fixtures_data(obj):\n        \"\"\"Loads content of one fileobject in the MinIO fixtures bucket\n        to DuckDB.\"\"\"\n\n        minio_client = gv.get_minio_client()\n        minio_client.fget_object(gv.FIXTURES_BUCKET_NAME, obj, file_path=obj)\n\n        cursor = duckdb.connect(gv.DUCKDB_INSTANCE_NAME)\n        cursor.execute(\n            f\"\"\"DROP TABLE IF EXISTS {gv.FIXTURES_IN_TABLE_NAME};\"\"\"\n        )\n        cursor.execute(\n            f\"\"\"CREATE TABLE IF NOT EXISTS {gv.FIXTURES_IN_TABLE_NAME} AS\n            SELECT * FROM read_csv_auto('{obj}');\"\"\"\n        )\n        cursor.commit()\n        cursor.close()\n\n        os.remove(obj)\n\n    # set dependencies\n\n    fixtures_data = load_fixtures_data.expand(obj=list_files_fixtures_bucket.output)\n\nd_load_data()\n```\n:::\n\n\n------------------------------------------------------------------------\n\n# **Data Transformation and Analysis: Filtering and Transformation in DuckDB**\n\n1.  **`find_fixtures_c1` and `find_fixtures_c2` functions:**\n\n-   Defined using `@aql.dataframe` decorators, enabling interaction with DuckDB through the Astro SDK.\n\nThey both:\n\n-   Access the data from the `gv.FIXTURES_IN_TABLE_NAME` table using `Table` objects.\n\n-   Print the ingested data to Airflow logs for debugging purposes.\n\n**2. Filtering:**\n\n-   The functions call custom logic (`apply_filtering_logic`) defined in `include.logic`, containing filtering criteria based on conditions specified by the client.\n\n-   The `lenzi` function checks if the resulting dataframe is empty. If empty, a predefined dataframe with a message (“no fixtures satisfy condition yet”) is returned to indicate no matches met the criteria.\n\n**3. Transformation:**\n\n-   The functions might also call another custom function (`won_last_5_matches`), potentially implementing calculations or transformations based on your analysis needs.\n\n**4. Storing transformed data:**\n\n-   The transformed dataframes are stored in separate DuckDB tables:\n\n-   `gv.REPORTING_TABLE_NAME_1` and `gv.REPORTING_TABLE_NAME_2`, likely corresponding to the results of `find_fixtures_c1` and `find_fixtures_c2`, respectively.\n\n-   This is achieved by specifying the `output_table` argument within the `Table` object during function calls.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n\"DAG that runs a transformation on data in DuckDB using the Astro SDK.\"\n\n# --------------- #\n# PACKAGE IMPORTS #\n# --------------- #\n\nfrom airflow.decorators import dag\nfrom pendulum import datetime\nimport pandas as pd\n\n# import tools from the Astro SDK\nfrom astro import sql as aql\nfrom astro.sql.table import Table\n\n# -------------------- #\n# Local module imports #\n# -------------------- #\n\nfrom include.global_variables import global_variables as gv\n\nfrom include.logic import apply_filtering_logic, won_last_5_matches\n\n\ndef lenzi(df):\n    \"\"\"Function to check for empty dataframe\n    https://stackoverflow.com/questions/19828822/how-do-i-check-if-a-pandas-dataframe-is-empty\n    \"\"\"\n    return len(df.index) == 0\n\n# creating a dataframe to show if no fixtures obey filtering conditions\ndefault_df = pd.DataFrame(columns=['Date', 'HomeTeam', 'HomeScore', 'AwayScore', 'AwayTeam'])\n\nrow_data = {\n    'Date': 'no',\n    'HomeTeam': 'fixtures',\n    'HomeScore': 'satisfies',\n    'AwayScore': 'condition',\n    'AwayTeam': ' yet'\n}\ndefault_df = default_df.append(row_data, ignore_index=True)\n\n\n# ----------------- #\n# Astro SDK Queries #\n# ----------------- #\n\n@aql.dataframe(pool=\"duckdb\")\ndef find_fixtures_c1(in_table: pd.DataFrame):\n    gv.task_log.info(in_table)\n\n    df = in_table\n    output_df = apply_filtering_logic(df)\n    \n    gv.task_log.info(output_df)\n    if lenzi(output_df) == True:\n        gv.task_log.info(\"df is empty\")\n        return default_df\n\n    return output_df\n\n@aql.dataframe(pool=\"duckdb\")\ndef find_fixtures_c2(in_table: pd.DataFrame):\n    gv.task_log.info(in_table)\n\n    df = in_table\n\n    output_df = won_last_5_matches(df)\n    \n    gv.task_log.info(output_df)\n    if lenzi(output_df) == True:\n        gv.task_log.info(\"df is empty\")\n        return default_df\n\n    return output_df\n\n\n# --- #\n# DAG #\n# --- #\n\n\n@dag(\n    start_date=datetime(2023, 1, 1),\n    schedule=[gv.DS_DUCKDB_IN_FIXTURES],\n    catchup=False,\n    default_args=gv.default_args,\n    description=\"Runs transformations on fixtures data in DuckDB.\",\n    tags=[\"transformation\"],\n)\ndef e_transform_fixtures():\n\n\n    find_fixtures_c1(\n        in_table=Table(\n            name=gv.FIXTURES_IN_TABLE_NAME, conn_id=gv.CONN_ID_DUCKDB\n        ),\n        output_table=Table(\n            name=gv.REPORTING_TABLE_NAME_1, conn_id=gv.CONN_ID_DUCKDB\n        ),\n    )\n\n    find_fixtures_c2(\n        in_table=Table(\n            name=gv.FIXTURES_IN_TABLE_NAME, conn_id=gv.CONN_ID_DUCKDB\n        ),\n        output_table=Table(\n            name=gv.REPORTING_TABLE_NAME_2, conn_id=gv.CONN_ID_DUCKDB\n        ),\n    )\n\n\ne_transform_fixtures()\n```\n:::\n\n\n------------------------------------------------------------------------\n\n# **Frontend with Streamlit: Data Exploration and Visualization**\n\n**Data Retrieval:**\n\n-   The app utilizes functions like `list_currently_available_tables` and `get_fixtures_1/2` to:\n\n-   List available tables in DuckDB (`reporting_table_1` and `reporting_table_2`).\n\n-   Fetch data from these tables using DuckDB queries and convert them into Pandas DataFrames.\n\n**User Interface:**\n\n-   The app displays a title, a welcome message, and uses expanders — a Streamlit component — to organize the content.\n\n-   It conditionally displays the retrieved Dataframes based on their availability in DuckDB.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# --------------- #\n# PACKAGE IMPORTS #\n# --------------- #\n\nimport streamlit as st\nimport duckdb\nimport pandas as pd\nfrom datetime import date, datetime\nimport altair as alt\n# from global_variables import global_variables as gv\n\n\n\n\nduck_db_instance_path = (\n    \"/app/include/dwh\"  # when changing this value also change the db name in .env\n)\n\n\n# -------------- #\n# DuckDB Queries #\n# -------------- #\n\n\ndef list_currently_available_tables(db=duck_db_instance_path):\n    cursor = duckdb.connect(db)\n    tables = cursor.execute(\"SHOW TABLES;\").fetchall()\n    cursor.close()\n    return [table[0] for table in tables]\n\n\n\n\n\n\n\ndef get_fixtures_1(db=duck_db_instance_path):\n\n    cursor = duckdb.connect(db)\n    fixtures_data = cursor.execute(\n        f\"\"\"SELECT * FROM reporting_table_1;\"\"\"\n    ).fetchall()\n\n    fixtures_data_col_names = cursor.execute(\n        f\"\"\"SELECT column_name from information_schema.columns where table_name = 'reporting_table_1';\"\"\"\n    ).fetchall()\n\n    df = pd.DataFrame(\n        fixtures_data, columns=[x[0] for x in fixtures_data_col_names]\n    )\n    cursor.close()\n\n    return df\n\ndef get_fixtures_2(db=duck_db_instance_path):\n\n    cursor = duckdb.connect(db)\n    fixtures_data = cursor.execute(\n        f\"\"\"SELECT * FROM reporting_table_2;\"\"\"\n    ).fetchall()\n\n    fixtures_data_col_names = cursor.execute(\n        f\"\"\"SELECT column_name from information_schema.columns where table_name = 'reporting_table_2';\"\"\"\n    ).fetchall()\n\n    df = pd.DataFrame(\n        fixtures_data, columns=[x[0] for x in fixtures_data_col_names]\n    )\n    cursor.close()\n\n    return df\n\n\n\n\n# ------------ #\n# Query DuckDB #\n# ------------ #\n\n\ntables = list_currently_available_tables()\n\n\nif \"reporting_table_1\" in tables:\n    fixtures_result_table_1 = get_fixtures_1()\n\n\nif \"reporting_table_2\" in tables:\n    fixtures_result_table_2 = get_fixtures_2()\n\n\n\n# ------------- #\n# STREAMLIT APP #\n# ------------- #\n\nst.title(\"Fixtures Transformation Results\")\n\nst.markdown(f\"Hello Joshua :wave: Welcome to your Streamlit App! :blush:\")\n# Get the DataFrame\n\n# Display the DataFrame as a table in Streamlit\n# st.dataframe(fixtures_result_table_1)\n\nwith st.expander(\"Section 1: Fixtures in which either teams have won 3 games or more against common opponents in the league\"):\n    st.dataframe(fixtures_result_table_1)  # Optional styling\n\nwith st.expander(\"Section 2: Fixtures in which either teams have won all 5 games in their last 5 league matches\"):\n    st.dataframe(fixtures_result_table_2)\n```\n:::\n\n\nI would have posted a link to allow users to access the app but I ran out of Github Codespaces compute credits. I will update this post with the link when my monthly quota resets.\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}