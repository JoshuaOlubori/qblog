{
  "hash": "d258f3236dff2993c0e0ecc837abea04",
  "result": {
    "markdown": "---\ntitle: \"Developing a Modern Data Pipeline — Part 1\"\nsubtitle: \"A 3-part series featuring web-scraping, data pipelines, orchestration and containerisation\"\nauthor: \"Edun Joshua\"\ndate: \"2024-03-05\"\ncategories: [data engineering, web-scraping]\nimage: \"image.png\"\neval: false\n---\n\n::: {style=\"color: #25D366;\"}\n## Context\n:::\n\nIn January, a client approached me regarding an analytics project involving football data. This blog post chronicles the first iteration of my data gathering approach: a custom web scraper. I detail my code section by section, explaining its logic. See [Part 3](https://joshuaolubori.onrender.com/posts/2024-03-05_yusuf_app_3) for the final implementation of the pipeline.\n\n::: {style=\"color: #25D366;\"}\n## 1. Setting Up The Stage: Imports and Options\n:::\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import TimeoutException\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n```\n:::\n\n\nThis section imports the necessary libraries for web scraping. Here’s a breakdown of their roles:\n\n-   **selenium**: Controls the browser and interacts with web elements.\n\n-   **BeautifulSoup**: Parses and navigates the downloaded HTML content.\n\n-   **pandas**: Creates and manipulates dataframes for efficient data storage.\n\nNext, the code defines Chrome `webdriver` options:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Define the Chrome webdriver options\noptions = webdriver.ChromeOptions()\noptions.add_argument(\"--headless\")\n\noptions.page_load_strategy = \"none\"\n```\n:::\n\n\nThese lines configure the Chrome webdriver:\n\n-   --`headless`: Runs the browser in headless mode, making it invisible for better performance.\n\n-   `page_load_strategy`=\"none\": Instructs Selenium to not wait for full page load, as the relevant content is dynamically generated after the initial load.\n\nFinally, a `webdriver` instance is created and directed to the target URL:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndriver = webdriver.Chrome(options=options)\ndriver.get(url)\n```\n:::\n\n\n::: {style=\"color: #25D366;\"}\n## 2. Infinite Scrolling and Button Handling\n:::\n\nThis section utilizes Selenium to simulate user behavior and dynamically load additional content:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nhtml_content = \"\"\n\nwhile True:\n    try:\n        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n\n        button = driver.find_element(By.CLASS_NAME, \"event__more--static\")\n\n        driver.execute_script(\"arguments[0].click();\", button)\n\n        time.sleep(2)\n\n        html_content += driver.page_source\n\n    except NoSuchElementException:\n        break\n```\n:::\n\n\nThis loop continuously performs the following actions:\n\n1.  Scrolls down to the page bottom: This ensures the “Show more matches” button, if present, is loaded within the viewport.\n\n2.  Locates the button: It utilizes find_element with `By.CLASS_NAME` to search for the element with the class name \"event\\_\\_more--static\".\n\n3.  Clicks the button: It employs JavaScript execution (`execute_script`) to simulate a click on the button element.\n\n4.  Waits for content to load: A short delay (`time.sleep(2)`) allows the page to update with additional matches after clicking.\n\n5.  Appends content: The current HTML content is retrieved using `driver.page_source` and appended to the `html_content` string.\n\n6.  Handles button absence: The `try...except` block catches the `NoSuchElementException` if the button is no longer found, indicating all matches have been loaded, and subsequently breaks the loop.\n\n::: {style=\"color: #25D366;\"}\n## 3. Data Extraction with BeautifulSoup\n:::\n\nOnce all data is loaded, the loop terminates, and the script utilizes BeautifulSoup for parsing content:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nsoup = BeautifulSoup(html_content, 'html.parser')\n\nmatch_containers = soup.find_all('div', class_='event__match event__match--static event__match--twoLine')\n\ndates = []\nhome_teams = []\nhome_scores = []\naway_scores = []\naway_teams = []\n\nmatch_containers = soup.find_all('div', class_=['event__match event__match--static event__match--twoLine', 'event__match event__match--static event__match--last event__match--twoLine'])\n\nfor match in match_containers:\n    date_time_str = match.find('div', class_='event__time').text.strip()\n    date_time_str = f\"2023-{date_time_str.replace('.', '-')}\"\n\n    home_team = match.find('div', class_='event__participant--home').text.strip()\n    home_score = int(match.find('div', class_='event__score--home').text.strip())\n    away_score = int(match.find('div', class_='event__score--away').text.strip())\n    away_team = match.find('div', class_='event__participant--away').text.strip()\n\n    dates.append(date_time_str)\n    home_teams.append(home_team)\n    home_scores.append(home_score)\n    away_scores.append(away_score)\n    away_teams.append(away_team)\n```\n:::\n\n\nThis section performs the following tasks:\n\n1.  Initializes empty lists: Creates lists to store extracted data — dates, home team names, home scores, away scores, and away team names.\n\n2.  Expands match container selection: Updates `find_all` to search for two class combinations, ensuring all potential match containers are captured.\n\n3.  Loops through each match: Iterates over each `match_container` element.\n\n4.  Extracts date and time: Locates the '`event__time`' element within the current match and extracts its text content using `.text.strip()` to remove leading/trailing whitespace. It then formats the date string for consistency.\n\n5.  Extracts team and score information: Similar to the date, the script finds elements with specific class names within the match container and extracts their text content for each team and score.\n\n6.  Appends data to lists: The extracted information for each match is added to the corresponding lists.\n\n::: {style=\"color: #25D366;\"}\n## 4. Data Organization and Saving\n:::\n\nAfter scraping individual match details, the script organizes and saves the data:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndata = {'Date': dates, 'HomeTeam': home_teams, 'HomeScore': home_scores, 'AwayScore': away_scores, 'AwayTeam': away_teams}\ndf = pd.DataFrame(data)\n\nleague_name = url.split('/')[-3]\n\noutput_file_name = f\"{league_name}_results.csv\"\ndf.to_csv(output_file_name, index=False)\n\nprint(\"DONE SCRAPING!\")\ndriver.quit()\n```\n:::\n\n\nHaving explored the web scraping technique, we’ll shift to the API approach in the next part, as the client opted for a robust solution using an API (as previously suggested by me).\n\nUp next, [Part 2!](https://joshuaolubori.onrender.com/posts/2024-03-05_yusuf_app_2/) of the series.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}